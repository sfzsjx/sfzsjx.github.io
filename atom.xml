<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>上善若水博客</title>
  
  <subtitle>大数据</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-05-29T07:21:00.827Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>上善若水</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>flink 算子记录</title>
    <link href="http://yoursite.com/2020/05/29/flink-%E7%AE%97%E5%AD%90%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2020/05/29/flink-算子记录/</id>
    <published>2020-05-29T05:37:12.000Z</published>
    <updated>2020-05-29T07:21:00.827Z</updated>
    
    <content type="html"><![CDATA[<p>1、join静态数据库<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 关联运单表,以及订单完成情况</span><br><span class="line"> * 输入 Tuple5&lt;&gt;(positionTime, imei, latitude, longitude, geoHashValue)</span><br><span class="line"> * 输出 WayBill</span><br><span class="line"> * @author hadoop</span><br><span class="line"> */</span><br><span class="line">public class JoinWaybill extends RichFlatMapFunction&lt;Tuple5&lt;Timestamp, String, Double, Double, String&gt;, WayBill&gt; &#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void flatMap(Tuple5&lt;Timestamp, String, Double, Double, String&gt; value, Collector&lt;WayBill&gt; out) throws Exception &#123;</span><br><span class="line">        Connection conn;</span><br><span class="line">        Statement statement;</span><br><span class="line">        conn = DruidPoolUtils.getConnection();</span><br><span class="line">        statement = conn.createStatement();</span><br><span class="line">        ResultSet resultSet =  statement.executeQuery(&quot;SELECT waybill_no,device_id,longitude,latitude,&quot; +</span><br><span class="line">                    &quot; geo_hash,factory_time,bind_status,leave_time,distance_departure,waybill_status,finish_time FROM  t_gps_waybill where device_id = &quot; + value.f1);</span><br><span class="line"></span><br><span class="line">        while (resultSet.next())&#123;</span><br><span class="line">                WayBill wayBill = new WayBill();</span><br><span class="line">                wayBill.setWayBill(resultSet.getString(&quot;waybill_no&quot;));</span><br><span class="line">                out.collect(wayBill);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        DruidPoolUtils.close(conn,statement);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>2.sink mysql<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">.flatMap(new JoinWaybill())</span><br><span class="line">      .addSink(new RichSinkFunction&lt;WayBill&gt;() &#123;</span><br><span class="line">          @Override</span><br><span class="line">          public void invoke(WayBill value, Context context) throws Exception &#123;</span><br><span class="line">              Connection conn;</span><br><span class="line">              PreparedStatement statement;</span><br><span class="line">              conn = DruidPoolUtils.getConnection();</span><br><span class="line"></span><br><span class="line">              statement = conn.prepareStatement(</span><br><span class="line">                      &quot; INSERT INTO t_gps_test (waybill_no)&quot; +</span><br><span class="line">                              &quot; values(?)&quot; );</span><br><span class="line"></span><br><span class="line">              statement.setString( 1,  value.getWayBill() );</span><br><span class="line">              statement.execute();</span><br><span class="line">              DruidPoolUtils.close(conn,statement);</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;)</span><br></pre></td></tr></table></figure></p><p>3.window<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.window(SlidingEventTimeWindows.of(Time.minutes(1),Time.minutes(1)))</span><br><span class="line">                .process(new JoinWaybill()).print()</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1、join静态数据库&lt;br&gt;&lt;figure class=&quot;highlight plain hljs&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;
      
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>mysql 常见问题</title>
    <link href="http://yoursite.com/2020/05/29/mysql-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2020/05/29/mysql-常见问题/</id>
    <published>2020-05-29T05:33:17.000Z</published>
    <updated>2020-06-02T07:55:07.144Z</updated>
    
    <content type="html"><![CDATA[<p>1.druidpool连接池空指针异常<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Connection conn;</span><br><span class="line">       Statement statement;</span><br><span class="line">       conn = DruidPoolUtils.getConnection();</span><br><span class="line">       statement = conn.createStatement();</span><br></pre></td></tr></table></figure></p><p>2.Exception in thread “main” java.sql.SQLException: Can not call getNString() when field’s charset isn’t UTF-8<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wayBill.setWayBill(resultSet.getNString(&quot;waybill_no&quot;));</span><br><span class="line">          System.out.println(resultSet.getString(&quot;waybill_no&quot;));</span><br></pre></td></tr></table></figure></p><p>3.当数据为null时，插入数据库是，程序异常</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">   statement.setInt( 1,  wayBillInfo.getBindStatus() );</span><br><span class="line">        statement.setTimestamp( 2, wayBillInfo.getLeaveTime());</span><br><span class="line">        statement.setDouble( 3, wayBillInfo.getDistanceDeparture());</span><br><span class="line">        statement.setLong( 4, wayBillInfo.getTimeDeparture());</span><br><span class="line">        statement.setInt( 5, wayBillInfo.getWayBillStatus());</span><br><span class="line">        statement.setTimestamp( 6,  wayBillInfo.getFinishTime() );</span><br><span class="line">        statement.setString( 7, wayBillInfo.getWayBillNo() );</span><br><span class="line"></span><br><span class="line">解决办法：setObject()</span><br></pre></td></tr></table></figure><p>4.sql防注入以及参数传递<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Connection conn = DruidPoolUtils.getConnection();</span><br><span class="line">        PreparedStatement statement = conn.prepareStatement(&quot;&quot;);</span><br><span class="line"></span><br><span class="line">        ResultSet resultSet = statement.executeQuery(&quot;SELECT order_no,purpose_latitude,purpose_longitude ,finish_time FROM t_gps_order_list WHERE waybill_no = &apos;&quot; +wayBill.getWayBillNo() +&quot;&apos;&quot; );</span><br></pre></td></tr></table></figure></p><p>5.mysql差集<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.oname, a.odesc </span><br><span class="line">FROM</span><br><span class="line">  object_a a </span><br><span class="line">  LEFT JOIN object_b b </span><br><span class="line">    ON a.oname = b.oname </span><br><span class="line">    AND a.odesc = b.odesc </span><br><span class="line">WHERE b.id IS NULL</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SELECT a.oname, a.odesc FROM object_a a </span><br><span class="line">MINUS </span><br><span class="line">SELECT b.oname, b.odesc FROM object_b b</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1.druidpool连接池空指针异常&lt;br&gt;&lt;figure class=&quot;highlight plain hljs&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class
      
    
    </summary>
    
      <category term="数据库 " scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="mysql" scheme="http://yoursite.com/tags/mysql/"/>
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>flink 常见问题</title>
    <link href="http://yoursite.com/2020/05/27/flink-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2020/05/27/flink-常见问题/</id>
    <published>2020-05-27T08:15:57.000Z</published>
    <updated>2020-06-01T08:14:53.011Z</updated>
    
    <content type="html"><![CDATA[<p>1.Exception in thread “main” org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of ‘Collector’ are missing. In many cases lambda methods don’t provide enough information for automatic type extraction when Java generics are involved. An easy workaround is to use an (anonymous) class instead that implements the ‘org.apache.flink.streaming.api.functions.windowing.AllWindowFunction’ interface. Otherwise the type has to be specified explicitly using type information</p><p><img src="/images/2020/05/27/05f0a360-9ff2-11ea-ba39-730617437852.png" alt="image.png"></p><p>解决办法：加上返回数据类型，如：<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.returns(TypeInformation.of(new TypeHint&lt;List&lt;Tuple8&lt;Timestamp, String, Double, Double, String, Integer, Long, String&gt;&gt;&gt;()&#123;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure></p><p>2.flink 一直重复计算第一条数据<br><img src="/images/2020/05/29/c3e8f080-a19a-11ea-ba39-730617437852.png" alt="image.png"></p><p>原因：<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">与flink的状态值有关</span><br></pre></td></tr></table></figure></p><p>解决办法<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">每次用到value是都调用 .value方法,如：</span><br><span class="line"></span><br><span class="line"> wayBillState.value().equals(0)</span><br></pre></td></tr></table></figure></p><p>可能原因2：<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">该算子自身报错，flinkcheckpoint 后重新计算导致程序一直在处理第一条数据</span><br></pre></td></tr></table></figure></p><p>3.flink的状态变量如何只更新元组中的某个字段，其他字段不更新<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">目前没有什么好办法：现提供两个不是办法的办法</span><br><span class="line">1.将两个字段分别用两个不同的状态变量保存，这样就可以分别更新了，但是造成状态变量过多，内存浪费</span><br><span class="line">2.还是用一个变量，在更新其中的某个字段时，取出不更新的字段与更新的字段和并，更新这个状态值</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1.Exception in thread “main” org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of ‘Collector’ are 
      
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>常用工具类</title>
    <link href="http://yoursite.com/2020/05/11/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%B1%BB/"/>
    <id>http://yoursite.com/2020/05/11/常用工具类/</id>
    <published>2020-05-11T07:27:49.000Z</published>
    <updated>2020-05-11T07:27:50.021Z</updated>
    
    <content type="html"><![CDATA[<h3 id="时间工具类"><a href="#时间工具类" class="headerlink" title="时间工具类"></a>时间工具类</h3><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Timestamp;</span><br><span class="line">import java.text.DateFormat;</span><br><span class="line">import java.text.ParseException;</span><br><span class="line">import java.text.SimpleDateFormat;</span><br><span class="line">import java.util.Calendar;</span><br><span class="line">import java.util.Date;</span><br><span class="line">import java.util.Locale;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 时间工具类</span><br><span class="line"> * @author hadoop</span><br><span class="line"> * @date 2020-03-24</span><br><span class="line"> */</span><br><span class="line">public class DateUtils &#123;</span><br><span class="line"></span><br><span class="line">    private static  SimpleDateFormat standardTimeFormat = new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;);</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 比较两个时间大小 格式yyyy-MM-dd hh:mm:ss</span><br><span class="line">     * @return 布尔值</span><br><span class="line">     */</span><br><span class="line">    public static Boolean compare2Time(Timestamp startTime, Timestamp endTime) throws ParseException &#123;</span><br><span class="line"></span><br><span class="line">        if (startTime == null || endTime == null)&#123;</span><br><span class="line">            return false;</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            return startTime.before(endTime);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 日期格式转换</span><br><span class="line">     * @param oldDateStr 原先时间格式</span><br><span class="line">     * @return 新时间格式</span><br><span class="line">     * @throws ParseException 转换</span><br><span class="line">     */</span><br><span class="line">    public static String dealDateFormat(String oldDateStr) throws ParseException&#123;</span><br><span class="line">        //此格式只有  jdk 1.7才支持  yyyy-MM-dd‘T‘HH:mm:ss.SSSXXX</span><br><span class="line">        DateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd&apos;T&apos;HH:mm:ss.SSSXXX&quot;);</span><br><span class="line">        if (oldDateStr != null)&#123;</span><br><span class="line">            Date  date = df.parse(oldDateStr);</span><br><span class="line">            SimpleDateFormat df1 = new SimpleDateFormat (&quot;EEE MMM dd HH:mm:ss Z yyyy&quot;, Locale.UK);</span><br><span class="line">            Date date1 =  df1.parse(date.toString());</span><br><span class="line">            DateFormat df2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">            return df2.format(date1);</span><br><span class="line">        &#125;else &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 得到两个时间差  格式yyyy-MM-dd HH:mm:ss</span><br><span class="line">     * @param start 2019-06-27 14:12:40</span><br><span class="line">     * @param end   2019-08-27 14:12:40</span><br><span class="line">     * @return 5270400000</span><br><span class="line">     */</span><br><span class="line">    public static Long dateSubtraction(String start, String end) &#123;</span><br><span class="line">        SimpleDateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">        try &#123;</span><br><span class="line">            Date date1 = df.parse(start);</span><br><span class="line">            Date date2 = df.parse(end);</span><br><span class="line">            Long diff = date2.getTime() - date1.getTime();</span><br><span class="line">            Long days = diff / (1000 * 60 * 60 * 24);</span><br><span class="line">            Long hours = (diff-days*(1000 * 60 * 60 * 24))/(1000* 60 * 60);</span><br><span class="line">            Long minutes = (diff-days*(1000 * 60 * 60 * 24)-hours*(1000* 60 * 60))/(1000* 60);</span><br><span class="line">            Long totalMin = days * 12 + hours*60 + minutes;</span><br><span class="line">            return totalMin;</span><br><span class="line">        &#125; catch (ParseException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            return 0L;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 获取当前日期前一天</span><br><span class="line">     * @return 2019-08-26</span><br><span class="line">     */</span><br><span class="line">    public static String getBeforeDay() &#123;</span><br><span class="line"></span><br><span class="line">        SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);</span><br><span class="line">        Date date = new Date();</span><br><span class="line">        Calendar calendar = Calendar.getInstance();</span><br><span class="line">        calendar.setTime(date);</span><br><span class="line">        calendar.add(Calendar.DAY_OF_MONTH, -1);</span><br><span class="line">        date = calendar.getTime();</span><br><span class="line">        return sdf.format(date);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 获取当前日期前一天新格式（yyyyMMdd）</span><br><span class="line">     * @return 2019-08-26</span><br><span class="line">     */</span><br><span class="line">    public static String getBeforeDayNewFormat() &#123;</span><br><span class="line">        SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMdd&quot;);</span><br><span class="line">        Date date = new Date();</span><br><span class="line">        Calendar calendar = Calendar.getInstance();</span><br><span class="line">        calendar.setTime(date);</span><br><span class="line">        calendar.add(Calendar.DAY_OF_MONTH, -1);</span><br><span class="line">        date = calendar.getTime();</span><br><span class="line">        return sdf.format(date);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 添加几分钟时间</span><br><span class="line">     * @param timestamp 时间戳</span><br><span class="line">     * @param time 毫秒数</span><br><span class="line">     * @return 时间戳</span><br><span class="line">     */</span><br><span class="line">    public static Timestamp addTime(Timestamp timestamp ,Long time)&#123;</span><br><span class="line"></span><br><span class="line">        return  new Timestamp(timestamp.getTime() + time);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;时间工具类&quot;&gt;&lt;a href=&quot;#时间工具类&quot; class=&quot;headerlink&quot; title=&quot;时间工具类&quot;&gt;&lt;/a&gt;时间工具类&lt;/h3&gt;&lt;figure class=&quot;highlight plain hljs&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gu
      
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="工具类" scheme="http://yoursite.com/tags/%E5%B7%A5%E5%85%B7%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>大数据处理地理数据</title>
    <link href="http://yoursite.com/2020/05/07/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%9C%B0%E7%90%86%E6%95%B0%E6%8D%AE/"/>
    <id>http://yoursite.com/2020/05/07/大数据处理地理数据/</id>
    <published>2020-05-07T11:55:22.000Z</published>
    <updated>2020-05-11T02:51:49.792Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-调用http请求工具类"><a href="#1-调用http请求工具类" class="headerlink" title="1.调用http请求工具类"></a>1.调用http请求工具类</h2><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- http start --&gt;</span><br><span class="line">       &lt;dependency&gt;</span><br><span class="line">           &lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt;</span><br><span class="line">           &lt;artifactId&gt;okhttp&lt;/artifactId&gt;</span><br><span class="line">           &lt;version&gt;4.6.0&lt;/version&gt;</span><br><span class="line">       &lt;/dependency&gt;</span><br><span class="line">       &lt;!-- http end --&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line">import okhttp3.*;</span><br><span class="line">import org.slf4j.Logger;</span><br><span class="line">import org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line">public class OkHttpUtil &#123;</span><br><span class="line">    private static final Logger log = LoggerFactory.getLogger(OkHttpUtil.class);</span><br><span class="line">    public final static int CONNECT_TIMEOUT = 10;</span><br><span class="line">    public final static int READ_TIMEOUT = 30;</span><br><span class="line">    public final static int WRITE_TIMEOUT = 60;</span><br><span class="line">    private OkHttpClient mOkHttpClient;</span><br><span class="line"></span><br><span class="line">    private OkHttpUtil() &#123;</span><br><span class="line">        OkHttpClient.Builder clientBuilder = new OkHttpClient.Builder();</span><br><span class="line">        clientBuilder.readTimeout(READ_TIMEOUT, TimeUnit.SECONDS);//读取超时</span><br><span class="line">        clientBuilder.connectTimeout(CONNECT_TIMEOUT, TimeUnit.SECONDS);//连接超时</span><br><span class="line">        clientBuilder.writeTimeout(WRITE_TIMEOUT, TimeUnit.SECONDS);//写入超时</span><br><span class="line">        mOkHttpClient = clientBuilder.build();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static OkHttpUtil getInstance() &#123;</span><br><span class="line">        return Holder.OK_HTTP_UTIL;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String get(String url) throws Exception &#123;</span><br><span class="line">        return get(url, null);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String get(String url, HashMap&lt;String, String&gt; headers, HashMap&lt;String, String&gt; params) &#123;</span><br><span class="line">        String result = &quot;&quot;;</span><br><span class="line">        url = handleRequestParams(url, params);</span><br><span class="line"></span><br><span class="line">        Request request = new Request.Builder()</span><br><span class="line">                .url(url)</span><br><span class="line">                .headers(Headers.of(headers))</span><br><span class="line">                .get()</span><br><span class="line">                .build();</span><br><span class="line">        Call call = mOkHttpClient.newCall(request);</span><br><span class="line">        try &#123;</span><br><span class="line">            Response execute = call.execute();</span><br><span class="line">            result = execute.body().string();</span><br><span class="line"></span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            log.error(&quot;http 访问错误：&quot; + e);</span><br><span class="line">        &#125;</span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String get(String url, HashMap&lt;String, String&gt; params) throws Exception &#123;</span><br><span class="line">        String result = &quot;&quot;;</span><br><span class="line">        url = handleRequestParams(url, params);</span><br><span class="line"></span><br><span class="line">        Request request = new Request.Builder()</span><br><span class="line">                .url(url)</span><br><span class="line">                .get()</span><br><span class="line">                .build();</span><br><span class="line">        Call call = mOkHttpClient.newCall(request);</span><br><span class="line">        try &#123;</span><br><span class="line">            Response execute = call.execute();</span><br><span class="line">            int code = execute.code();</span><br><span class="line"></span><br><span class="line">            result = execute.body().string();</span><br><span class="line"></span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            log.error(&quot;http 访问错误：&quot; + e.getMessage());</span><br><span class="line">            throw new IOException(e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private String handleRequestParams(String url, HashMap&lt;String, String&gt; params) &#123;</span><br><span class="line">        if (params != null &amp;&amp; params.values().size() &gt; 0) &#123;</span><br><span class="line">            StringBuilder stringBuilder = new StringBuilder();</span><br><span class="line">            stringBuilder.append(&quot;?&quot;);</span><br><span class="line">            for (Map.Entry&lt;String, String&gt; entry : params.entrySet()) &#123;</span><br><span class="line">                stringBuilder.append(entry.getKey() + &quot;=&quot; + entry.getValue() + &quot;&amp;&quot;);</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">            String substring = stringBuilder.substring(0, stringBuilder.length() - 1);</span><br><span class="line">            url += substring;</span><br><span class="line">        &#125;</span><br><span class="line">        return url;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String post(String url, HashMap&lt;String, String&gt; headerParams, String body) &#123;</span><br><span class="line">        String result = &quot;&quot;;</span><br><span class="line">        Request request = new Request.Builder()</span><br><span class="line">                .url(url)</span><br><span class="line">                .headers(Headers.of(headerParams))</span><br><span class="line">                .post(RequestBody.create(MediaType.parse(&quot;application/json&quot;), body))</span><br><span class="line">                .build();</span><br><span class="line">        Call call = mOkHttpClient.newCall(request);</span><br><span class="line">        try &#123;</span><br><span class="line">            Response execute = call.execute();</span><br><span class="line">            int code = execute.code();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            log.info(&quot;post请求异常&quot; + e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String post(String url, String body) &#123;</span><br><span class="line">        String result = &quot;&quot;;</span><br><span class="line">        Request request = new Request.Builder()</span><br><span class="line">                .url(url)</span><br><span class="line">                .post(RequestBody.create(MediaType.parse(&quot;application/json&quot;), body))</span><br><span class="line">                .build();</span><br><span class="line">        Call call = mOkHttpClient.newCall(request);</span><br><span class="line">        try &#123;</span><br><span class="line">            Response execute = call.execute();</span><br><span class="line">            result = execute.body().string();</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String xmlPost(String url, String body) &#123;</span><br><span class="line">        String result = &quot;&quot;;</span><br><span class="line">        Request request = new Request.Builder()</span><br><span class="line">                .url(url)</span><br><span class="line">                .post(RequestBody.create(MediaType.parse(&quot;application/xml&quot;), body))</span><br><span class="line">                .build();</span><br><span class="line">        Call call = mOkHttpClient.newCall(request);</span><br><span class="line">        try &#123;</span><br><span class="line">            Response execute = call.execute();</span><br><span class="line">            result = execute.body().string();</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    private static class Holder &#123;</span><br><span class="line">        private static final OkHttpUtil OK_HTTP_UTIL = new OkHttpUtil();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-腾讯地图调用工具类"><a href="#2-腾讯地图调用工具类" class="headerlink" title="2.腾讯地图调用工具类"></a>2.腾讯地图调用工具类</h2><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">package com.gree.bdc.util;</span><br><span class="line"></span><br><span class="line">import com.alibaba.fastjson.JSON;</span><br><span class="line">import com.gree.bdc.entity.AddressToCoordinateResult;</span><br><span class="line">import com.gree.bdc.entity.CoordinateToAddressResult;</span><br><span class="line">import com.gree.bdc.entity.Point;</span><br><span class="line">import org.slf4j.Logger;</span><br><span class="line">import org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line">import java.util.HashMap;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @author hadoop</span><br><span class="line"> */</span><br><span class="line">public class MapUtil &#123;</span><br><span class="line">    private static final Logger log = LoggerFactory.getLogger(MapUtil.class);</span><br><span class="line"></span><br><span class="line">    //key需要自己申请</span><br><span class="line">    public static final String KEY = &quot;xxxx&quot;;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 通过地址转坐标</span><br><span class="line">     * @param address 地址</span><br><span class="line">     * @throws Exception exception</span><br><span class="line">     */</span><br><span class="line">  public static Point addressToCoordinate(String address) throws Exception &#123;</span><br><span class="line">      String url = &quot;https://apis.map.qq.com/ws/geocoder/v1/&quot;;</span><br><span class="line">      HashMap&lt;String, String&gt; params = new HashMap&lt;&gt;(8);</span><br><span class="line">      params.put(&quot;address&quot;,address);</span><br><span class="line">      params.put(&quot;key&quot;,KEY);</span><br><span class="line"></span><br><span class="line">      String result = OkHttpUtil.getInstance().get(url, params);</span><br><span class="line">      AddressToCoordinateResult addressToCoordinateResult = JSON.parseObject(result, AddressToCoordinateResult.class);</span><br><span class="line">      Integer status = addressToCoordinateResult.getStatus();</span><br><span class="line">      if (status == 0 )&#123;</span><br><span class="line">          AddressToCoordinateResult.Result.Location location = addressToCoordinateResult.getResult().getLocation();</span><br><span class="line">          Double lng = location.getLng();</span><br><span class="line">          Double lat = location.getLat();</span><br><span class="line">          Point point = new Point();</span><br><span class="line">          point.setLat(lat);</span><br><span class="line">          point.setLng(lng);</span><br><span class="line">          return point;</span><br><span class="line">      &#125;</span><br><span class="line">      return null;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">*两个坐标的距离</span><br><span class="line">*</span><br><span class="line">**/</span><br><span class="line">  public static Double distanceCalculate(double lat1, double lon1, double lat2, double lon2)&#123;</span><br><span class="line"></span><br><span class="line">          Double latitude1, longitude1, latitude2, longitude2;</span><br><span class="line">          Double dlat, dlon;</span><br><span class="line">          latitude1 = lat1;</span><br><span class="line">          longitude1 = lon1;</span><br><span class="line">          latitude2 = lat2;</span><br><span class="line">          longitude2 = lon2;</span><br><span class="line">          //computing procedure</span><br><span class="line">          Double a, c, distance;</span><br><span class="line"></span><br><span class="line">          dlon = Math.abs((longitude2 - longitude1)) * Math.PI / 180;</span><br><span class="line">          dlat = Math.abs((latitude2 - latitude1)) * Math.PI / 180;</span><br><span class="line">          a = (Math.sin(dlat / 2) * Math.sin(dlat / 2)) + Math.cos(latitude1 * Math.PI / 180) * Math.cos(latitude2 * Math.PI / 180) * (Math.sin(dlon / 2) * Math.sin(dlon / 2));</span><br><span class="line">          if (a == 1.0)&#123;</span><br><span class="line">              c = Math.PI;</span><br><span class="line">          &#125; else&#123;</span><br><span class="line">              c = 2 * Math.atan(Math.sqrt(a) / Math.sqrt(1 - a));</span><br><span class="line"></span><br><span class="line">          &#125;</span><br><span class="line">          distance = 6378137.0 * c;</span><br><span class="line">          return distance;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 通过地址转坐标</span><br><span class="line">     * @param location 地址</span><br><span class="line">     * @throws Exception exception</span><br><span class="line">     */</span><br><span class="line">    public static String coordinateToAddress(String location) throws Exception &#123;</span><br><span class="line">        String url = &quot;https://apis.map.qq.com/ws/geocoder/v1/&quot;;</span><br><span class="line">        HashMap&lt;String, String&gt; params = new HashMap&lt;&gt;(8);</span><br><span class="line">        params.put(&quot;location&quot;,location);</span><br><span class="line">        params.put(&quot;key&quot;,KEY);</span><br><span class="line"></span><br><span class="line">        String result = OkHttpUtil.getInstance().get(url, params);</span><br><span class="line">        CoordinateToAddressResult coordinateToAddressResult = JSON.parseObject(result, CoordinateToAddressResult.class);</span><br><span class="line">        ///System.out.println(coordinateToAddressResult);</span><br><span class="line">        Integer status = coordinateToAddressResult.getStatus();</span><br><span class="line">        if (status == 0 )&#123;</span><br><span class="line">            String address = coordinateToAddressResult.getResult().getAddress();</span><br><span class="line"></span><br><span class="line">            return address;</span><br><span class="line">        &#125;</span><br><span class="line">        return null;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>一般而言，腾讯地图的接口调用都有限制，对于普通的开发者上线为10000，并发5次/s，不建议正规项目使用，一旦并发过高会导致很多的地址解析失败，单个解析没问题，很难发现这个问题，故最好还是申请企业认证，申请企业认证每天可以300万的次数和1000的并发。<a href="https://lbs.qq.com/service/webService/webServiceGuide/webServiceMatrix" target="_blank" rel="noopener">腾讯地图位置服务网站</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-调用http请求工具类&quot;&gt;&lt;a href=&quot;#1-调用http请求工具类&quot; class=&quot;headerlink&quot; title=&quot;1.调用http请求工具类&quot;&gt;&lt;/a&gt;1.调用http请求工具类&lt;/h2&gt;&lt;figure class=&quot;highlight plai
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>mysql常见异常处理</title>
    <link href="http://yoursite.com/2020/04/14/mysql%E5%B8%B8%E8%A7%81%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2020/04/14/mysql常见异常处理/</id>
    <published>2020-04-14T09:04:24.000Z</published>
    <updated>2020-05-13T01:28:11.745Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mysql异常处理"><a href="#mysql异常处理" class="headerlink" title="mysql异常处理"></a>mysql异常处理</h1><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">解决java.sql.SQLException: Value &apos;0000-00-00&apos; can not be represented as java.sql.Date</span><br><span class="line"></span><br><span class="line">jdbc:mysql://yourserver:3306/yourdatabase?zeroDateTimeBehavior=convertToNull</span><br></pre></td></tr></table></figure><p>#mysql文本类型数据存储<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql中text 最大长来度为65,535(2的16次方–1)字符的TEXT列。</span><br><span class="line">如果你觉得text长度自不够百，可以选择</span><br><span class="line">MEDIUMTEXT最大长度为16,777,215。</span><br><span class="line">LONGTEXT最大长度为4,294,967,295</span><br><span class="line">Text主要度是用来存放非二进制的文本</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;mysql异常处理&quot;&gt;&lt;a href=&quot;#mysql异常处理&quot; class=&quot;headerlink&quot; title=&quot;mysql异常处理&quot;&gt;&lt;/a&gt;mysql异常处理&lt;/h1&gt;&lt;figure class=&quot;highlight plain hljs&quot;&gt;&lt;table&gt;&lt;
      
    
    </summary>
    
      <category term="数据库 " scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="mysql" scheme="http://yoursite.com/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>接口调试问题</title>
    <link href="http://yoursite.com/2020/04/11/%E6%8E%A5%E5%8F%A3%E8%B0%83%E8%AF%95%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2020/04/11/接口调试问题/</id>
    <published>2020-04-11T02:11:31.000Z</published>
    <updated>2020-05-09T07:06:15.343Z</updated>
    
    <content type="html"><![CDATA[<p>一、postman接口测试工具如何接收java.util.date参数</p><blockquote><p>时间格式采用：2020/04/11 14:12:14</p></blockquote><p>二、java 参数类型与mysql类型比较<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">===========java注入数据库==========</span><br><span class="line"></span><br><span class="line">java类型   mysql类型        成功与否</span><br><span class="line">date         date               yes</span><br><span class="line">date         time               no</span><br><span class="line">date         timestamp       no</span><br><span class="line">date         datetime         no</span><br><span class="line"></span><br><span class="line">time         date               no</span><br><span class="line">time         time               yes</span><br><span class="line">time         timestamp       no</span><br><span class="line">time         datetime         no</span><br><span class="line"></span><br><span class="line">timestamp date              yes</span><br><span class="line">timestamp time              yes</span><br><span class="line">timestamp timestamp     yes</span><br><span class="line">timestamp datetime        yes</span><br><span class="line">==========end java注入数据库========</span><br><span class="line">总规律，如果A完全包含B，则A可以向B注入数据，否则报错</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">==========从数据库提取到java ==========</span><br><span class="line"></span><br><span class="line">mysql类型    java类型     成与否</span><br><span class="line">date             date         yes</span><br><span class="line">date             time         yes --------------缺少的部分使用历元</span><br><span class="line">date           timestamp   yes --------------缺少的部分使用历元  </span><br><span class="line"></span><br><span class="line">time           date           yes --------------缺少的部分使用历元</span><br><span class="line">time           time           yes</span><br><span class="line">time          timestamp    yes --------------缺少的部分使用历元</span><br><span class="line"></span><br><span class="line">timestamp date           yes</span><br><span class="line">timestamp time           yes</span><br><span class="line">timestamp timestamp   yes</span><br><span class="line"></span><br><span class="line">datetime      date         yes</span><br><span class="line">datetime      time         yes</span><br><span class="line">datetime    timestamp   yes</span><br><span class="line">==========end 从数据库提取到java=======</span><br><span class="line">不会出错，缺少的部分使用历元，而不是当前日期时间</span><br></pre></td></tr></table></figure></p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">实战经验：在java中处理时间数据，建议采用java8中的时间工具类</span><br><span class="line">localtime</span><br><span class="line">localdate</span><br><span class="line">localdatetime</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在spark中处理时间的字段类型采用timestamp来计算，不要使用date类型计算</span><br><span class="line">mysql数据库中的日期类型字段采用date类型</span><br><span class="line">时间字段采用datetime 或者timestamp都可以，避免使用字符串，看起来更加正规。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一、postman接口测试工具如何接收java.util.date参数&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;时间格式采用：2020/04/11 14:12:14&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;二、java 参数类型与mysql类型比较&lt;br&gt;&lt;figure 
      
    
    </summary>
    
      <category term="后台" scheme="http://yoursite.com/categories/%E5%90%8E%E5%8F%B0/"/>
    
    
      <category term="postman" scheme="http://yoursite.com/tags/postman/"/>
    
  </entry>
  
  <entry>
    <title>flink数据处理</title>
    <link href="http://yoursite.com/2020/04/09/flink%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2020/04/09/flink数据处理/</id>
    <published>2020-04-09T01:54:23.000Z</published>
    <updated>2020-04-09T02:09:11.969Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">package com.gree.bdc.device;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import com.google.gson.Gson;</span><br><span class="line">import com.gree.bdc.entity.DevicePosition;</span><br><span class="line">import com.gree.bdc.position.LongSpot;</span><br><span class="line">import com.gree.bdc.util.InitFlinkUtils;</span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeHint;</span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line">import org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple4;</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;</span><br><span class="line">import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;</span><br><span class="line">import org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"></span><br><span class="line">import java.sql.Timestamp;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 设备久置点计算</span><br><span class="line"> * @author hadoop</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public class DeviceLongSpotInfo &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        System.setProperty(&quot;java.security.auth.login.config&quot;,&quot;/home/bdcopt/spark/jaas.conf&quot;);</span><br><span class="line">        System.setProperty(&quot;java.security.krb5.conf&quot;,&quot;/home/bdcopt/spark/krb5.conf&quot;);</span><br><span class="line"></span><br><span class="line">        StreamExecutionEnvironment env = InitFlinkUtils.getEnv();</span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"></span><br><span class="line">        //读取kafka数据源</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;,&quot;kafka01:30001,kafka02:30002,kafka03:30003,kafka04:30004,kafka05:30005&quot;);</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;,&quot;test&quot;);</span><br><span class="line">        properties.setProperty(&quot;auto.offset.reset&quot;,&quot;latest&quot;);</span><br><span class="line">        properties.setProperty(&quot;security.protocol&quot;,&quot;SASL_PLAINTEXT&quot;);</span><br><span class="line">        properties.setProperty(&quot;sasl.kerberos.service.name&quot;,&quot;kafka&quot;);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkatopic0619 =</span><br><span class="line">                new FlinkKafkaConsumer&lt;String&gt;(&quot;test&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">        kafkatopic0619.setStartFromLatest();</span><br><span class="line">        env.setParallelism(4);</span><br><span class="line">        DataStreamSource&lt;String&gt; sourceStream = env.addSource(kafkatopic0619);</span><br><span class="line"></span><br><span class="line">        //数据处理：</span><br><span class="line">        //1.按照设备id进行分组计算，按照每个久置点周期为一个窗口大小</span><br><span class="line">        // 2.移动距离为每个上报频率为滑动窗口的距离，计算在这个窗口的上报的设备点的距离与第一个设备的点的距离在</span><br><span class="line">        //可以接受的范围，那么这个点就是一个久置点。</span><br><span class="line"></span><br><span class="line">        sourceStream.map(value -&gt; &#123;</span><br><span class="line">            Gson gson = new Gson();</span><br><span class="line">            DevicePosition devicePosition = gson.fromJson(value, DevicePosition.class);</span><br><span class="line">            String imei = devicePosition.getImei();</span><br><span class="line">            Timestamp positionTime = devicePosition.getPosition_time();</span><br><span class="line">            String longitude = devicePosition.getLongitude();</span><br><span class="line">            String latitude = devicePosition.getLatitude();</span><br><span class="line">            return new Tuple4&lt;&gt;(positionTime,imei,latitude,longitude);</span><br><span class="line">        &#125;).returns(TypeInformation.of(new TypeHint&lt;Tuple4&lt;Timestamp, String, String, String&gt;&gt;() &#123;</span><br><span class="line">        &#125;)).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;Tuple4&lt;Timestamp, String, String, String&gt;&gt;(Time.minutes(1)) &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public long extractTimestamp(Tuple4&lt;Timestamp, String, String, String&gt; element) &#123;</span><br><span class="line">                return element.f0.getTime();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).keyBy((KeySelector&lt;Tuple4&lt;Timestamp,String,String,String&gt;,String&gt;) map -&gt; map.f1)</span><br><span class="line">        .window(SlidingEventTimeWindows.of(Time.minutes(30L),Time.minutes(3L)))</span><br><span class="line">        .process(new LongSpot());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line">package com.gree.bdc;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import com.google.gson.Gson;</span><br><span class="line">import com.gree.bdc.entity.DeviceLongSpot;</span><br><span class="line">import com.gree.bdc.entity.DevicePosition;</span><br><span class="line">import com.gree.bdc.entity.DeviceResidenceTime;</span><br><span class="line">import com.gree.bdc.position.LongSpotNew;</span><br><span class="line">import com.gree.bdc.sink.MysqlSink;</span><br><span class="line">import com.gree.bdc.util.DateUtils;</span><br><span class="line">import com.gree.bdc.util.InitFlinkUtils;</span><br><span class="line">import com.gree.bdc.util.MapUtils;</span><br><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.common.functions.RichFlatMapFunction;</span><br><span class="line">import org.apache.flink.api.common.functions.RichMapFunction;</span><br><span class="line">import org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line">import org.apache.flink.api.common.state.StateTtlConfig;</span><br><span class="line">import org.apache.flink.api.common.state.ValueState;</span><br><span class="line">import org.apache.flink.api.common.state.ValueStateDescriptor;</span><br><span class="line">import org.apache.flink.api.common.time.Time;</span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeHint;</span><br><span class="line">import org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line">import org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple4;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple6;</span><br><span class="line">import org.apache.flink.configuration.Configuration;</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line">import org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line">import java.sql.Timestamp;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">public class ReadKafkaTest &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        StreamExecutionEnvironment env = InitFlinkUtils.getEnv();</span><br><span class="line">        env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        //properties.setProperty(&quot;bootstrap.servers&quot;,&quot;kafka01:30001,kafka02:30002,kafka03:30003,kafka04:30004,kafka05:30005&quot;);</span><br><span class="line">        properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);</span><br><span class="line">        properties.setProperty(&quot;group.id&quot;,&quot;test&quot;);</span><br><span class="line">        FlinkKafkaConsumer&lt;String&gt; kafkatopic0619 =</span><br><span class="line">                new FlinkKafkaConsumer&lt;String&gt;(&quot;test08&quot;, new SimpleStringSchema(), properties);</span><br><span class="line">        kafkatopic0619.setStartFromLatest();</span><br><span class="line">        DataStreamSource&lt;String&gt; sourceStream = env.addSource(kafkatopic0619);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple6&lt;String,Timestamp,String,Double,Double,Long&gt;&gt; map = sourceStream.map(value -&gt; &#123;</span><br><span class="line">            Gson gson = new Gson();</span><br><span class="line">            DevicePosition devicePosition = gson.fromJson(value, DevicePosition.class);</span><br><span class="line">            String imei = devicePosition.getImei();</span><br><span class="line">            Timestamp positionTime = devicePosition.getPosition_time();</span><br><span class="line">            String longitude = devicePosition.getLongitude();</span><br><span class="line">            String latitude = devicePosition.getLatitude();</span><br><span class="line">            return new Tuple4&lt;&gt;(positionTime, imei, latitude, longitude);</span><br><span class="line">        &#125;).returns(TypeInformation.of(new TypeHint&lt;Tuple4&lt;Timestamp, String, String, String&gt;&gt;() &#123;</span><br><span class="line">        &#125;)).keyBy(1)</span><br><span class="line">                .map(new RichMapFunction&lt;Tuple4&lt;Timestamp, String, String, String&gt;, Tuple6&lt;String,Timestamp,String,Double,Double,Long&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    private ValueState&lt;Tuple4&lt;Timestamp, String, String, String&gt;&gt; valueState;</span><br><span class="line"></span><br><span class="line">                    @Override</span><br><span class="line">                    public Tuple6&lt;String,Timestamp,String,Double,Double,Long&gt; map(Tuple4&lt;Timestamp, String, String, String&gt; value)</span><br><span class="line">                            throws Exception &#123;</span><br><span class="line">                        Tuple4&lt;Timestamp, String, String, String&gt; currentValue = valueState.value();</span><br><span class="line"></span><br><span class="line">                        System.out.println(&quot;刚开始的状态 ： &quot; + currentValue);</span><br><span class="line"></span><br><span class="line">                        if (currentValue == null) &#123;</span><br><span class="line">                            currentValue = Tuple4.of(value.f0, value.f1, value.f2, value.f3);</span><br><span class="line">                            valueState.update(currentValue);</span><br><span class="line">                        &#125;</span><br><span class="line">                        double currentLat = Double.parseDouble(currentValue.f2);</span><br><span class="line">                        double currentLong = Double.parseDouble(currentValue.f3);</span><br><span class="line">                        Timestamp currentTime = currentValue.f0;</span><br><span class="line"></span><br><span class="line">                        double valueLat = Double.parseDouble(value.f2);</span><br><span class="line">                        double valueLong = Double.parseDouble(value.f3);</span><br><span class="line">                        Timestamp valueTime = value.f0;</span><br><span class="line"></span><br><span class="line">                        Double distanceCalculate = MapUtils.distanceCalculate(currentLat, currentLong, valueLat, valueLong);</span><br><span class="line"></span><br><span class="line">                        Long min = DateUtils.dateSubtraction(currentTime.toString(), valueTime.toString());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                        System.out.println(distanceCalculate);</span><br><span class="line">                        System.out.println(&quot;状态点： &quot; + currentValue);</span><br><span class="line">                        System.out.println(&quot;当前数据： &quot; + value);</span><br><span class="line"></span><br><span class="line">                        if (distanceCalculate &gt; 500) &#123;</span><br><span class="line">                            currentValue.f0 = value.f0;</span><br><span class="line">                            currentValue.f1 = value.f1;</span><br><span class="line">                            currentValue.f2 = value.f2;</span><br><span class="line">                            currentValue.f3 = value.f3;</span><br><span class="line">                            valueState.update(currentValue);</span><br><span class="line">                            if (min &gt; 30) &#123;</span><br><span class="line">                                return Tuple6.of(&quot;久置点&quot;, currentTime, value.f1, currentLat, currentLong, min);</span><br><span class="line">                            &#125; else &#123;</span><br><span class="line">                                return Tuple6.of(&quot;非久置点&quot;, currentTime, value.f1, currentLat, currentLong, min);</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        return Tuple6.of(&quot;邻域点&quot;, currentTime, value.f1, currentLat, currentLong, min);</span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    @Override</span><br><span class="line">                    public void open(Configuration parameters) throws Exception &#123;</span><br><span class="line">                        super.open(parameters);</span><br><span class="line"></span><br><span class="line">                        // keyedState可以设置TTL过期时间</span><br><span class="line">//                        StateTtlConfig config= StateTtlConfig</span><br><span class="line">//                                .newBuilder(Time.seconds(30))</span><br><span class="line">//                                .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)</span><br><span class="line">//                                .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)</span><br><span class="line">//                                .build();</span><br><span class="line"></span><br><span class="line">                        ValueStateDescriptor valueStateDescriptor = new ValueStateDescriptor(&quot;agvKeyedState&quot;,</span><br><span class="line">                                TypeInformation.of(new TypeHint&lt;Tuple4&lt;Timestamp, String, String, String&gt;&gt;() &#123;</span><br><span class="line">                                &#125;));</span><br><span class="line"></span><br><span class="line">                        //设置支持TTL配置</span><br><span class="line">//                        valueStateDescriptor.enableTimeToLive(config);</span><br><span class="line"></span><br><span class="line">                        valueState = getRuntimeContext().getState(valueStateDescriptor);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple6&lt;String, Timestamp, String, Double, Double, Long&gt;&gt; filter = map.filter(value -&gt; &#123;</span><br><span class="line">            return value.f0.equals(&quot;久置点&quot;);</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        filter.addSink(new MysqlSink());</span><br><span class="line"></span><br><span class="line">        env.execute(&quot;read-kafka-test&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain hljs&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>spark 读取 kafka 并写入mysql</title>
    <link href="http://yoursite.com/2020/04/08/spark-%E8%AF%BB%E5%8F%96-kafka-%E5%B9%B6%E5%86%99%E5%85%A5mysql/"/>
    <id>http://yoursite.com/2020/04/08/spark-读取-kafka-并写入mysql/</id>
    <published>2020-04-08T01:46:21.000Z</published>
    <updated>2020-04-20T06:31:58.151Z</updated>
    
    <content type="html"><![CDATA[<h3 id="spark-maven-依赖"><a href="#spark-maven-依赖" class="headerlink" title="spark maven 依赖"></a>spark maven 依赖</h3><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;</span><br><span class="line">         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;org.gree.bdc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;dispatch-kafka-installer&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">    &lt;properties&gt;</span><br><span class="line">        &lt;spark.version&gt;2.4.0&lt;/spark.version&gt;</span><br><span class="line">    &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!-- spark start--&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;!-- spark end --&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!-- structStreaming kafka --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!-- sparkStreaming kafka --&gt;</span><br><span class="line">        &lt;!-- 建议新集群使用sparkStreaming处理kafka数据--&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.4.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!-- 数据库连接--&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;8.0.12&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!-- 数据库连接 end --&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!-- 常用工具 --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;lombok&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.18.2&lt;/version&gt;</span><br><span class="line">            &lt;optional&gt;true&lt;/optional&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.2.58&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;!-- 常用工具 end --&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.5.5&lt;/version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;/mainClass&gt;</span><br><span class="line">                        &lt;/manifest&gt;</span><br><span class="line">                    &lt;/archive&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</span><br><span class="line">                    &lt;/descriptorRefs&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;/id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;/goal&gt;</span><br><span class="line">                        &lt;/goals&gt;</span><br><span class="line">                    &lt;/execution&gt;</span><br><span class="line">                &lt;/executions&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.3.1&lt;/version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/build&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure><h3 id="读取kafka-写入mysql"><a href="#读取kafka-写入mysql" class="headerlink" title="读取kafka ,写入mysql"></a>读取kafka ,写入mysql</h3><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line">package com.gree.bdc.installer;</span><br><span class="line"></span><br><span class="line">import com.alibaba.fastjson.JSON;</span><br><span class="line">import com.gree.bdc.entity.InstallerInfo;</span><br><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line">import org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line">import org.apache.spark.api.java.JavaRDD;</span><br><span class="line">import org.apache.spark.sql.*;</span><br><span class="line">import org.apache.spark.streaming.Durations;</span><br><span class="line">import org.apache.spark.streaming.api.java.JavaInputDStream;</span><br><span class="line">import org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line">import org.apache.spark.streaming.kafka010.ConsumerStrategies;</span><br><span class="line">import org.apache.spark.streaming.kafka010.KafkaUtils;</span><br><span class="line">import org.apache.spark.streaming.kafka010.LocationStrategies;</span><br><span class="line"></span><br><span class="line">import java.util.*;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 读取kafka</span><br><span class="line"> * @author hadoop</span><br><span class="line"> * @date 2020-03-26</span><br><span class="line"> */</span><br><span class="line">public class ReadKafkaInstaller &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        ///本地测试</span><br><span class="line">        //System.setProperty(&quot;java.security.auth.login.config&quot;,  &quot;E:\\workspace\\dispatch-kafka-data\\src\\main\\resources\\jaas.conf&quot;);</span><br><span class="line">        //System.setProperty(&quot;java.security.krb5.conf&quot;, &quot;E:\\workspace\\dispatch-kafka-data\\src\\main\\resources\\krb5.conf&quot;);</span><br><span class="line"></span><br><span class="line">        //服务器上跑</span><br><span class="line">        System.setProperty(&quot;java.security.auth.login.config&quot;,&quot;/home/bdcopt/spark/jaas.conf&quot;);</span><br><span class="line">        System.setProperty(&quot;java.security.krb5.conf&quot;,&quot;/home/bdcopt/spark/krb5.conf&quot;);</span><br><span class="line"></span><br><span class="line">        SparkSession spark = SparkSession.builder()</span><br><span class="line">                .appName(&quot;dispatch-installer&quot;)</span><br><span class="line">                .master(&quot;local[2]&quot;)</span><br><span class="line">                .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">                .config( &quot;spark.driver.allowMultipleContexts&quot;,true )</span><br><span class="line">                .getOrCreate();</span><br><span class="line"></span><br><span class="line">        // 读取kafka配置</span><br><span class="line">        Map&lt;String,Object&gt; kafkaParams = new HashMap&lt;&gt;( 16);</span><br><span class="line">        kafkaParams.put( &quot;bootstrap.servers&quot;,&quot;kafka01:30001,kafka02:30002,kafka03:30003,kafka04:30004,kafka05:30005&quot; );</span><br><span class="line">        kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class);</span><br><span class="line">        kafkaParams.put(&quot;value.deserializer&quot;, StringDeserializer.class);</span><br><span class="line">        kafkaParams.put(&quot;group.id&quot;, &quot;dispatchInstaller297&quot;);</span><br><span class="line">        kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;);</span><br><span class="line">        kafkaParams.put(&quot;security.protocol&quot;,&quot;SASL_PLAINTEXT&quot; );</span><br><span class="line">        kafkaParams.put(&quot;sasl.kerberos.service.name&quot;,&quot;kafka&quot; );</span><br><span class="line">        kafkaParams.put(&quot;enable.auto.commit&quot;, true);</span><br><span class="line"></span><br><span class="line">        Collection&lt;String&gt; topics = Collections.singletonList(&quot;dispatchInstaller&quot;);</span><br><span class="line">        JavaStreamingContext javaStreamingContext = new JavaStreamingContext( spark.sparkContext().conf(),</span><br><span class="line">                Durations.seconds( 1 ));</span><br><span class="line"></span><br><span class="line">        JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; stream =</span><br><span class="line">                KafkaUtils.createDirectStream(</span><br><span class="line">                        javaStreamingContext,</span><br><span class="line">                        LocationStrategies.PreferConsistent(),</span><br><span class="line">                        ConsumerStrategies.Subscribe(topics, kafkaParams)</span><br><span class="line">                );</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //数据遍历写入mysql</span><br><span class="line">        stream.foreachRDD((rdd,time) -&gt;&#123;</span><br><span class="line">            JavaRDD&lt;InstallerInfo&gt; installerInfoJavaRdd = rdd.map(stringValue -&gt; JSON.parseObject(stringValue.value(), InstallerInfo.class));</span><br><span class="line">            Dataset&lt;Row&gt; installerInfoDataFrame = spark.createDataFrame(installerInfoJavaRdd, InstallerInfo.class);</span><br><span class="line">///           String url = &quot;jdbc:mysql://localhost:3306/gpsdev?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot;;</span><br><span class="line">//            //rm-wz9k56kkaxj0e9mct33150.mysql.rds.aliyuncs.com</span><br><span class="line">//            String tableName = &quot;assign_ls_lc&quot;;</span><br><span class="line">//            Properties prop = new Properties();</span><br><span class="line">//            prop.put(&quot;user&quot;,&quot;root&quot;);</span><br><span class="line">//            prop.put(&quot;password&quot;,&quot;123456&quot;);</span><br><span class="line">//            prop.put(&quot;driver&quot;,&quot;com.mysql.cj.jdbc.Driver&quot;);</span><br><span class="line"></span><br><span class="line">            //新增自增id</span><br><span class="line">//            StructType schema = installerInfoDataFrame.schema().add(DataTypes.createStructField(&quot;id&quot;, DataTypes.LongType, false));</span><br><span class="line">//            JavaRDD&lt;Row&gt; installerInfoDataFrameRdd = installerInfoDataFrame</span><br><span class="line">//                    .javaRDD() // 转为JavaRDD</span><br><span class="line">//                    .zipWithIndex() // 添加索引，结果为JavaPairRDD&lt;Row, Long&gt;，即行数据和对应的索引</span><br><span class="line">//                    .map(new Function&lt;Tuple2&lt;Row, Long&gt;, Row&gt;() &#123;</span><br><span class="line">//                        @Override</span><br><span class="line">//                        public Row call(Tuple2&lt;Row, Long&gt; v1) throws Exception &#123;</span><br><span class="line">//                            Object[] objects = new Object[v1._1.size() + 1];</span><br><span class="line">//                            for (int i = 0; i &lt; v1._1.size(); i++) &#123;</span><br><span class="line">//                                objects[i] = v1._1.get(i);</span><br><span class="line">//                            &#125;</span><br><span class="line">//                            objects[objects.length - 1] = v1._2;</span><br><span class="line">//                            return RowFactory.create(objects);</span><br><span class="line">//                        &#125;</span><br><span class="line">//                    &#125;); // 把索引值作为ID字段值，构造新的行数据</span><br><span class="line"></span><br><span class="line">//            installerInfoDataFrame = spark.createDataFrame(installerInfoDataFrameRdd, schema);</span><br><span class="line"></span><br><span class="line">            String url = &quot;jdbc:mysql://localhost:3306/paigong_map?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot;;</span><br><span class="line">            String tableName = &quot;t_installer_original_info&quot;;</span><br><span class="line">            Properties prop = new Properties();</span><br><span class="line">            prop.put(&quot;user&quot;,&quot;paigong_map&quot;);</span><br><span class="line">            prop.put(&quot;password&quot;,&quot;paigong_map@dept1&quot;);</span><br><span class="line">            prop.put(&quot;driver&quot;,&quot;com.mysql.cj.jdbc.Driver&quot;);</span><br><span class="line">            installerInfoDataFrame.write().mode( SaveMode.Append).jdbc(url,tableName,prop);</span><br><span class="line"></span><br><span class="line">        &#125;);</span><br><span class="line">        //启动程序</span><br><span class="line">        javaStreamingContext.start();</span><br><span class="line">        try &#123;</span><br><span class="line">            javaStreamingContext.awaitTermination();</span><br><span class="line">        &#125; catch (InterruptedException e) &#123;</span><br><span class="line">// TODO Auto-generated catch block</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>采用上述方式写入mysql 速度较快，但是也有局限性，它仅适用于新增的模式，大批量的导入，对于数据量较小，而且需要更灵活导入（如 更新操作）时建议采用原生的kafka消费者</p></blockquote><h3 id="kafka-consumer"><a href="#kafka-consumer" class="headerlink" title="kafka consumer"></a>kafka consumer</h3><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;</span><br><span class="line">         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;com.gree.bdc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;dispatch-kafka-order-consumer&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;!--引入kafka-client依赖--&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.1.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.2.62&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;lombok&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.18.2&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &lt;!-- 数据库连接--&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;druid&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.1.12&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;5.1.44&lt;/version&gt;</span><br><span class="line">            &lt;scope&gt;runtime&lt;/scope&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;!-- 数据库连接 end --&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.5.5&lt;/version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;mainClass&gt;&lt;/mainClass&gt;</span><br><span class="line">                        &lt;/manifest&gt;</span><br><span class="line">                    &lt;/archive&gt;</span><br><span class="line">                    &lt;descriptorRefs&gt;</span><br><span class="line">                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</span><br><span class="line">                    &lt;/descriptorRefs&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;/id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;/goal&gt;</span><br><span class="line">                        &lt;/goals&gt;</span><br><span class="line">                    &lt;/execution&gt;</span><br><span class="line">                &lt;/executions&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.3.1&lt;/version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/build&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure><p>consumer.properties<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#配置消费者集群</span><br><span class="line">bootstrap.servers=kafka01:30001</span><br><span class="line">#消费者的组ID，</span><br><span class="line">group.id=</span><br><span class="line">enable.auto.commit=true</span><br><span class="line">session.timeout.ms=300000</span><br><span class="line">#消费策略</span><br><span class="line">auto.offset.reset=earliest</span><br><span class="line">#实现了Serializer接口的序列化类。用于告诉kafka如何序列化key</span><br><span class="line">key.deserializer=org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">#实现了Serializer接口的序列化类。用于告诉kafka如何序列化value</span><br><span class="line">value.deserializer=org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">security.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.kerberos.service.name=kafka</span><br></pre></td></tr></table></figure></p><p>druid.properties<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">driverClassName=com.mysql.jdbc.Driver</span><br><span class="line">url=jdbc:mysql://localhost:3306/test?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false</span><br><span class="line">username=root</span><br><span class="line">password=123456</span><br><span class="line">initialSize=5</span><br><span class="line">maxActive=20</span><br><span class="line">maxWait=3000</span><br><span class="line">minIdle=3</span><br></pre></td></tr></table></figure></p><p>jaas.conf<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">KafkaClient &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  doNotPrompt=true</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  storeKey=true</span><br><span class="line">  renewTicket=true</span><br><span class="line">  keyTab=&quot;E:\\test.keytab&quot;</span><br><span class="line">  principal=&quot;test&quot;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Client &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  storeKey=true</span><br><span class="line">  keyTab=&quot;test.keytab&quot;</span><br><span class="line">  principal=&quot;test&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>krb5.conf等配置文件<br>创建消费者<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">package com.gree.bdc.consumer;</span><br><span class="line"></span><br><span class="line">import org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @author :yelinlin()</span><br><span class="line"> * @date :2019/3/10</span><br><span class="line"> * @description :消费者</span><br><span class="line"> */</span><br><span class="line">public class Consumer &#123;</span><br><span class="line">    /**</span><br><span class="line">     * 获得一个Kafka消费者</span><br><span class="line">     * @return kafkaConsumer</span><br><span class="line">     */</span><br><span class="line">    public KafkaConsumer&lt;String,String&gt; getConsumer ()&#123;</span><br><span class="line">        //配置读取配置文件</span><br><span class="line">        System.setProperty(&quot;java.security.auth.login.config&quot;,  &quot;/home/bdcopt/spark/jaas.conf&quot;);</span><br><span class="line">        System.setProperty(&quot;java.security.krb5.conf&quot;, &quot;/home/bdcopt/spark/krb5.conf&quot;);</span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        try &#123;</span><br><span class="line">            //获取kafka消费者配置信息</span><br><span class="line">            props.load(this.getClass().getResourceAsStream(&quot;/consumer.properties&quot;));</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return new KafkaConsumer&lt;String,String&gt;(props);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>读取数据写入mysql</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">package com.gree.bdc.order;</span><br><span class="line"></span><br><span class="line">import com.alibaba.fastjson.JSON;</span><br><span class="line">import com.gree.bdc.consumer.Consumer;</span><br><span class="line">import com.gree.bdc.entity.DispatchInfo;</span><br><span class="line">import com.gree.bdc.util.DateUtils;</span><br><span class="line">import com.gree.bdc.util.DruidPoolUtils;</span><br><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line">import org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line">import org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line">import org.slf4j.Logger;</span><br><span class="line">import org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.PreparedStatement;</span><br><span class="line">import java.sql.SQLException;</span><br><span class="line">import java.text.ParseException;</span><br><span class="line">import java.util.Collections;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * @author yelinlin (260269)</span><br><span class="line"> * @createTime 2019/11/21</span><br><span class="line"> * @description kafka消费者启动类</span><br><span class="line"> */</span><br><span class="line">public class OrderConsumerStarter &#123;</span><br><span class="line">    private static final String TOPIC=&quot;test&quot;;</span><br><span class="line">    private static final Logger logger= LoggerFactory.getLogger(OrderConsumerStarter.class);</span><br><span class="line">    public static void main(String[] args) throws SQLException, ParseException &#123;</span><br><span class="line">        Connection connection;</span><br><span class="line">        PreparedStatement statement;</span><br><span class="line">        //获取一个消费者</span><br><span class="line">        KafkaConsumer&lt;String,String&gt; consumer = new Consumer().getConsumer();</span><br><span class="line">        consumer.subscribe(Collections.singletonList(TOPIC));</span><br><span class="line">        //消费并打印消费结果</span><br><span class="line">        while (true) &#123;</span><br><span class="line">            //进行消费</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);</span><br><span class="line">            for (ConsumerRecord&lt;String,String&gt; record: records) &#123;</span><br><span class="line">                connection = DruidPoolUtils.getConnection();</span><br><span class="line">                DispatchInfo dispatchInfo = JSON.parseObject(record.value(), DispatchInfo.class);</span><br><span class="line">                //写入mysql</span><br><span class="line">                statement = connection.prepareStatement(&quot;&quot; +</span><br><span class="line">                        &quot;insert into paigong_map.t_dispatch_info (customer_address,customer_name,&quot; +</span><br><span class="line">                        &quot; customer_tel,dispatching_time,finish_time,installation_number,order_number,&quot; +</span><br><span class="line">                        &quot; unfinish_number,work_number) values(?,?,?,?,?,?,?,?,?)&quot;);</span><br><span class="line">                statement.setString(1,dispatchInfo.getCustomer_address());</span><br><span class="line">                statement.setString(2,dispatchInfo.getCustomer_name());</span><br><span class="line">                statement.setString(3,dispatchInfo.getCustomer_tel());</span><br><span class="line">                statement.setString(4,DateUtils.dealDateFormat(dispatchInfo.getDispatching_time()));</span><br><span class="line">                statement.setString(5,DateUtils.dealDateFormat(dispatchInfo.getFinish_time()));</span><br><span class="line">                statement.setDouble(6,dispatchInfo.getInstallation_number());</span><br><span class="line">                statement.setString(7,dispatchInfo.getOrder_number());</span><br><span class="line">                statement.setDouble(8,dispatchInfo.getUnfinish_number());</span><br><span class="line">                statement.setString(9,dispatchInfo.getWork_number().split(&quot;\\.&quot;)[0]);</span><br><span class="line">                statement.execute();</span><br><span class="line">                DruidPoolUtils.close(connection,statement);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>补充下数据库连接池工具类<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">package com.gree.bdc.util;</span><br><span class="line"></span><br><span class="line">import com.alibaba.druid.pool.DruidDataSourceFactory;</span><br><span class="line"></span><br><span class="line">import javax.sql.DataSource;</span><br><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line">import java.sql.SQLException;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line">import java.util.Properties;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Druid 数据库连接池</span><br><span class="line"> * @author hadoop</span><br><span class="line"> * @date 2020-04-06</span><br><span class="line"> */</span><br><span class="line">public class DruidPoolUtils &#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 创建成员变量,获取数据源</span><br><span class="line">     */</span><br><span class="line">    private static DataSource dataSource;</span><br><span class="line">    //加载配置文件</span><br><span class="line">    static &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            Properties properties = new Properties();</span><br><span class="line">//加载类路径</span><br><span class="line">            properties.load(DruidPoolUtils.class.getResourceAsStream(&quot;/druid.properties&quot;));</span><br><span class="line"></span><br><span class="line">//读取属性文件，创建连接池</span><br><span class="line">            dataSource = DruidDataSourceFactory.createDataSource(properties);</span><br><span class="line">        &#125;catch (Exception e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 获取数据源</span><br><span class="line">     * @return 数据源</span><br><span class="line">     */</span><br><span class="line">    public static DataSource getDataSource()&#123;</span><br><span class="line">        return dataSource;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 获取连接对象</span><br><span class="line">     * @return 连接对象</span><br><span class="line">     */</span><br><span class="line">    public static Connection getConnection()&#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            return dataSource.getConnection();</span><br><span class="line">        &#125;catch (SQLException e)&#123;</span><br><span class="line">            throw new RuntimeException(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //释放资源</span><br><span class="line"></span><br><span class="line">    private static void close(Connection connection, Statement statement, ResultSet resultSet)&#123;</span><br><span class="line">        if (resultSet != null)&#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                resultSet.close();</span><br><span class="line">            &#125; catch (SQLException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        if (statement != null)&#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                statement.close();</span><br><span class="line">            &#125;catch (SQLException e)&#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        if (connection != null)&#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                connection.close();</span><br><span class="line">            &#125;catch (SQLException e)&#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void close(Connection connection,Statement statement)&#123;</span><br><span class="line">        close(connection,statement,null);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="spark自定义批量写入mysql"><a href="#spark自定义批量写入mysql" class="headerlink" title="spark自定义批量写入mysql"></a>spark自定义批量写入mysql</h3><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"> //自定义批量写入mysql</span><br><span class="line">        dispatchChange.foreachPartition(new ForeachPartitionFunction&lt;Row&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public void call(Iterator&lt;Row&gt; t) throws Exception &#123;</span><br><span class="line">//                while(t.hasNext())&#123;</span><br><span class="line">//                    System.out.println(t.next());</span><br><span class="line">//                &#125;</span><br><span class="line">                PreparedStatement statement;</span><br><span class="line">                Connection connection = DruidPoolUtils.getConnection();</span><br><span class="line">                statement = connection.prepareStatement(&quot;replace into test&quot; +</span><br><span class="line">                        &quot;  (dispatching_time,order_number,imei,work_number,driver_name,license_plate,&quot; +</span><br><span class="line">                        &quot; installation_number,unfinish_number,customer_address,arrive_time,finish_time,&quot; +</span><br><span class="line">                        &quot; leave_time,total_time,average_time,order_status,affiliation,prod_id) values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)&quot;);</span><br><span class="line"></span><br><span class="line">                while(t.hasNext())&#123;</span><br><span class="line">                    Row next = t.next();</span><br><span class="line">                    statement.setTimestamp(1,next.getTimestamp(0));</span><br><span class="line">                    statement.setString(2,next.getString(1));</span><br><span class="line">                    statement.setString(3,next.getString(2));</span><br><span class="line">                    statement.setString(4,next.getString(3));</span><br><span class="line">                    statement.setString(5,next.getString(4));</span><br><span class="line">                    statement.setString(6,next.getString(5));</span><br><span class="line">                    statement.setDouble(7,next.getDouble(6));</span><br><span class="line">                    statement.setDouble(8,next.getDouble(7));</span><br><span class="line">                    statement.setString(9,next.getString(8));</span><br><span class="line">                    statement.setTimestamp(10,next.getTimestamp(9));</span><br><span class="line">                    statement.setTimestamp(11,next.getTimestamp(10));</span><br><span class="line">                    statement.setTimestamp(12,next.getTimestamp(11));</span><br><span class="line">                    statement.setDouble(13,next.getDouble(12));</span><br><span class="line">                    statement.setDouble(14,next.getDouble(13));</span><br><span class="line">                    statement.setInt(15,next.getInt(14));</span><br><span class="line">                    statement.setString(16,next.getString(15));</span><br><span class="line">                    statement.setString(17,next.getString(16));</span><br><span class="line"></span><br><span class="line">                    statement.addBatch();</span><br><span class="line">                &#125;</span><br><span class="line">                statement.executeBatch();</span><br><span class="line">                DruidPoolUtils.close(connection,statement);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;spark-maven-依赖&quot;&gt;&lt;a href=&quot;#spark-maven-依赖&quot; class=&quot;headerlink&quot; title=&quot;spark maven 依赖&quot;&gt;&lt;/a&gt;spark maven 依赖&lt;/h3&gt;&lt;figure class=&quot;highlight 
      
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>spark 常用操作</title>
    <link href="http://yoursite.com/2020/04/07/spark-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/2020/04/07/spark-常用操作/</id>
    <published>2020-04-07T11:28:53.000Z</published>
    <updated>2020-04-20T06:25:06.973Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、spark-分组计算（转rdd-分组计算完再转回-dateset）"><a href="#一、spark-分组计算（转rdd-分组计算完再转回-dateset）" class="headerlink" title="一、spark 分组计算（转rdd 分组计算完再转回 dateset）"></a>一、spark 分组计算（转rdd 分组计算完再转回 dateset）</h3><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Row&gt; rowJavaRdd = orderDistanceCustomer.javaRDD();</span><br><span class="line">JavaRDD&lt;OrderLeaveTime&gt; leaveTimeJavaRDD = rowJavaRdd.groupBy(row -&gt; row.getString(1))</span><br><span class="line">              .map(new Function&lt;Tuple2&lt;String, Iterable&lt;Row&gt;&gt;, OrderLeaveTime&gt;() &#123;</span><br><span class="line">                  @Override</span><br><span class="line">                  public OrderLeaveTime call(Tuple2&lt;String, Iterable&lt;Row&gt;&gt; v1) throws Exception &#123;</span><br><span class="line">                      String order_number = v1._1;</span><br><span class="line">                      Iterator&lt;Row&gt; iterator = v1._2.iterator();</span><br><span class="line">                      String report_time = &quot;&quot;;</span><br><span class="line">                      String flag = &quot;&quot;;</span><br><span class="line">                      while (iterator.hasNext()) &#123;</span><br><span class="line">                          Row next = iterator.next();</span><br><span class="line">                          double order_distance_customer = next.getDouble(14);</span><br><span class="line">                          report_time = next.getString(11);</span><br><span class="line">                          if (order_distance_customer - 2000 &lt; 0) &#123;</span><br><span class="line">                              flag = &quot;已进入&quot;;</span><br><span class="line">                          &#125;</span><br><span class="line">                          if (&quot;已进入&quot;.equals(flag) &amp;&amp; order_distance_customer - 2000 &gt; 0) &#123;</span><br><span class="line">                              break;</span><br><span class="line">                          &#125;</span><br><span class="line">                      &#125;</span><br><span class="line">                      OrderLeaveTime orderArriveTime1 = new OrderLeaveTime();</span><br><span class="line">                      orderArriveTime1.setOrderNumber(order_number);</span><br><span class="line">                      orderArriveTime1.setLeaveTime(report_time);</span><br><span class="line">                      return orderArriveTime1;</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;);</span><br><span class="line"></span><br><span class="line">      Encoder&lt;OrderLeaveTime&gt; orderLeaveTimeEncode = Encoders.bean(OrderLeaveTime.class);</span><br><span class="line">      Dataset&lt;OrderLeaveTime&gt; leaveTimeDataset = spark.createDataset(leaveTimeJavaRDD.rdd(),orderLeaveTimeEncode);</span><br></pre></td></tr></table></figure><h2 id="二、spark-dataset-分组计算（使用groupbyKey）"><a href="#二、spark-dataset-分组计算（使用groupbyKey）" class="headerlink" title="二、spark dataset 分组计算（使用groupbyKey）"></a>二、spark dataset 分组计算（使用groupbyKey）</h2><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;DeviceLocation&gt; deviceLocationDataset = devicePosition.groupByKey(new MapFunction&lt;Row, String&gt;() &#123;</span><br><span class="line">           @Override</span><br><span class="line">           public String call(Row value) throws Exception &#123;</span><br><span class="line">               return value.getString(2);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;, Encoders.STRING()).flatMapGroups(new FlatMapGroupsFunction&lt;String, Row, DeviceLocation&gt;() &#123;</span><br><span class="line">           @Override</span><br><span class="line">           public Iterator&lt;DeviceLocation&gt; call(String key, Iterator&lt;Row&gt; values) throws Exception &#123;</span><br><span class="line">               List&lt;DeviceLocation.Location&gt; locations = new ArrayList&lt;&gt;();</span><br><span class="line">               while (values.hasNext()) &#123;</span><br><span class="line">                   DeviceLocation.Location location = new DeviceLocation.Location();</span><br><span class="line">                   Row next = values.next();</span><br><span class="line">                   location.setId((int)next.getLong(0));</span><br><span class="line">                   location.setLocation_code(next.getString(1));</span><br><span class="line">                   location.setLongitude(next.getString(3));</span><br><span class="line">                   location.setLongmark(next.getString(4));</span><br><span class="line">                   location.setLatitude(next.getString(5));</span><br><span class="line">                   location.setLati_mark(next.getString(6));</span><br><span class="line">                   location.setPosition_time(next.getTimestamp(7));</span><br><span class="line">                   location.setSignal_strength(next.getString(8));</span><br><span class="line">                   location.setPower(next.getString(9));</span><br><span class="line">                   location.setAlarm_status(next.getString(10));</span><br><span class="line">                   location.setExtended_data(next.getString(11));</span><br><span class="line">                   locations.add(location);</span><br><span class="line">               &#125;</span><br><span class="line"></span><br><span class="line">               DeviceLocation deviceLocation = new DeviceLocation();</span><br><span class="line">               deviceLocation.setImei(key);</span><br><span class="line">               deviceLocation.setDateTime(&quot;2020-04-05&quot;);</span><br><span class="line">               deviceLocation.setLocation(JSON.toJSONString(locations));</span><br><span class="line">               ArrayList&lt;DeviceLocation&gt; deviceLocations = new ArrayList&lt;&gt;();</span><br><span class="line">               deviceLocations.add(deviceLocation);</span><br><span class="line">               return deviceLocations.iterator();</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;, Encoders.bean(DeviceLocation.class));</span><br><span class="line"></span><br><span class="line">       deviceLocationDataset.select(col(&quot;imei&quot;),col(&quot;dateTime&quot;),col(&quot;location&quot;).cast(&quot;String&quot;)).write().mode(SaveMode.Append).jdbc(url,&quot;test&quot;,properties);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h2 id="spark-分组计算"><a href="#spark-分组计算" class="headerlink" title="spark 分组计算"></a>spark 分组计算</h2><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">//Encoder&lt;OrderArriveTime&gt; orderArriveTimeEncoder = Encoders.bean(OrderArriveTime.class);</span><br><span class="line"></span><br><span class="line"> //通过转成list来遍历得到数据</span><br><span class="line">//        List&lt;OrderArriveTime&gt; list = new ArrayList&lt;&gt;();</span><br><span class="line">//        //效率太低，考虑自定义函数</span><br><span class="line">//        List&lt;OrderArriveTime&gt; orderArriveTimes = orderArriveTimeDataset.collectAsList();</span><br><span class="line">//        orderArriveTimes.stream().collect(Collectors.groupingBy(OrderArriveTime::getCustomerAddress))</span><br><span class="line">//                .forEach(new BiConsumer&lt;String, List&lt;OrderArriveTime&gt;&gt;() &#123;</span><br><span class="line">//                    @Override</span><br><span class="line">//                    public void accept(String s, List&lt;OrderArriveTime&gt; orderArriveTimes) &#123;</span><br><span class="line">//</span><br><span class="line">//                        String arriveTime = null;</span><br><span class="line">//                        String finishTime = null;</span><br><span class="line">//</span><br><span class="line">//                        for (OrderArriveTime next : orderArriveTimes) &#123;</span><br><span class="line">//                            OrderArriveTime orderArriveTime = new OrderArriveTime();</span><br><span class="line">//                            if (arriveTime == null) &#123;</span><br><span class="line">//                                arriveTime = next.getArriveTime();</span><br><span class="line">//                            &#125; else &#123;</span><br><span class="line">//                                arriveTime = finishTime;</span><br><span class="line">//                            &#125;</span><br><span class="line">//                            finishTime = next.getFinishTime();</span><br><span class="line">//                            orderArriveTime.setOrderNumber(next.getOrderNumber());</span><br><span class="line">//                            orderArriveTime.setCustomerAddress(next.getCustomerAddress());</span><br><span class="line">//                            orderArriveTime.setArriveTime(arriveTime);</span><br><span class="line">//                            orderArriveTime.setFinishTime(finishTime);</span><br><span class="line">//                            list.add(orderArriveTime);</span><br><span class="line">//                        &#125;</span><br><span class="line">//</span><br><span class="line">//                    &#125;</span><br><span class="line">//                &#125;);</span><br><span class="line">//</span><br><span class="line">//        Dataset&lt;OrderArriveTime&gt; realArriveDataset = spark.createDataset(list, orderArriveTimeEncoder);</span><br></pre></td></tr></table></figure><h3 id="spark-自定义函数"><a href="#spark-自定义函数" class="headerlink" title="spark 自定义函数"></a>spark 自定义函数</h3><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">时间格式转换自定义函数</span><br><span class="line">//        spark.udf().register(&quot;TimeFormat&quot;, new UDF1&lt;String, String&gt;() &#123;</span><br><span class="line">//            @Override</span><br><span class="line">//            public String call(String oldDateStr) throws Exception &#123;</span><br><span class="line">//                if (oldDateStr != null)&#123;</span><br><span class="line">//                    DateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd&apos;T&apos;HH:mm:ss.SSSXXX&quot;);</span><br><span class="line">//                    Date date = df.parse(oldDateStr);</span><br><span class="line">//                    SimpleDateFormat df1 = new SimpleDateFormat (&quot;EEE MMM dd HH:mm:ss Z yyyy&quot;, Locale.UK);</span><br><span class="line">//                    Date date1 =  df1.parse(date.toString());</span><br><span class="line">//                    DateFormat df2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);</span><br><span class="line">//                    return df2.format(date1);</span><br><span class="line">//                &#125;</span><br><span class="line">//              return null;</span><br><span class="line">//            &#125;</span><br><span class="line">//        &#125;, DataTypes.StringType);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> //自定义函数适应于多列数据通过逻辑处理得到一列数据的情况</span><br><span class="line"> //定义自定义函数</span><br><span class="line">        spark.udf().register(&quot;orderDistanceCustomer&quot;, (UDF4&lt;Double, Double, Double, Double, Double&gt;)</span><br><span class="line">                (lat1, lon1, lat2, lon2) -&gt; &#123;</span><br><span class="line">            if (lat1 != null &amp;&amp; lon1 != null &amp;&amp; lat2 != null &amp;&amp; lon2 != null)&#123;</span><br><span class="line">                return MapUtil.distanceCalculate(lat1, lon1, lat2, lon2);</span><br><span class="line">            &#125;else &#123;</span><br><span class="line">                return -1.0;</span><br><span class="line">            &#125; &#125;, DataTypes.DoubleType);</span><br><span class="line"></span><br><span class="line">   //使用自定义函数   </span><br><span class="line">        Dataset&lt;Row&gt; orderDistanceCustomer = orderLocalInfo.withColumn(&quot;order_distance_customer&quot;,</span><br><span class="line">                callUDF(&quot;orderDistanceCustomer&quot;,</span><br><span class="line">                orderLocalInfo.col(&quot;customer_lat&quot;), orderLocalInfo.col(&quot;customer_long&quot;),</span><br><span class="line">                orderLocalInfo.col(&quot;latitude&quot;), orderLocalInfo.col(&quot;longitude&quot;)));</span><br></pre></td></tr></table></figure><h3 id="spark提交命令"><a href="#spark提交命令" class="headerlink" title="spark提交命令"></a>spark提交命令</h3><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup spark-submit --class com.gree.bdc.analysis.DispatchDetailsInfo  --master local[2] --executor-memory 8G --total-executor-cores 4  /home/bdcopt/spark/order-details-handle-1.0-SNAPSHOT-jar-with-dependencies.jar  &gt; /tmp/spark/order/order-datails.log &amp;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、spark-分组计算（转rdd-分组计算完再转回-dateset）&quot;&gt;&lt;a href=&quot;#一、spark-分组计算（转rdd-分组计算完再转回-dateset）&quot; class=&quot;headerlink&quot; title=&quot;一、spark 分组计算（转rdd 分组计算
      
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>shiro</title>
    <link href="http://yoursite.com/2020/02/26/shiro/"/>
    <id>http://yoursite.com/2020/02/26/shiro/</id>
    <published>2020-02-26T02:31:17.000Z</published>
    <updated>2020-02-26T02:31:17.432Z</updated>
    
    <content type="html"><![CDATA[<h1 id="shiro"><a href="#shiro" class="headerlink" title="shiro"></a>shiro</h1><blockquote><p>Apache Shiro 是一个强大易用的 Java 安全框架，提供了认证、授权、加密和会话管理等功能，对于任何一个应用程序，Shiro 都可以提供全面的安全管理服务。</p></blockquote><p><img src="/images/2020/02/25/3fbc6bf0-5773-11ea-b312-7dd32fa3bd34.png" alt="image.png"></p><blockquote><p>Authentication：身份认证 / 登录，验证用户是不是拥有相应的身份；</p></blockquote><p>Authorization：授权，即权限验证，验证某个已认证的用户是否拥有某个权限；即判断用户是否能做事情，常见的如：验证某个用户是否拥有某个角色。或者细粒度的验证某个用户对某个资源是否具有某个权限；</p><p>Session Manager：会话管理，即用户登录后就是一次会话，在没有退出之前，它的所有信息都在会话中；会话可以是普通 JavaSE 环境的，也可以是如 Web 环境的；</p><p>Cryptography：加密，保护数据的安全性，如密码加密存储到数据库，而不是明文存储；</p><p>Web Support：Web 支持，可以非常容易的集成到 Web 环境；</p><p>Caching：缓存，比如用户登录后，其用户信息、拥有的角色 / 权限不必每次去查，这样可以提高效率；</p><p>Concurrency：shiro 支持多线程应用的并发验证，即如在一个线程中开启另一个线程，能把权限自动传播过去；</p><p>Testing：提供测试支持；</p><p>Run As：允许一个用户假装为另一个用户（如果他们允许）的身份进行访问；</p><p>Remember Me：记住我，这个是非常常见的功能，即一次登录后，下次再来的话不用登录了。</p><p><img src="/images/2020/02/25/6d86ae60-5773-11ea-b312-7dd32fa3bd34.png" alt="image.png"></p><blockquote><p>Subject：主体，代表了当前 “用户”，这个用户不一定是一个具体的人，与当前应用交互的任何东西都是 Subject，如网络爬虫，机器人等；即一个抽象概念；所有 Subject 都绑定到 SecurityManager，与 Subject 的所有交互都会委托给 SecurityManager；可以把 Subject 认为是一个门面；SecurityManager 才是实际的执行者；</p></blockquote><p>SecurityManager：安全管理器；即所有与安全有关的操作都会与 SecurityManager 交互；且它管理着所有 Subject；可以看出它是 Shiro 的核心，它负责与后边介绍的其他组件进行交互，如果学习过 SpringMVC，你可以把它看成 DispatcherServlet 前端控制器；</p><p>Realm：域，Shiro 从从 Realm 获取安全数据（如用户、角色、权限），就是说 SecurityManager 要验证用户身份，那么它需要从 Realm 获取相应的用户进行比较以确定用户身份是否合法；也需要从 Realm 得到用户相应的角色 / 权限进行验证用户是否能进行操作；可以把 Realm 看成 DataSource，即安全数据源。</p><p>参考：<a href="https://www.w3cschool.cn/shiro/" target="_blank" rel="noopener">跟我学shiro</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;shiro&quot;&gt;&lt;a href=&quot;#shiro&quot; class=&quot;headerlink&quot; title=&quot;shiro&quot;&gt;&lt;/a&gt;shiro&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Apache Shiro 是一个强大易用的 Java 安全框架，提供了认证、授权、加密和会
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>权限设计管理</title>
    <link href="http://yoursite.com/2020/02/24/%E6%9D%83%E9%99%90%E8%AE%BE%E8%AE%A1%E7%AE%A1%E7%90%86/"/>
    <id>http://yoursite.com/2020/02/24/权限设计管理/</id>
    <published>2020-02-24T01:10:01.000Z</published>
    <updated>2020-02-26T03:09:42.487Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基于RBAC（Resource-Based-Access-Control）"><a href="#基于RBAC（Resource-Based-Access-Control）" class="headerlink" title="基于RBAC（Resource-Based Access Control）"></a>基于RBAC（Resource-Based Access Control）</h2><h4 id="资源权限控制抽象为uri的访问控制（uri权限控制表）"><a href="#资源权限控制抽象为uri的访问控制（uri权限控制表）" class="headerlink" title="资源权限控制抽象为uri的访问控制（uri权限控制表）"></a>资源权限控制抽象为uri的访问控制（uri权限控制表）</h4><p>自增长id<br>uri(根据业务需要控制颗粒度，例如细可以精确到一条数据的访问，或者，粗可以只是控制一个页面的访问)<br>access_allow</p><h4 id="用户角色（user-role）"><a href="#用户角色（user-role）" class="headerlink" title="用户角色（user role）"></a>用户角色（user role）</h4><p>自增长id<br>用户角色描述</p><h4 id="用户组（user-group）"><a href="#用户组（user-group）" class="headerlink" title="用户组（user group）"></a>用户组（user group）</h4><p>自增长id<br>用户组描述</p><h4 id="用户（user）"><a href="#用户（user）" class="headerlink" title="用户（user）"></a>用户（user）</h4><p>自增长id<br>用户其他相关信息（用户名，password等等）</p><h4 id="用户角色（user-role）-uri访问权限集合（角色-uri权限关系表）"><a href="#用户角色（user-role）-uri访问权限集合（角色-uri权限关系表）" class="headerlink" title="用户角色（user role）-uri访问权限集合（角色-uri权限关系表）"></a>用户角色（user role）-uri访问权限集合（角色-uri权限关系表）</h4><p>用户角色id<br>uri权限id</p><h4 id="用户组（user-group）-uri访问权限集合（用户组-uri权限关系表）"><a href="#用户组（user-group）-uri访问权限集合（用户组-uri权限关系表）" class="headerlink" title="用户组（user group）-uri访问权限集合（用户组-uri权限关系表）"></a>用户组（user group）-uri访问权限集合（用户组-uri权限关系表）</h4><p>用户组id<br>uri权限id</p><h4 id="用户（user-）-uri访问权限集合（用户-uri权限关系表）"><a href="#用户（user-）-uri访问权限集合（用户-uri权限关系表）" class="headerlink" title="用户（user ）-uri访问权限集合（用户-uri权限关系表）"></a>用户（user ）-uri访问权限集合（用户-uri权限关系表）</h4><p>用户id<br>uri权限id</p><h4 id="用户组（user-group）-用户角色（user-role）集合（用户组-用户角色关系表）"><a href="#用户组（user-group）-用户角色（user-role）集合（用户组-用户角色关系表）" class="headerlink" title="用户组（user group）- 用户角色（user role）集合（用户组-用户角色关系表）"></a>用户组（user group）- 用户角色（user role）集合（用户组-用户角色关系表）</h4><p>用户角色id<br>用户组id</p><h4 id="用户（user）-用户组（user-group）集合（用户-用户组关系表）"><a href="#用户（user）-用户组（user-group）集合（用户-用户组关系表）" class="headerlink" title="用户（user）- 用户组（user group）集合（用户-用户组关系表）"></a>用户（user）- 用户组（user group）集合（用户-用户组关系表）</h4><p>用户id<br>用户组id</p><h4 id="用户（user）-用户角色（user-role）集合（用户-用户角色表）"><a href="#用户（user）-用户角色（user-role）集合（用户-用户角色表）" class="headerlink" title="用户（user）- 用户角色（user role）集合（用户-用户角色表）"></a>用户（user）- 用户角色（user role）集合（用户-用户角色表）</h4><p>用户角色id<br>用户id</p><p><img src="/images/2020/02/19/44fa5c10-52e8-11ea-8c8e-7b6faf42a2fd.png" alt="image.png"></p><p><img src="/images/2020/02/22/68cdca20-5516-11ea-a50a-c34d50658225.png" alt="image.png"></p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> Navicat Premium Data Transfer</span><br><span class="line"></span><br><span class="line"> Source Server         : 本地</span><br><span class="line"> Source Server Type    : MySQL</span><br><span class="line"> Source Server Version : 80012</span><br><span class="line"> Source Host           : localhost:3306</span><br><span class="line"> Source Schema         : gps_test</span><br><span class="line"></span><br><span class="line"> Target Server Type    : MySQL</span><br><span class="line"> Target Server Version : 80012</span><br><span class="line"> File Encoding         : 65001</span><br><span class="line"></span><br><span class="line"> Date: 21/02/2020 15:14:54</span><br><span class="line">*/</span><br><span class="line"></span><br><span class="line">SET NAMES utf8mb4;</span><br><span class="line">SET FOREIGN_KEY_CHECKS = 0;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for permission</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `permission`;</span><br><span class="line">CREATE TABLE `permission`  (</span><br><span class="line">  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;权限ID&apos;,</span><br><span class="line">  `parent_id` bigint(20) DEFAULT NULL COMMENT &apos;所属父级权限ID&apos;,</span><br><span class="line">  `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限唯一CODE代码&apos;,</span><br><span class="line">  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限名称&apos;,</span><br><span class="line">  `dsca` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限描述&apos;,</span><br><span class="line">  `category` tinyint(1) DEFAULT NULL COMMENT &apos;权限类别&apos;,</span><br><span class="line">  `uri` bigint(20) DEFAULT NULL COMMENT &apos;URL规则&apos;,</span><br><span class="line">  `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;,</span><br><span class="line">  `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;,</span><br><span class="line">  `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;,</span><br><span class="line">  `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;,</span><br><span class="line">  `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;,</span><br><span class="line">  PRIMARY KEY (`id`) USING BTREE,</span><br><span class="line">  INDEX `parent_id`(`parent_id`) USING BTREE COMMENT &apos;父级权限ID&apos;,</span><br><span class="line">  INDEX `code`(`code`) USING BTREE COMMENT &apos;权限CODE代码&apos;</span><br><span class="line">) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;权限&apos; ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for role</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `role`;</span><br><span class="line">CREATE TABLE `role`  (</span><br><span class="line">  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;角色ID&apos;,</span><br><span class="line">  `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色唯一CODE代码&apos;,</span><br><span class="line">  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色名称&apos;,</span><br><span class="line">  `desc` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色描述&apos;,</span><br><span class="line">  `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;,</span><br><span class="line">  `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;,</span><br><span class="line">  `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;,</span><br><span class="line">  `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更改者&apos;,</span><br><span class="line">  `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;,</span><br><span class="line">  `type` tinyint(1) NOT NULL DEFAULT 1 COMMENT &apos;0：超管、1：管理员、2：普通人员&apos;,</span><br><span class="line">  PRIMARY KEY (`id`) USING BTREE,</span><br><span class="line">  INDEX `code`(`code`) USING BTREE COMMENT &apos;权限CODE代码&apos;</span><br><span class="line">) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;角色&apos; ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for role_permission</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `role_permission`;</span><br><span class="line">CREATE TABLE `role_permission`  (</span><br><span class="line">  `role_id` int(11) NOT NULL COMMENT &apos;角色id&apos;,</span><br><span class="line">  `permission_id` int(11) DEFAULT NULL COMMENT &apos;权限id&apos;</span><br><span class="line">) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user`;</span><br><span class="line">CREATE TABLE `user`  (</span><br><span class="line">  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;用户ID&apos;,</span><br><span class="line">  `state` tinyint(1) DEFAULT NULL COMMENT &apos;用户状态:0=正常,1=禁用&apos;,</span><br><span class="line">  `user_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;姓名&apos;,</span><br><span class="line">  `user_tel_number` varchar(11) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;手机号码&apos;,</span><br><span class="line">  `salt` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;密码加盐&apos;,</span><br><span class="line">  `password` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;登录密码&apos;,</span><br><span class="line">  `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;,</span><br><span class="line">  `creator` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;,</span><br><span class="line">  `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;,</span><br><span class="line">  `regenerator` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;,</span><br><span class="line">  `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;,</span><br><span class="line">  PRIMARY KEY (`id`) USING BTREE</span><br><span class="line">) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;用户&apos; ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user_group</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user_group`;</span><br><span class="line">CREATE TABLE `user_group`  (</span><br><span class="line">  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;ID&apos;,</span><br><span class="line">  `parent_id` bigint(20) DEFAULT NULL COMMENT &apos;所属父级用户组ID&apos;,</span><br><span class="line">  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组名称&apos;,</span><br><span class="line">  `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组CODE唯一代码&apos;,</span><br><span class="line">  `desc` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组描述&apos;,</span><br><span class="line">  `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;,</span><br><span class="line">  `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;,</span><br><span class="line">  `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;,</span><br><span class="line">  `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;,</span><br><span class="line">  `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;,</span><br><span class="line">  PRIMARY KEY (`id`) USING BTREE,</span><br><span class="line">  INDEX `parent_id`(`parent_id`) USING BTREE COMMENT &apos;父级用户组ID&apos;,</span><br><span class="line">  INDEX `code`(`code`) USING BTREE COMMENT &apos;用户组CODE代码&apos;</span><br><span class="line">) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;用户组&apos; ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user_group_role</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user_group_role`;</span><br><span class="line">CREATE TABLE `user_group_role`  (</span><br><span class="line">  `user_group_id` int(11) NOT NULL COMMENT &apos;用户组id&apos;,</span><br><span class="line">  `role_id` int(11) DEFAULT NULL COMMENT &apos;角色id&apos;</span><br><span class="line">) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user_role</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user_role`;</span><br><span class="line">CREATE TABLE `user_role`  (</span><br><span class="line">  `user_id` int(11) NOT NULL COMMENT &apos;用户id&apos;,</span><br><span class="line">  `role_id` int(11) DEFAULT NULL COMMENT &apos;角色id&apos;</span><br><span class="line">) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user_user_group</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user_user_group`;</span><br><span class="line">CREATE TABLE `user_user_group`  (</span><br><span class="line">  `user_id` int(11) NOT NULL COMMENT &apos;用户id&apos;,</span><br><span class="line">  `user_group_id` int(11) DEFAULT NULL COMMENT &apos;用户组id&apos;</span><br><span class="line">) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">SET FOREIGN_KEY_CHECKS = 1;</span><br></pre></td></tr></table></figure><blockquote><p>第二版：权限设计表结构（主要新增了user_permission表 和 user_group_permission表 用于灵活的控制权限修改和授权操作，user_group 用户组表 parent_id属性的删除，主要区别于组织架构的系统，能够更加通用化设计）</p></blockquote><p><img src="/images/2020/02/24/e109f130-56a1-11ea-a50a-c34d50658225.png" alt="image.png"></p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> Navicat Premium Data Transfer</span><br><span class="line"></span><br><span class="line"> Source Server         : 本地</span><br><span class="line"> Source Server Type    : MySQL</span><br><span class="line"> Source Server Version : 80012</span><br><span class="line"> Source Host           : localhost:3306</span><br><span class="line"> Source Schema         : gps_test</span><br><span class="line"></span><br><span class="line"> Target Server Type    : MySQL</span><br><span class="line"> Target Server Version : 80012</span><br><span class="line"> File Encoding         : 65001</span><br><span class="line"></span><br><span class="line"> Date: 24/02/2020 08:51:04</span><br><span class="line">*/</span><br><span class="line"></span><br><span class="line">SET NAMES utf8mb4;</span><br><span class="line">SET FOREIGN_KEY_CHECKS = 0;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for permission</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `permission`;</span><br><span class="line">CREATE TABLE `permission`  (</span><br><span class="line">  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;权限ID&apos;,</span><br><span class="line">  `parent_id` bigint(20) DEFAULT NULL COMMENT &apos;所属父级权限ID&apos;,</span><br><span class="line">  `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限唯一CODE代码&apos;,</span><br><span class="line">  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限名称&apos;,</span><br><span class="line">  `dsca` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限描述&apos;,</span><br><span class="line">  `category` tinyint(1) DEFAULT NULL COMMENT &apos;权限类别&apos;,</span><br><span class="line">  `uri` bigint(20) DEFAULT NULL COMMENT &apos;URL规则&apos;,</span><br><span class="line">  `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;,</span><br><span class="line">  `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;,</span><br><span class="line">  `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;,</span><br><span class="line">  `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;,</span><br><span class="line">  `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;,</span><br><span class="line">  PRIMARY KEY (`id`) USING BTREE,</span><br><span class="line">  INDEX `parent_id`(`parent_id`) USING BTREE COMMENT &apos;父级权限ID&apos;,</span><br><span class="line">  INDEX `code`(`code`) USING BTREE COMMENT &apos;权限CODE代码&apos;</span><br><span class="line">) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;权限&apos; ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for role</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `role`;</span><br><span class="line">CREATE TABLE `role`  (</span><br><span class="line">  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;角色ID&apos;,</span><br><span class="line">  `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色唯一CODE代码&apos;,</span><br><span class="line">  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色名称&apos;,</span><br><span class="line">  `desc` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色描述&apos;,</span><br><span class="line">  `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;,</span><br><span class="line">  `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;,</span><br><span class="line">  `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;,</span><br><span class="line">  `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更改者&apos;,</span><br><span class="line">  `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;,</span><br><span class="line">  `type` tinyint(1) NOT NULL DEFAULT 1 COMMENT &apos;0：超管、1：管理员、2：普通人员&apos;,</span><br><span class="line">  PRIMARY KEY (`id`) USING BTREE,</span><br><span class="line">  INDEX `code`(`code`) USING BTREE COMMENT &apos;权限CODE代码&apos;</span><br><span class="line">) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;角色&apos; ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for role_permission</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `role_permission`;</span><br><span class="line">CREATE TABLE `role_permission`  (</span><br><span class="line">  `role_id` int(11) NOT NULL COMMENT &apos;角色id&apos;,</span><br><span class="line">  `permission_id` int(11) DEFAULT NULL COMMENT &apos;权限id&apos;</span><br><span class="line">) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user`;</span><br><span class="line">CREATE TABLE `user`  (</span><br><span class="line">  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;用户ID&apos;,</span><br><span class="line">  `state` tinyint(1) DEFAULT NULL COMMENT &apos;用户状态:0=正常,1=禁用&apos;,</span><br><span class="line">  `user_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;姓名&apos;,</span><br><span class="line">  `user_tel_number` varchar(11) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;手机号码&apos;,</span><br><span class="line">  `salt` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;密码加盐&apos;,</span><br><span class="line">  `password` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;登录密码&apos;,</span><br><span class="line">  `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;,</span><br><span class="line">  `creator` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;,</span><br><span class="line">  `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;,</span><br><span class="line">  `regenerator` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;,</span><br><span class="line">  `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;,</span><br><span class="line">  PRIMARY KEY (`id`) USING BTREE</span><br><span class="line">) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;用户&apos; ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user_group</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user_group`;</span><br><span class="line">CREATE TABLE `user_group`  (</span><br><span class="line">  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;ID&apos;,</span><br><span class="line">  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组名称&apos;,</span><br><span class="line">  `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组CODE唯一代码&apos;,</span><br><span class="line">  `desc` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组描述&apos;,</span><br><span class="line">  `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;,</span><br><span class="line">  `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;,</span><br><span class="line">  `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;,</span><br><span class="line">  `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;,</span><br><span class="line">  `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;,</span><br><span class="line">  PRIMARY KEY (`id`) USING BTREE,</span><br><span class="line">  INDEX `code`(`code`) USING BTREE COMMENT &apos;用户组CODE代码&apos;</span><br><span class="line">) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;用户组&apos; ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user_group_permission</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user_group_permission`;</span><br><span class="line">CREATE TABLE `user_group_permission`  (</span><br><span class="line">  `user_group_id` int(11) NOT NULL COMMENT &apos;用户组id&apos;,</span><br><span class="line">  `permission_id` int(11) DEFAULT NULL COMMENT &apos;权限id&apos;</span><br><span class="line">) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user_group_role</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user_group_role`;</span><br><span class="line">CREATE TABLE `user_group_role`  (</span><br><span class="line">  `user_group_id` int(11) NOT NULL COMMENT &apos;用户组id&apos;,</span><br><span class="line">  `role_id` int(11) DEFAULT NULL COMMENT &apos;角色id&apos;</span><br><span class="line">) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user_permission</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user_permission`;</span><br><span class="line">CREATE TABLE `user_permission`  (</span><br><span class="line">  `user_id` int(11) NOT NULL COMMENT &apos;用户id&apos;,</span><br><span class="line">  `permission_id` int(11) DEFAULT NULL COMMENT &apos;权限id&apos;</span><br><span class="line">) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user_role</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user_role`;</span><br><span class="line">CREATE TABLE `user_role`  (</span><br><span class="line">  `user_id` int(11) NOT NULL COMMENT &apos;用户id&apos;,</span><br><span class="line">  `role_id` int(11) DEFAULT NULL COMMENT &apos;角色id&apos;</span><br><span class="line">) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">-- Table structure for user_user_group</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `user_user_group`;</span><br><span class="line">CREATE TABLE `user_user_group`  (</span><br><span class="line">  `user_id` int(11) NOT NULL COMMENT &apos;用户id&apos;,</span><br><span class="line">  `user_group_id` int(11) DEFAULT NULL COMMENT &apos;用户组id&apos;</span><br><span class="line">) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;</span><br><span class="line"></span><br><span class="line">SET FOREIGN_KEY_CHECKS = 1;</span><br></pre></td></tr></table></figure><p><a href="https://www.processon.com/view/link/5e4c8ffce4b00aefb7e77730%20" target="_blank" rel="noopener">设计表逻辑图</a></p><p><img src="/images/2020/02/26/46e26010-5845-11ea-9939-157051740e67.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基于RBAC（Resource-Based-Access-Control）&quot;&gt;&lt;a href=&quot;#基于RBAC（Resource-Based-Access-Control）&quot; class=&quot;headerlink&quot; title=&quot;基于RBAC（Resource-Ba
      
    
    </summary>
    
      <category term="后台" scheme="http://yoursite.com/categories/%E5%90%8E%E5%8F%B0/"/>
    
    
      <category term="权限设计" scheme="http://yoursite.com/tags/%E6%9D%83%E9%99%90%E8%AE%BE%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>2019年度总结</title>
    <link href="http://yoursite.com/2020/01/18/2019%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2020/01/18/2019年度总结/</id>
    <published>2020-01-18T03:21:20.000Z</published>
    <updated>2020-01-19T07:54:27.413Z</updated>
    
    <content type="html"><![CDATA[<p>1.回顾<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">主要作业</span><br><span class="line">1.机器学习（Python，spark ml）</span><br><span class="line">2.成品码大数据分析（Scala ，spark sql ,hive）</span><br><span class="line">3.微服务架构（spring boot ，spring cloud等）</span><br><span class="line">4.蓝牙定位（netty -》spark streaming ， structured streaming -&gt; redis,mysql --&gt; websocket）</span><br><span class="line">5.电工 （redis ，flink，java）</span><br></pre></td></tr></table></figure></p><p>2.详细分析</p><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  2019学了很多，但真正精通，学的深的不多。</span><br><span class="line">1.年初学习了机器学习，主要是python的机器学习的包，但之后未用到实际的项目中，现在基本忘的差不多了，也就不多说了。</span><br><span class="line">2.成品码大数据分析项目，主要分为两部分，前一个月主要熟悉项目，这个项目是半路接手，准备第二期，我主要负责的数据处理部分。将需要的数据通过spark sql 处理形成中间结果表，给到后台进一步处理。</span><br><span class="line">后半部分则是彻底整合家用，商用，生活电器形成一张大而完整的表。</span><br><span class="line">主要学习到了spark 中 dataframe中的api,常用的有 spark.table获取hive数据仓库中的表，.where用来筛选符合条件的记录或者用.filter也是同样的作用，.startWith用来获取字段开始符合的条件，还有substr,na.fill,withColumn,lit,join，seq(),union等等，</span><br><span class="line">在项目中使用正则匹配进行数据处理时还是不够灵活，在与洪工对接需求时，需求较明确，反馈问题也能够及时得到回应。</span><br><span class="line">3.微服务架构，最近微服务也很流行，主要了解了spring boot 和 spring cloud 相关的知识，仅限于简单的使用，没有在复杂的场景中运用。</span><br><span class="line">4.蓝牙定位是本年最主要（即时间最久）的项目，从数据的接入到数据处理生成实时的定位信息传入到前端，在这个项目中学到了很多，也接触到了之前未接触到的的东西，比如kafak集群搭建（三个节点没有权限zk自己自带的），spark 集群（三个节点standalone模式）搭建，只是单独的spark集群没有hdfs。</span><br><span class="line">同时也接触到netty，网络之间数据传输（tcp/udp）数据传输。在做项目是由于前期对数据对接时考虑的问题不是很充足，以及对netty和kafak等一些参数不够了解耽误了不少时间，其中关于流处理的接触让我对流处理的一些框架有一定的了解，</span><br><span class="line">同时也了解了flink这个流处理的利器。总的来说还是完成了这个项目，尽管这个项目总是在关键时刻出现问题，但都合理的解决了。</span><br><span class="line">5.电工这个项目是临时接收的，当初只是简单的处理了一个上报的数据，替换相关数据源，将mysql数据源换成redis数据源，</span><br><span class="line">这里简单说明下，由于上个项目了解到redis，加之这个需求改动不大所有=以很快解决了，之后再很长一段时间是在看之前交接人写的代码，插一句由于项目交接多手，而且项目没有正规需求文档和相关的数据字典，难受。</span><br><span class="line">之后接了个上报收线重量的需求，由于之前的种种，以及flink窗口处理的能力出众，所以选择flink，数据源从activemq中读取，处理后上报给webservice服务。</span><br><span class="line">大致这样。下面配上几张数据流程图。</span><br></pre></td></tr></table></figure><p>1.电工数据流图<br><img src="/images/2020/01/18/399a4c70-39c7-11ea-b221-c32b78c1642c.png" alt="image.png"><br><img src="/images/2020/01/18/5d622650-39c7-11ea-b221-c32b78c1642c.png" alt="image.png"></p><p>2.蓝牙定位数据流图<br><img src="/images/2020/01/18/756dd9b0-39c7-11ea-b221-c32b78c1642c.png" alt="image.png"></p><p>3.微服务组件关系图<br><img src="/images/2020/01/18/d1e592a0-39c7-11ea-b221-c32b78c1642c.png" alt="image.png"></p><p>3.展望<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">多读书多看报，少吃零食多睡觉。</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1.回顾&lt;br&gt;&lt;figure class=&quot;highlight plain hljs&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span
      
    
    </summary>
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="总结" scheme="http://yoursite.com/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>flink sources</title>
    <link href="http://yoursite.com/2020/01/10/flink-sources/"/>
    <id>http://yoursite.com/2020/01/10/flink-sources/</id>
    <published>2020-01-10T09:19:36.000Z</published>
    <updated>2020-01-10T09:20:13.894Z</updated>
    
    <content type="html"><![CDATA[<ol><li>activemq </li></ol><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;</span><br><span class="line">         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;com.gree.bdc&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-table-api&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.8.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-java&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.8.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flink-connector-activemq_2.11&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">package com.gree.bdc.io;</span><br><span class="line"></span><br><span class="line">import org.apache.activemq.ActiveMQConnectionFactory;</span><br><span class="line">import org.apache.flink.streaming.api.functions.source.SourceFunction;</span><br><span class="line"></span><br><span class="line">import javax.jms.*;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 获取activeMQ数据</span><br><span class="line"> */</span><br><span class="line">public class FlinkActiveMQStreamSources implements SourceFunction &#123;</span><br><span class="line"></span><br><span class="line">    private Connection connection = null;</span><br><span class="line">    private Session session = null;</span><br><span class="line">    private MessageConsumer messageConsumer = null;</span><br><span class="line">    private boolean running = Boolean.TRUE;</span><br><span class="line"></span><br><span class="line">    public void run(SourceContext ctx) throws Exception &#123;</span><br><span class="line">        ActiveMQConnectionFactory activeMQConnectionFactory = new ActiveMQConnectionFactory(ActiveMQConnectionFactory.DEFAULT_USER, ActiveMQConnectionFactory.DEFAULT_PASSWORD, ActiveMQConnectionFactory.DEFAULT_BROKER_URL);</span><br><span class="line">        try &#123;</span><br><span class="line">            connection = activeMQConnectionFactory.createConnection();</span><br><span class="line">            connection.start();</span><br><span class="line">            session = connection.createSession(Boolean.FALSE,Session.AUTO_ACKNOWLEDGE);</span><br><span class="line">            String queueName = &quot;test?consumer.exclusive=true&quot;;</span><br><span class="line">            Destination destination  = session.createQueue(queueName);</span><br><span class="line">            messageConsumer = session.createConsumer(destination);</span><br><span class="line"></span><br><span class="line">            while (running)&#123;</span><br><span class="line">                TextMessage message = (TextMessage)messageConsumer.receive(60 * 100);</span><br><span class="line">                if (message instanceof TextMessage)&#123;</span><br><span class="line">                    TextMessage textMessage =(TextMessage) message;</span><br><span class="line">                    String text = textMessage.getText();</span><br><span class="line">                    ctx.collect(text);</span><br><span class="line">                &#125;else &#123;</span><br><span class="line">                    System.out.println(&quot;未收到消息！！！&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;catch (Exception e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;finally &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                messageConsumer.close();</span><br><span class="line">                session.close();</span><br><span class="line">                connection.close();</span><br><span class="line">            &#125;catch (JMSException e)&#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;finally &#123;</span><br><span class="line">                Thread.sleep(10 * 1000);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void cancel() &#123;</span><br><span class="line">        running = Boolean.FALSE;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;activemq &lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight plain hljs&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span c
      
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>nifi process 介绍与使用</title>
    <link href="http://yoursite.com/2020/01/09/nifi-process-%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2020/01/09/nifi-process-介绍与使用/</id>
    <published>2020-01-09T02:02:54.000Z</published>
    <updated>2020-01-09T02:19:32.936Z</updated>
    
    <content type="html"><![CDATA[<ol><li><strong>GetJMSQueue</strong></li></ol><p><img src="/images/2020/01/09/ae19c350-3283-11ea-80df-39774ea2e31e.png" alt="image.png"></p><p>作用：读取activeMQ消息队列的数据</p><p><img src="/images/2020/01/09/daf9f3e0-3283-11ea-80df-39774ea2e31e.png" alt="image.png"></p><p>主要 URL ：tcp://localhost:61616 需要将ip地址写入到host文件中<br>Destination Name : 消息队列的名称</p><p>2.<strong>Mysql –&gt; kafka</strong></p><p><img src="/images/2020/01/09/680cc280-3284-11ea-80df-39774ea2e31e.png" alt="image.png"></p><p>主要读取mysql数据 ExecuteSQL<br><img src="/images/2020/01/09/a6f8da60-3284-11ea-80df-39774ea2e31e.png" alt="image.png"><br>主要配置 Database Connection Pooling Service </p><p><img src="/images/2020/01/09/d80ec560-3284-11ea-80df-39774ea2e31e.png" alt="image.png"></p><p>配置 kafaka 生产者<br><img src="/images/2020/01/09/3b494c40-3285-11ea-80df-39774ea2e31e.png" alt="image.png"></p><p>3.<strong>Mysql增量 –》kafka</strong><br><img src="/images/2020/01/09/9ac82ab0-3285-11ea-80df-39774ea2e31e.png" alt="image.png"></p><p>主要将 ExecuteSQL 替换为 QueryDatabaseTable<br><img src="/images/2020/01/09/fbe80ef0-3285-11ea-80df-39774ea2e31e.png" alt="image.png"></p><p>主要配置Maximum-value Columns 增量字段，nifi每次请求数据时都取前一次字段数据后的数据，保证增量读取数据，避免重复读。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;strong&gt;GetJMSQueue&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;/images/2020/01/09/ae19c350-3283-11ea-80df-39774ea2e31e.png&quot; alt=&quot;image.png&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="NIFI" scheme="http://yoursite.com/tags/NIFI/"/>
    
  </entry>
  
  <entry>
    <title>Flink DataStream Api 编程</title>
    <link href="http://yoursite.com/2019/12/30/Flink-DataStream-Api-%E7%BC%96%E7%A8%8B/"/>
    <id>http://yoursite.com/2019/12/30/Flink-DataStream-Api-编程/</id>
    <published>2019-12-30T06:56:27.000Z</published>
    <updated>2019-12-31T01:38:24.414Z</updated>
    
    <content type="html"><![CDATA[<h2 id="流处理基本概念"><a href="#流处理基本概念" class="headerlink" title="流处理基本概念"></a>流处理基本概念</h2><blockquote><p>对于什么是流处理，从不同的角度有不同的定义。其实流处理与批处理这两个概念是对立统一的，它们的关系有点类似于对于 Java 中的 ArrayList 中的元素，是直接看作一个有限数据集并用下标去访问，还是用迭代器去访问。<br>  流处理系统本身有很多自己的特点。一般来说，由于需要支持无限数据集的处理，流处理系统一般采用一种数据驱动的处理方式。它会提前设置一些算子，然后等到数据到达后对数据进行处理。为了表达复杂的计算逻辑，包括 Flink 在内的分布式流处理引擎一般采用 DAG 图来表示整个计算逻辑，其中 DAG 图中的每一个点就代表一个基本的逻辑单元，也就是前面说的算子。由于计算逻辑被组织成有向图，数据会按照边的方向，从一些特殊的 Source 节点流入系统，然后通过网络传输、本地传输等不同的数据传输方式在算子之间进行发送和处理，最后会通过另外一些特殊的 Sink 节点将计算结果发送到某个外部系统或数据库中。</p></blockquote><blockquote><p>总结一下：流数据本身就是源源不断的产生，流数据处理就是提前设置一些算子，然后等待数据流到这些提前设置的算子进行处理的过程。而实际在分布式的情况下，需要考虑不同的实例之间数据传输。</p></blockquote><p><img src="/images/2019/12/30/230086f0-2ad1-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p>对于实际的分布式流处理引擎，它们的实际运行时物理模型要更复杂一些，这是由于每个算子都可能有多个实例。如图 2 所示，作为 Source 的 A 算子有两个实例，中间算子 C 也有两个实例。在逻辑模型中，A 和 B 是 C 的上游节点，而在对应的物理逻辑中，C 的所有实例和 A、B 的所有实例之间可能都存在数据交换。在物理模型中，我们会根据计算逻辑，采用系统自动优化或人为指定的方式将计算工作分布到不同的实例中。只有当算子实例分布到不同进程上时，才需要通过网络进行数据传输，而同一进程中的多个实例之间的数据传输通常是不需要通过网络的。</p></blockquote><h2 id="Flink-DataStram-API-概览"><a href="#Flink-DataStram-API-概览" class="headerlink" title="Flink DataStram API 概览"></a>Flink DataStram API 概览</h2><p> 首先用一个简单例子说明：</p><blockquote><p>//1、设置运行环境<br>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();<br>//2、配置数据源读取数据<br>DataStream<string> text = env.readTextFile (“input”);<br>//3、进行一系列转换<br>DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text.flatMap(new Tokenizer()).keyBy(0).sum(1);<br>//4、配置数据汇写出数据<br>counts.writeAsText(“output”);<br>//5、提交执行<br>env.execute(“Streaming WordCount”);</string></p></blockquote><blockquote><p> 为了实现流式 Word Count，我们首先要先获得一个 StreamExecutionEnvironment 对象。它是我们构建图过程中的上下文对象。基于这个对象，我们可以添加一些算子。对于流处理程度，我们一般需要首先创建一个数据源去接入数据。在这个例子中，我们使用了 Environment 对象中内置的读取文件的数据源。这一步之后，我们拿到的是一个 DataStream 对象，它可以看作一个无限的数据集，可以在该集合上进行一序列的操作。例如，在 Word Count 例子中，我们首先将每一条记录（即文件中的一行）分隔为单词，这是通过 FlatMap 操作来实现的。调用 FlatMap 将会在底层的 DAG 图中添加一个 FlatMap 算子。然后，我们得到了一个记录是单词的流。我们将流中的单词进行分组（keyBy），然后累积计算每一个单词的数据（sum(1)）。计算出的单词的数据组成了一个新的流，我们将它写入到输出文件中。<br>最后，我们需要调用 env#execute 方法来开始程序的执行。需要强调的是，前面我们调用的所有方法，都不是在实际处理数据，而是在构通表达计算逻辑的 DAG 图。只有当我们将整个图构建完成并显式的调用 Execute 方法后，框架才会把计算图提供到集群中，接入数据并执行实际的逻辑。<br>基于流式 Word Count 的例子可以看出，基于 Flink 的 DataStream API 来编写流处理程序一般需要三步：通过 Source 接入数据、进行一系统列的处理以及将数据写出。最后，不要忘记显式调用 Execute 方式，否则前面编写的逻辑并不会真正执行。</p></blockquote><p><img src="/images/2019/12/30/e5920700-2ad3-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p>从上面的例子中还可以看出，Flink DataStream API 的核心，就是代表流数据的 DataStream 对象。整个计算逻辑图的构建就是围绕调用 DataStream 对象上的不同操作产生新的 DataStream 对象展开的。整体来说，DataStream 上的操作可以分为四类。第一类是对于单条记录的操作，比如筛除掉不符合要求的记录（Filter 操作），或者将每条记录都做一个转换（Map 操作）。第二类是对多条记录的操作。比如说统计一个小时内的订单总成交量，就需要将一个小时内的所有订单记录的成交量加到一起。为了支持这种类型的操作，就得通过 Window 将需要的记录关联到一起进行处理。第三类是对多个流进行操作并转换为单个流。例如，多个流可以通过 Union、Join 或 Connect 等操作合到一起。这些操作合并的逻辑不同，但是它们最终都会产生了一个新的统一的流，从而可以进行一些跨流的操作。最后， DataStream 还支持与合并对称的操作，即把一个流按一定规则拆分为多个流（Split 操作），每个流是之前流的一个子集，这样我们就可以对不同的流作不同的处理。</p></blockquote><p> <img src="/images/2019/12/30/7d4e2c90-2ad4-11ea-80df-39774ea2e31e.png" alt="image.png"></p><p>参考文献：<a href="https://ververica.cn/developers/apache-flink-basic-zero-iii-datastream-api-programming/" target="_blank" rel="noopener">Apache Flink 零基础入门（四）：DataStream API 编程</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;流处理基本概念&quot;&gt;&lt;a href=&quot;#流处理基本概念&quot; class=&quot;headerlink&quot; title=&quot;流处理基本概念&quot;&gt;&lt;/a&gt;流处理基本概念&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;对于什么是流处理，从不同的角度有不同的定义。其实流处理与批处理这两个概念是
      
    
    </summary>
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
      <category term="学习" scheme="http://yoursite.com/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>scala 引包中遇到问题</title>
    <link href="http://yoursite.com/2019/12/29/scala-%E5%BC%95%E5%8C%85%E4%B8%AD%E9%81%87%E5%88%B0%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2019/12/29/scala-引包中遇到问题/</id>
    <published>2019-12-29T01:53:45.000Z</published>
    <updated>2019-12-29T01:53:45.858Z</updated>
    
    <content type="html"><![CDATA[<p>1.Error:(12, 30) could not find implicit value for evidence parameter of type org.apache.flink.api.common.typeinfo.TypeInformation[(String, String)]     val value = env.addSource(new HBaseReader)</p><blockquote><p>解决方法：import org.apache.flink.api.scala._</p></blockquote><p>2.Scala编程中常见错误：Error:(28, 21) value foreach is not a member of java.util.List[String]</p><blockquote><p>解决方法：因为 Java 集合类型在 Scala 操作时没有 foreach 方法, 所以需要将其转换为Scala的集合类型,<br>因此需要在代码中加入如下内容(Scala支持与Java的隐式转换),<br>import scala.collection.JavaConversions._</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1.Error:(12, 30) could not find implicit value for evidence parameter of type org.apache.flink.api.common.typeinfo.TypeInformation[(Strin
      
    
    </summary>
    
      <category term="程序语言" scheme="http://yoursite.com/categories/%E7%A8%8B%E5%BA%8F%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="scala" scheme="http://yoursite.com/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>Flink 基本概念（有状态流式处理引擎）</title>
    <link href="http://yoursite.com/2019/12/27/Flink-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%88%E6%9C%89%E7%8A%B6%E6%80%81%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8E%EF%BC%89/"/>
    <id>http://yoursite.com/2019/12/27/Flink-基本概念（有状态流式处理引擎）/</id>
    <published>2019-12-27T01:29:44.000Z</published>
    <updated>2019-12-29T03:14:16.710Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、什么是有状态处理"><a href="#一、什么是有状态处理" class="headerlink" title="一、什么是有状态处理"></a>一、什么是有状态处理</h2><ol><li>传统批次处理方法<br><img src="/images/2019/12/27/8f2108b0-2846-11ea-80df-39774ea2e31e.png" alt="image.png"><blockquote><p>可以看出传统批处理的方法是采用持续收取数据，以时间作为划分多个批次的依据，在周期性执行批次处理。<br><img src="/images/2019/12/27/f1950b40-2846-11ea-80df-39774ea2e31e.png" alt="image.png"><br>但当出现需要就算每个小时出现事件转换次数的需求时，并且事件转换跨越了所定义的时间划分，也即两个事件出现在了，两个不同的批次当中，<strong>传统批处理会将中介运算结果带到下一个批次进行计算</strong> ，还有出现接收的事件顺序颠倒时，传统批处理仍会将中介状态带到下一批次的运算结果中。</p></blockquote></li><li>理想方法<br><img src="/images/2019/12/27/8aa1e1d0-2849-11ea-80df-39774ea2e31e.png" alt="image.png"><blockquote><p>第一点，要有理想方法，这个理想方法是引擎必须要有能力可以累积状态和维护状态，累积状态代表着过去历史中接收过的所有事件，会影响到输出。<br>第二点，时间，时间意味着引擎对于数据完整性有机制可以操控，当所有数据都完全接收到后，输出计算结果。<br>第三点，理想方法模型需要实时产生结果，但更重要的是采用新的持续性数据处理模型来处理实时数据，这样才最符合Continuous data 的特性。<br><img src="/images/2019/12/27/cd4164c0-2849-11ea-80df-39774ea2e31e.png" alt="image.png"></p></blockquote></li><li>流式处理<br><img src="/images/2019/12/27/43597ad0-284a-11ea-80df-39774ea2e31e.png" alt="image.png"><blockquote><p>流式处理简单来讲即有一个无穷无尽的数据源在持续收取数据，以代码作为数据处理的基础逻辑，数据源的数据经过代码处理后产生出结果，然后输出，这就是流式处理的基本原理。</p></blockquote></li><li>分布式流式处理<br><img src="/images/2019/12/27/0a495de0-284b-11ea-80df-39774ea2e31e.png" alt="image.png"><blockquote><p>假设Input Streams 有很多个使用者，每个使用者都有自己的ID，如果计算每个使用者出现的次数，我们需要让同一个使用者的出现事件流到同一运算代码，这跟其他批次需要做Group by 是同样的概念，所以跟Stream 一样需要做分区，设定相应的Key，然后让同样的 Key 流到同一个 Computation instance 做同样的运算。</p></blockquote></li><li>有状态分布式流式处理<br><img src="/images/2019/12/27/673360a0-284b-11ea-80df-39774ea2e31e.png" alt="image.png"><blockquote><p>如图，上述代码中定义了变数X，X 在数据处理过程中会进行读和写，在最后输出结果时，可以依据变数X 决定输出的内容，即状态X 会影响最终的输出结果。这个过程中，第一个重点是先进行了状态Co-partitioned key by，同样的 Key 都会流到Computation instance，与使用者出现次数的原理相同，次数即所谓的状态，这个状态一定会跟同一个Key 的事件累积在同一个 Computation instance。类似于根据输入流的Key 重新分区的状态，当分区进入 Stream 之后，这个 Stream 会累积起来的状态也变成 Copartiton .<br>第二个重点是embeded local state backend。有状态分散式流式处理的引擎，状态可能会累积到非常大，当 Key 非常多时，状态可能就会超出单一节点的 Memory 的负荷量，这时候状态必须有状态后端去维护它；在这个状态后端在正常状况下，用In-memory 维护即可。</p></blockquote></li></ol><h2 id="二、有状态流式处理的挑战"><a href="#二、有状态流式处理的挑战" class="headerlink" title="二、有状态流式处理的挑战"></a>二、有状态流式处理的挑战</h2><ol><li>状态容错<br>需要解决下面三个问题：</li></ol><ul><li>如何确保状态拥有精确一次（exactly-once guarantee）容错保证</li><li>如何在分布式场景下替多个拥有本地状态的运算子产生一个全域一致的快照（global consistent snapshot）</li><li>如何在不中断运算的前提下产生快照<br>1.1 简单场景的精确一次容错方法<blockquote><p>先考虑最简单的使用场景，如无限流的数据进入，后面单一的Process 进行运算，每处理完一笔计算即会累积一次状态，这种情况下如果要确保Process 产生精确一次的状态容错，每处理完一笔数据，更改完状态后进行一次快照，快照包含在队列中并与相应的状态进行对比，完成一致的快照，就能确保精确一次。</p></blockquote></li></ul><p>1.2 分布式状态容错</p><blockquote><p>Flink作为分布式的处理引擎，在分布式的场景下，进行多个本地状态的运算，只产生一个全域一致的快照，如需要在不中断运算值的前提下产生全域一致的快照，就涉及到分散式状态容错。</p></blockquote><p><img src="/images/2019/12/27/9078f390-2872-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p> 关于Global consistent snapshot，当Operator 在分布式的环境中，在各个节点做运算，首先产生Global consistent snapshot 的方式就是处理每一笔数据的快照点是连续的，这笔运算流过所有的运算值，更改完所有的运算值后，能够看到每一个运算值的状态与该笔运算的位置，即可称为Consistent snapshot，当然，Global consistent snapshot 也是简易场景的延伸。</p></blockquote><p>容错恢复</p><p><img src="/images/2019/12/27/09adb2f0-2873-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p>首先了解一下Checkpoint，上面提到连续性快照每个Operator 运算值本地的状态后端都要维护状态，也就是每次将产生检查点时会将它们传入共享的DFS 中。当任何一个Process 挂掉后，可以直接从三个完整的Checkpoint 将所有的运算值的状态恢复，重新设定到相应位置。Checkpoint的存在使整个Process 能够实现分散式环境中的Exactly-once。</p></blockquote><p>1.3 分布式快照的方法（distribution snapshot）<br><img src="/images/2019/12/27/b51633f0-2874-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p>关于Flink 如何在不中断运算的状况下持续产生Global consistent snapshot，其方式是基于用 Simple lamport 演算法机制下延伸的。已知的一个点Checkpoint barrier，Flink 在某个Datastream 中会一直安插Checkpoint barrier，Checkpoint barrier 也会N – 1等等，Checkpoint barrier N 代表着所有在这个范围里面的数据都是Checkpoint barrier N。</p></blockquote><p><img src="/images/2019/12/27/a0fa8a00-2875-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p>举例：假设现在需要产生Checkpoint barrier N，但实际上在Flink 中是由Job manager 触发Checkpoint，Checkpoint 被触发后开始从数据源产生Checkpoint barrier。当Job 开始做Checkpoint barrier N 的时候，可以理解为Checkpoint barrier N 需要逐步填充左下角的表格。</p></blockquote><p><img src="/images/2019/12/27/3cecb820-2876-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p>如图，当部分事件标为红色，Checkpoint barrier N 也是红色时，代表着这些数据或事件都由Checkpoint barrier N 负责。Checkpoint barrier N 后面白色部分的数据或事件则不属于Checkpoint barrier N。</p></blockquote><p>在以上的基础上，当数据源收到Checkpoint barrier N 之后会先将自己的状态保存，以读取Kafka资料为例，数据源的状态就是目前它在Kafka 分区的位置，这个状态也会写入到上面提到的表格中。下游的Operator 1 会开始运算属于Checkpoint barrier N 的数据，当Checkpoint barrier N 跟着这些数据流动到Operator 1 之后,Operator 1 也将属于Checkpoint barrier N 的所有数据都反映在状态中，当收到Checkpoint barrier N 时也会直接对Checkpoint去做快照。</p><p><img src="/images/2019/12/27/6ba1cfc0-2876-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p>当快照完成后继续往下游走，Operator 2 也会接收到所有数据，然后搜索Checkpoint barrier N 的数据并直接反映到状态，当状态收到Checkpoint barrier N 之后也会直接写入到Checkpoint N 中。以上过程到此可以看到Checkpoint barrier N 已经完成了一个完整的表格，这个表格叫做Distributed Snapshots，即分布式快照。分布式快照可以用来做状态容错，任何一个节点挂掉的时候可以在之前的Checkpoint 中将其恢复。继续以上Process，当多个Checkpoint 同时进行，Checkpoint barrier N 已经流到Job manager 2，Flink job manager 可以触发其他的Checkpoint，比如Checkpoint N + 1，Checkpoint N + 2 等等也同步进行，利用这种机制，可以在不阻挡运算的状况下持续地产生Checkpoint。</p></blockquote><p><img src="/images/2019/12/27/adee50b0-2876-11ea-80df-39774ea2e31e.png" alt="image.png"></p><ol start="2"><li>状态维护<blockquote><p>状态维护即用一段代码在本地维护状态值，当状态值非常大时需要本地的状态后端来支持。</p></blockquote></li></ol><p><img src="/images/2019/12/27/a35ab9c0-2878-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p>图中分别为： 状态识别id,状态数据形态资讯，注册状态<br>如图，在Flink 程序中，可以采用getRuntimeContext().getState(desc); 这组API 去注册状态。Flink 有多种状态后端，采用API 注册状态后，读取状态时都是通过状态后端来读取的。Flink 有两种不同的状态值，也有两种不同的状态后端：<br><img src="/images/2019/12/27/102f4a70-2879-11ea-80df-39774ea2e31e.png" alt="image.png"></p></blockquote><p>-JVM Heap状态后端，适合数量较小的状态，当状态量不大时就可以采用JVM Heap 的状态后端。JVM Heap 状态后端会在每一次运算值需要读取状态时，用Java object read / writes 进行读或写，不会产生较大代价，但当Checkpoint 需要将每一个运算值的本地状态放入Distributed Snapshots 的时候，就需要进行序列化了。</p><p><img src="/images/2019/12/27/30faad30-2879-11ea-80df-39774ea2e31e.png" alt="image.png"></p><p>-RocksDB状态后端，它是一种out of core 的状态后端。在Runtime 的本地状态后端让使用者去读取状态的时候会经过磁盘，相当于将状态维护在磁盘里，与之对应的代价可能就是每次读取状态时，都需要经过序列化和反序列化的过程。当需要进行快照时只将应用序列化即可，序列化后的数据直接传输到中央的共享DFS 中。</p><blockquote><p>Flink目前支持以上两种状态后端，一种是纯 Memory 的状态后端，另一种是有资源磁盘的状态后端，在维护状态时可以根据状态的数量选择相应的状态后端。</p></blockquote><ol start="3"><li>event-time process<br>3.1 不同的时间种类<blockquote><p>在Flink 及其他进阶的流式处理引擎出现之前，大数据处理引擎一直只支持Processing-time 的处理。假设定义一个运算 Windows 的窗口，Windows 运算设定每小时进行结算。Processing-time 进行运算时，可以发现数据引擎将3 点至4 点间收到的数据进行结算。实际上在做报表或者分析结果时是想了解真实世界中3 点至4 点之间实际产生数据的输出结果，了解实际数据的输出结果就必须采用Event – Time 了。<br><img src="/images/2019/12/29/1593f130-29e8-11ea-80df-39774ea2e31e.png" alt="image.png"></p></blockquote></li></ol><blockquote><p>如图，Event – Time 相当于事件，它在数据最源头产生时带有时间戳，后面都需要用时间戳来进行运算。用图来表示，最开始的队列收到数据，每小时对数据划分一个批次，这就是Event – Time Process 在做的事情。</p></blockquote><p>3.2 Event-Time处理<br><img src="/images/2019/12/29/4184b8b0-29e8-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p>Event – Time 是用事件真实产生的时间戳去做Re-bucketing，把对应时间3 点到4 点的数据放在3 点到4 点的Bucket，然后Bucket 产生结果。所以Event – Time 跟Processing – time 的概念是这样对比的存在。</p></blockquote><p><img src="/images/2019/12/29/5988a3e0-29e8-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p> Event – Time 的重要性在于记录引擎输出运算结果的时间。简单来说，流式引擎连续24 小时在运行、搜集资料，假设Pipeline 里有一个 Windows Operator 正在做运算，每小时能产生结果，何时输出 Windows的运算值，这个时间点就是Event – Time 处理的精髓，用来表示该收的数据已经收到。</p></blockquote><p>3.3 Watermarks(水位线)<br><img src="/images/2019/12/29/9b44cbb0-29e8-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p> Flink实际上是用 Watermarks 来实现Event – Time 的功能。Watermarks 在Flink 中也属于特殊事件，其精髓在于当某个运算值收到带有时间戳“ T ”的 Watermarks 时就意味着它不会接收到新的数据了。使用Watermarks 的好处在于可以准确预估收到数据的截止时间。举例，假设预期收到数据时间与输出结果时间的时间差延迟5 分钟，那么Flink 中所有的 Windows Operator 搜索3 点至4 点的数据，但因为存在延迟需要再多等5分钟直至收集完4：05 分的数据，此时方能判定4 点钟的资料收集完成了，然后才会产出3 点至4 点的数据结果。这个时间段的结果对应的就是 Watermarks 的部分。</p></blockquote><ol start="4"><li>状态保存与迁移<blockquote><p>流式处理应用无时无刻不在运行，运维上有几个重要考量：</p></blockquote></li></ol><ul><li>如何将前一步执行的的状态迁移到新的执行上</li><li>如何重新定义运行的平行化程度</li><li><blockquote><p> Checkpoint完美符合以上需求，不过Flink 中还有另外一个名词保存点（Savepoint），当手动产生一个Checkpoint 的时候，就叫做一个Savepoint。Savepoint 跟Checkpoint 的差别在于Checkpoint是Flink 对于一个有状态应用在运行中利用分布式快照持续周期性的产生Checkpoint，而Savepoint 则是手动产生的Checkpoint，Savepoint 记录着流式应用中所有运算元的状态。</p></blockquote></li></ul><p><img src="/images/2019/12/29/218dc780-29e9-11ea-80df-39774ea2e31e.png" alt="image.png"></p><blockquote><p> 如图，Savepoint A 和Savepoint B，无论是变更底层代码逻辑、修bug 或是升级Flink 版本，重新定义应用、计算的平行化程度等，最先需要做的事情就是产生Savepoint。</p></blockquote><p>Savepoint产生的原理是在Checkpoint barrier 流动到所有的Pipeline 中手动插入从而产生分布式快照，这些分布式快照点即Savepoint。Savepoint 可以放在任何位置保存，当完成变更时，可以直接从Savepoint 恢复、执行。</p><p>从Savepoint 的恢复执行需要注意，在变更应用的过程中时间在持续，如Kafka 在持续收集资料，当从Savepoint 恢复时，Savepoint 保存着Checkpoint 产生的时间以及Kafka 的相应位置，因此它需要恢复到最新的数据。无论是任何运算，Event – Time 都可以确保产生的结果完全一致。</p><p>假设恢复后的重新运算用Process Event – Time，将 Windows 窗口设为1 小时，重新运算能够在10 分钟内将所有的运算结果都包含到单一的 Windows 中。而如果使用Event – Time，则类似于做Bucketing。在Bucketing 的状况下，无论重新运算的数量多大，最终重新运算的时间以及Windows 产生的结果都一定能保证完全一致。</p><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><blockquote><p> 本文首先从Apache Flink 的定义、架构、基本原理入手，对大数据流计算相关的基本概念进行辨析，在此基础上简单回顾了大数据处理方式的历史演进以及有状态的流式数据处理的原理，最后从目前有状态的流式处理面临的挑战分析Apache Flink 作为业界公认为最好的流计算引擎之一所具备的天然优势。希望有助于大家厘清大数据流式处理引擎涉及的基本概念，能够更加得心应手的使用Flink。</p></blockquote><p>参考文章：<a href="https://ververica.cn/developers/flink-basic-tutorial-1-basic-concept/" target="_blank" rel="noopener">Apache Flink 零基础入门（一&amp;二）：基础概念解析</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、什么是有状态处理&quot;&gt;&lt;a href=&quot;#一、什么是有状态处理&quot; class=&quot;headerlink&quot; title=&quot;一、什么是有状态处理&quot;&gt;&lt;/a&gt;一、什么是有状态处理&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;传统批次处理方法&lt;br&gt;&lt;img src=&quot;/images/20
      
    
    </summary>
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink 基本概念</title>
    <link href="http://yoursite.com/2019/12/25/flink-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>http://yoursite.com/2019/12/25/flink-基本概念/</id>
    <published>2019-12-25T02:11:45.000Z</published>
    <updated>2019-12-25T07:35:37.085Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Apache-flink-基本概念，原理和架构"><a href="#一、Apache-flink-基本概念，原理和架构" class="headerlink" title="一、Apache flink 基本概念，原理和架构"></a>一、Apache flink 基本概念，原理和架构</h2><blockquote><p>Apache Flink is a <strong>framework</strong> and <strong>distributed</strong> processing engine for <strong>stateful</strong> computations over unbounded and bounded data <strong>streams</strong>.<br> Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态或无状态的计算，能够部署在各种集群环境，对各种规模大小的数据进行快速计算。</p></blockquote><h3 id="1-Flink-Application"><a href="#1-Flink-Application" class="headerlink" title="1. Flink Application"></a>1. Flink Application</h3><blockquote><p>首先了解Flink的Streams、State、Time等基本处理语义以及Flink兼顾灵活性和方便性的多层次API。</p><ul><li>Streams：流，分为有限数据流与无限数据流，unbounded stream 是有始无终的数据流，即无限数据流；而bounded stream 是限定大小的有始有终的数据集合，即有限数据流，二者的区别在于无限数据流的数据会随时间的推演而持续增加，计算持续进行且不存在结束的状态，相对的有限数据流数据大小固定，计算最终会完成并处于结束的状态。<br><img src="/images/2019/12/25/3e5f8a80-26e0-11ea-80df-39774ea2e31e.png" alt="image.png"></li></ul></blockquote><ul><li>State，状态是计算过程中的数据信息，在容错恢复和Checkpoint 中有重要的作用，流计算在本质上是Incremental Processing，因此需要不断查询保持状态；另外，为了确保Exactly- once 语义，需要数据能够写入到状态中；而持久化存储，能够保证在整个分布式系统运行失败或者挂掉的情况下做到Exactly- once，这是状态的另外一个价值。flink的状态管理还需加强研究。(Application state is a first-class citizen in flink)</li></ul><p><img src="/images/2019/12/25/c297cec0-26e0-11ea-80df-39774ea2e31e.png" alt="image.png"></p><ul><li>Time，分为Event time、Ingestion time、Processing time，Flink 的无限数据流是一个持续的过程，时间是我们判断业务状态是否滞后，数据处理是否及时的重要依据。（平常主要用Event Time 和 processing time）<br><img src="/images/2019/12/25/9d1afc70-26e1-11ea-80df-39774ea2e31e.png" alt="image.png"></li></ul><blockquote><p><strong>Event Time</strong> is the time when an event was created. It is usually described by a timestamp in the event.<br><strong>Ingestion time</strong> is the time when an event entrty the flink dataflow at the source operator.<br><strong>Processing Time</strong> is the local time at each operator that performs a time-based operation.</p></blockquote><ul><li>API，API 通常分为三层，由上而下可分为SQL / Table API、DataStream API、ProcessFunction 三层，API 的表达能力及业务抽象能力都非常强大，但越接近SQL 层，表达能力会逐步减弱，抽象能力会增强，反之，ProcessFunction 层API 的表达能力非常强，可以进行多种灵活方便的操作，但抽象能力也相对越小。(目前主要使用 DataStream API 和 ProcessFunction)<br><img src="/images/2019/12/25/e2271d20-26e2-11ea-80df-39774ea2e31e.png" alt="image.png"><blockquote><p><strong>ProcessFunctions</strong> ：the most expressive function interfaces that Flink oﬀers. Flink provides ProcessFunctions to process individual events from one or two input streams or events that were grouped in a window.<br><strong>DataStreamAPI</strong> ：provides primitives for many common stream processing operations, such as windowing, record-at-a-time transformations, and enriching events by querying an external data store.<br><strong>SQL/TableAPI</strong> :  relational APIs.</p></blockquote></li></ul><h3 id="2-Flink-Architecture"><a href="#2-Flink-Architecture" class="headerlink" title="2.Flink Architecture"></a>2.Flink Architecture</h3><p>  主要为以下四个部分：<br>     <img src="/images/2019/12/25/4cf5fd50-26e4-11ea-80df-39774ea2e31e.png" alt="image.png"></p><p>第一，Flink 具备统一的框架处理有界和无界两种数据流的能力</p><p>第二， 部署灵活，Flink 底层支持多种资源调度器，包括Yarn、Kubernetes 等。Flink 自身带的Standalone 的调度器，在部署上也十分灵活。</p><p>第三， 极高的可伸缩性，可伸缩性对于分布式系统十分重要，阿里巴巴双11大屏采用Flink 处理海量数据，使用过程中测得Flink 峰值可达17 亿/秒。</p><p>第四， 极致的流式处理性能。Flink 相对于Storm 最大的特点是将状态语义完全抽象到框架中，支持本地状态读取，避免了大量网络IO，可以极大提升状态存取的性能。</p><p><img src="/images/2019/12/25/1f87a110-26e5-11ea-80df-39774ea2e31e.png" alt="image.png"></p><h3 id="3-Flink-Operation"><a href="#3-Flink-Operation" class="headerlink" title="3. Flink Operation"></a>3. Flink Operation</h3><p>后面会有专门课程讲解，此处简单分享Flink 关于运维及业务监控的内容：</p><ul><li><p>Flink具备7 X 24 小时高可用的SOA（面向服务架构），原因是在实现上Flink 提供了一致性的Checkpoint。Checkpoint是Flink 实现容错机制的核心，它周期性的记录计算过程中Operator 的状态，并生成快照持久化存储。当Flink 作业发生故障崩溃时，可以有选择的从Checkpoint 中恢复，保证了计算的一致性。</p></li><li><p>Flink本身提供监控、运维等功能或接口，并有内置的WebUI，对运行的作业提供DAG 图以及各种Metric 等，协助用户管理作业状态。</p></li></ul><h3 id="4-Flink-场景应用"><a href="#4-Flink-场景应用" class="headerlink" title="4.Flink 场景应用"></a>4.Flink 场景应用</h3><h4 id="4-1-Flink-场景应用：Data-Pipeline"><a href="#4-1-Flink-场景应用：Data-Pipeline" class="headerlink" title="4.1 Flink 场景应用：Data Pipeline"></a>4.1 Flink 场景应用：Data Pipeline</h4><p><img src="/images/2019/12/25/e8acbe30-26e6-11ea-80df-39774ea2e31e.png" alt="image.png"><br>Data Pipeline 的核心场景类似于数据搬运并在搬运的过程中进行部分数据清洗或者处理，而整个业务架构图的左边是Periodic ETL，它提供了流式ETL 或者实时ETL，能够订阅消息队列的消息并进行处理，清洗完成后实时写入到下游的Database或File system 中。场景举例：</p><p><strong>实时数仓</strong><br>  当下游要构建实时数仓时，上游则可能需要实时的Stream ETL。这个过程会进行实时清洗或扩展数据，清洗完成后写入到下游的实时数仓的整个链路中，可保证数据查询的时效性，形成实时数据采集、实时数据处理以及下游的实时Query。</p><p><strong>搜索引擎推荐</strong><br>  搜索引擎这块以淘宝为例，当卖家上线新商品时，后台会实时产生消息流，该消息流经过Flink 系统时会进行数据的处理、扩展。然后将处理及扩展后的数据生成实时索引，写入到搜索引擎中。这样当淘宝卖家上线新商品时，能在秒级或者分钟级实现搜索引擎的搜索。</p><h4 id="4-2-Flink-场景应用：Data-Analytics"><a href="#4-2-Flink-场景应用：Data-Analytics" class="headerlink" title="4.2 Flink 场景应用：Data Analytics"></a>4.2 Flink 场景应用：Data Analytics</h4><p><img src="/images/2019/12/25/69d757d0-26e8-11ea-80df-39774ea2e31e.png" alt="image.png"><br>Data Analytics，如图，左边是Batch Analytics，右边是Streaming Analytics。Batch Analytics 就是传统意义上使用类似于Map Reduce、Hive、Spark Batch 等，对作业进行分析、处理、生成离线报表；Streaming Analytics 使用流式分析引擎如Storm、Flink 实时处理分析数据，应用较多的场景如实时大屏、实时报表。</p><h4 id="4-3-Flink-场景应用：Data-Driven"><a href="#4-3-Flink-场景应用：Data-Driven" class="headerlink" title="4.3 Flink 场景应用：Data Driven"></a>4.3 Flink 场景应用：Data Driven</h4><p><img src="/images/2019/12/25/d64efb70-26e8-11ea-80df-39774ea2e31e.png" alt="image.png"><br>从某种程度上来说，所有的实时的数据处理或者是流式数据处理都是属于Data Driven，流计算本质上是Data Driven 计算。应用较多的如风控系统，当风控系统需要处理各种各样复杂的规则时，Data Driven 就会把处理的规则和逻辑写入到Datastream 的API 或者是ProcessFunction 的API 中，然后将逻辑抽象到整个Flink 引擎，当外面的数据流或者是事件进入就会触发相应的规则，这就是Data Driven 的原理。在触发某些规则后，Data Driven 会进行处理或者是进行预警，这些预警会发到下游产生业务通知，这是Data Driven 的应用场景，Data Driven 在应用上更多应用于复杂事件的处理。</p><p>参考资料：<a href="https://ververica.cn/developers/flink-basic-tutorial-1-basic-concept/" target="_blank" rel="noopener">Apache Flink 零基础入门（一&amp;二）：基础概念解析</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、Apache-flink-基本概念，原理和架构&quot;&gt;&lt;a href=&quot;#一、Apache-flink-基本概念，原理和架构&quot; class=&quot;headerlink&quot; title=&quot;一、Apache flink 基本概念，原理和架构&quot;&gt;&lt;/a&gt;一、Apache fl
      
    
    </summary>
    
      <category term="学习" scheme="http://yoursite.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>flink 提交任务</title>
    <link href="http://yoursite.com/2019/12/19/flink-%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/"/>
    <id>http://yoursite.com/2019/12/19/flink-提交任务/</id>
    <published>2019-12-19T01:35:40.000Z</published>
    <updated>2020-05-07T06:53:02.418Z</updated>
    
    <content type="html"><![CDATA[<h1 id="flink-提交任务"><a href="#flink-提交任务" class="headerlink" title="flink 提交任务"></a>flink 提交任务</h1><h2 id="flink本地提交任务"><a href="#flink本地提交任务" class="headerlink" title="flink本地提交任务"></a>flink本地提交任务</h2><blockquote><p>一般在本地测试时，需要在本地自己电脑提交flink任务，本地提交分两种：<br> 1.通过命令行的方式提交flink任务：<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run ./examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>2.通过网页ip:8081提交任务<br>点击 add new 上传jar包<br><img src="/images/2019/12/19/3d365750-21ff-11ea-80df-39774ea2e31e.png" alt="image.png"></p></blockquote><blockquote><p>选择想要执行的jar包，点击主类，设置并行度，点击submit提交按钮<br><img src="/images/2019/12/19/1476e7b0-2201-11ea-80df-39774ea2e31e.png" alt="image.png"></p></blockquote><h2 id="flink集群提交任务"><a href="#flink集群提交任务" class="headerlink" title="flink集群提交任务"></a>flink集群提交任务</h2><blockquote><p>说明：flink集群采用oozie编写shell脚本在yarn上提交flink任务，其中flink版本1.8.1，oozie(5.1.0-cdh6.3.0),hue(4.2.0-cdh6.3.0)<br>并且集群采用了kerberos和ldap安全认证，由于oozie并未集成flink任务提交，所以需要配置kerberos认证</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1.上传jar包，flink配置文件以及kerberos的keytab到自己的工作目录</span><br><span class="line">2.配置kerberos认证，编辑flink-conf.yaml配置文件，新增配置</span><br><span class="line">security.kerberos.login.use-ticket-cache: false</span><br><span class="line">security.kerberos.login.keytab: ./conf/260164.keytab</span><br><span class="line">security.kerberos.login.principal: 260164@GREE.IO</span><br><span class="line">3.编写shell脚本</span><br><span class="line">#!/bin/bash</span><br><span class="line">env -i FLINK_CONF_DIR=./conf /lvm/data3/flink/flink-1.8.1/bin/flink run -m yarn-cluster -yqu root.dev1 -ynm test-flink./test.jar</span><br><span class="line">重新指定flink配置环境文件目录 -m 指定提交模式，-yqu 指定提交yarn队列，-ynm 指定flink任务名称。最后指定要运行的jar包</span><br><span class="line">需要注意的是shell脚本尽量自己手动写一遍，避免出现一些格式编码等奇怪问题</span><br><span class="line">4.创建oozie shell任务</span><br><span class="line">指定conf配置文件目录，shell脚本，以及上传的jar包</span><br><span class="line">指定shell的队列，默认是default队列</span><br></pre></td></tr></table></figure><h2 id="flink-集成-kafka-集群提交任务"><a href="#flink-集成-kafka-集群提交任务" class="headerlink" title="flink 集成 kafka 集群提交任务"></a>flink 集成 kafka 集群提交任务</h2><blockquote><p>由于cdh集群集成了kerberos 以及kafka集群也集成了kerberos 所以需要配置kafkad的jaas.conf文件和krb5文件<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">KafkaClient &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  doNotPrompt=true</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  storeKey=true</span><br><span class="line">  renewTicket=true</span><br><span class="line">  keyTab=&quot;./conf/xx.keytab&quot;</span><br><span class="line">  principal=&quot;xx@GREE.IO&quot;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Client &#123;</span><br><span class="line">  com.sun.security.auth.module.Krb5LoginModule required</span><br><span class="line">  useKeyTab=true</span><br><span class="line">  storeKey=true</span><br><span class="line">  keyTab=&quot;./conf/xx.keytab&quot;</span><br><span class="line">  principal=&quot;xx@GREE.IO&quot;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p></blockquote><p>krb5.conf一般都不需要更改，主要是配置kafka读取到jaas.conf文件，flink的配置则按照flink集群提交的方法完成就可以。<br><figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># Configuration snippets may be placed in this directory as well</span><br><span class="line">includedir /etc/krb5.conf.d/</span><br><span class="line"></span><br><span class="line">[logging]</span><br><span class="line">default = FILE:/lvm/data3/var/log/krb5libs.log</span><br><span class="line">kdc = FILE:/lvm/data3/var/log/krb5kdc.log</span><br><span class="line">admin_server = FILE:/lvm/data3/var/log/kadmind.log</span><br><span class="line"></span><br><span class="line">[libdefaults]</span><br><span class="line">dns_lookup_realm = false</span><br><span class="line">dns_lookup_kdc = false</span><br><span class="line">ticket_lifetime = 24h</span><br><span class="line">renew_lifetime = 7d</span><br><span class="line">forwardable = true</span><br><span class="line">udp_preference_limit = 1</span><br><span class="line">default_realm = GREE.IO</span><br><span class="line">pkinit_anchors = /etc/pki/tls/certs/ca-bundle.crt</span><br><span class="line"></span><br><span class="line">[realms]</span><br><span class="line">GREE.IO = &#123;</span><br><span class="line"> kdc = cdh-master01</span><br><span class="line"> admin_server = cdh-master01</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[domain_realm]</span><br><span class="line">.cdh-master01 = GREE.IO</span><br><span class="line">cdh-master01 = GREE.IO</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;flink-提交任务&quot;&gt;&lt;a href=&quot;#flink-提交任务&quot; class=&quot;headerlink&quot; title=&quot;flink 提交任务&quot;&gt;&lt;/a&gt;flink 提交任务&lt;/h1&gt;&lt;h2 id=&quot;flink本地提交任务&quot;&gt;&lt;a href=&quot;#flink本地提交任
      
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
</feed>
