<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>上善若水博客</title>
  
  <subtitle>大数据</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-11-30T01:13:54.521Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>上善若水</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>spring boot jpa</title>
    <link href="http://yoursite.com/2019/11/30/spring-boot-jpa/"/>
    <id>http://yoursite.com/2019/11/30/spring-boot-jpa/</id>
    <published>2019-11-30T01:04:38.000Z</published>
    <updated>2019-11-30T01:13:54.521Z</updated>
    
    <content type="html"><![CDATA[<p>spring boot jpa<br> jpa 常用于数据库访问</p><p>1.依赖</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;parent&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;        &lt;version&gt;2.1.5.RELEASE&lt;/version&gt;        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;    &lt;/parent&gt;    &lt;groupId&gt;com.gree.bdc&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-jpa&lt;/artifactId&gt;    &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;    &lt;name&gt;jpa-test&lt;/name&gt;    &lt;description&gt;jpa-test&lt;/description&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!-- spring boot begin--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;!--mysql 依赖--&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;8.0.11&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- log4j--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt;            &lt;version&gt;1.3.8.RELEASE&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- spring boot jpa --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><p>2.application.properties 配置，创建入口类。</p><pre><code>#通用数据源配置spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driverspring.datasource.url=jdbc:mysql://localhost:3306/springboot_jpa?charset=utf8mb4&amp;useSSL=false&amp;serverTimezone=GMTspring.datasource.username=rootspring.datasource.password=123456# Hikari 数据源专用配置spring.datasource.hikari.maximum-pool-size=20spring.datasource.hikari.minimum-idle=5# JPA 相关配置spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialectspring.jpa.show-sql=truespring.jpa.hibernate.ddl-auto=create</code></pre><pre><code>package com.gree.bdc.jpa;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class JpaApplication {    public static void main(String[] args) {        SpringApplication.run(JpaApplication.class,args);    }}</code></pre><p>3.创建实体</p><pre><code>package com.gree.bdc.jpa.entity;import javax.persistence.Column;import javax.persistence.Entity;import javax.persistence.Id;import javax.persistence.Table;@Entity@Table(name = &quot;AUTH_USER&quot;)public class UserDO {        @Id        private Long id;        @Column(length = 32)        private String name;        @Column(length = 32)        private String account;        @Column(length = 64)        private String pwd;        public Long getId() {            return id;        }        public void setId(Long id) {            this.id = id;        }        public String getName() {            return name;        }        public void setName(String name) {            this.name = name;        }        public String getAccount() {            return account;        }        public void setAccount(String account) {            this.account = account;        }        public String getPwd() {            return pwd;        }        public void setPwd(String pwd) {            this.pwd = pwd;        }}</code></pre><p>4.写一个DAO继承Repository或他的子类，执行入口类创建数据库表。后注释掉配置文件</p><p>spring.jpa.hibernate.ddl-auto=create</p><pre><code>package com.gree.bdc.jpa.repository;import com.gree.bdc.jpa.entity.UserDO;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.stereotype.Repository;@Repositorypublic interface UserDAO extends JpaRepository&lt;UserDO,Long&gt; {}</code></pre><p>5.使用DAO调用其操作数据库方法</p><pre><code>package com.gree.bdc.jpa;import com.gree.bdc.jpa.entity.UserDO;import com.gree.bdc.jpa.repository.UserDAO;import org.junit.After;import org.junit.Before;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;import java.util.List;@RunWith(SpringRunner.class)@SpringBootTestpublic class UserDOTest {    @Autowired    private UserDAO userDAO;    @Before    public void before(){        UserDO userDO = new UserDO();        userDO.setId(1L);        userDO.setName(&quot;风清扬&quot;);        userDO.setAccount(&quot;fengqy&quot;);        userDO.setPwd(&quot;123456&quot;);        userDAO.save(userDO);        userDO = new UserDO();        userDO.setId(3L);        userDO.setName(&quot;东方不败&quot;);        userDO.setAccount(&quot;bubai&quot;);        userDO.setPwd(&quot;123456&quot;);        userDAO.save(userDO);        userDO.setId(5L);        userDO.setName(&quot;向问天&quot;);        userDO.setAccount(&quot;wentian&quot;);        userDO.setPwd(&quot;123456&quot;);        userDAO.save(userDO);    }    @Test    public void testAdd(){        UserDO userDO = new UserDO();        userDO.setId(2L);        userDO.setName(&quot;任我行&quot;);        userDO.setAccount(&quot;renwox&quot;);        userDO.setPwd(&quot;123456&quot;);        userDAO.save(userDO);        userDO = new UserDO();        userDO.setId(4L);        userDO.setName(&quot;令狐冲&quot;);        userDO.setAccount(&quot;linghuc&quot;);        userDO.setPwd(&quot;123456&quot;);        userDAO.save(userDO);    }    @After    public void after(){        userDAO.deleteById(1L);        List&lt;UserDO&gt; users = userDAO.findAll();        for(UserDO user: users){            System.out.println(user.getName());        }    }}</code></pre><p>参考文档：<a href="https://www.jianshu.com/p/c14640b63653" target="_blank" rel="noopener">spring boot jpa</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;spring boot jpa&lt;br&gt; jpa 常用于数据库访问&lt;/p&gt;
&lt;p&gt;1.依赖&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;project 
      
    
    </summary>
    
      <category term="spring boot" scheme="http://yoursite.com/categories/spring-boot/"/>
    
    
      <category term="springboot" scheme="http://yoursite.com/tags/springboot/"/>
    
  </entry>
  
  <entry>
    <title>java 基本知识</title>
    <link href="http://yoursite.com/2019/11/29/java-%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/"/>
    <id>http://yoursite.com/2019/11/29/java-基本知识/</id>
    <published>2019-11-29T02:00:58.000Z</published>
    <updated>2019-11-30T03:09:48.230Z</updated>
    
    <content type="html"><![CDATA[<p>1.泛型<br>  通过泛型可以定义类型安全的数据结构（类型安全），而无须使用实际的数据类型（可扩展）。这能够显著提高性能并得到更高质量的代码（高性能），因为您可以重用数据处理算法，而无须复制类型特定的代码（可重用）。</p><p>2.java.sql.SQLException: Unknown system variable ‘query_cache_size’</p><pre><code> &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;</code></pre><p>3.java.sql.SQLException: The server time zone value ‘ÖÐ¹ú±ê×¼Ê±¼ä’ is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the serverTimezone configuration property) to use a more specifc time zone value if you want to utilize time zone support.</p><pre><code>jdbc:mysql://localhost:3306/springboot_jpa?charset=utf8mb4&amp;useSSL=false&amp;serverTimezone=GMT</code></pre><p>4.java.lang.Exception: No runnable methods</p><blockquote><p>测试时 未在test方法上加上 @test 注解，同时保证注解依赖org.junit.test</p></blockquote><p>5.STRING…</p><blockquote><p>类型后面三个点(String…)，是从Java 5开始，Java语言对方法参数支持一种新写法，叫可变长度参数列表，其语法就是类型后跟…，表示此处接受的参数为0到多个Object类型的对象，或者是一个Object[]。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1.泛型&lt;br&gt;  通过泛型可以定义类型安全的数据结构（类型安全），而无须使用实际的数据类型（可扩展）。这能够显著提高性能并得到更高质量的代码（高性能），因为您可以重用数据处理算法，而无须复制类型特定的代码（可重用）。&lt;/p&gt;
&lt;p&gt;2.java.sql.SQLExcept
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>spark 常用 api</title>
    <link href="http://yoursite.com/2019/11/26/spark-%E5%B8%B8%E7%94%A8-api/"/>
    <id>http://yoursite.com/2019/11/26/spark-常用-api/</id>
    <published>2019-11-26T03:20:57.000Z</published>
    <updated>2019-11-26T03:21:31.521Z</updated>
    
    <content type="html"><![CDATA[<p>1.spark 中采用部分关联字段</p><pre><code>//可以看出Temp_khcp 字段包含 first_chart 字段 ，通过部分关联来实现表连接  val alphabet_Beign = number_Beign.join(custom_file_regular, col(&quot;Temp_khcp&quot;).startsWith(col(&quot;first_chart&quot;)), &quot;left&quot;).drop(&quot;first_chart&quot;).na.fill(&quot;&quot;, Seq(&quot;before_file&quot;))</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1.spark 中采用部分关联字段&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;//可以看出Temp_khcp 字段包含 first_chart 字段 ，通过部分关联来实现表连接
  val alphabet_Beign = number_Beign.join(custom_file_r
      
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Docker 三剑客之 Docker Compose</title>
    <link href="http://yoursite.com/2019/10/25/Docker-%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8B-Docker-Compose/"/>
    <id>http://yoursite.com/2019/10/25/Docker-三剑客之-Docker-Compose/</id>
    <published>2019-10-25T00:55:10.000Z</published>
    <updated>2019-10-25T03:11:03.972Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><pre><code>Docker Compose 定位是 定义和运行多个docker容器应用（Defining and running multi-container Docker applications）它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）Compose 中有两个重要的概念：服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。</code></pre><h1 id="安装与卸载"><a href="#安装与卸载" class="headerlink" title="安装与卸载"></a>安装与卸载</h1><pre><code>1.安装$ sudo curl -L https://github.com/docker/compose/releases/download/1.17.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose2.卸载sudo rm /usr/local/bin/docker-compose</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;Docker Compose 定位是 定义和运行多个docker容器应用（Defining and running mult
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Docker 学习笔记</title>
    <link href="http://yoursite.com/2019/10/23/Docker-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/10/23/Docker-学习笔记/</id>
    <published>2019-10-23T07:13:40.000Z</published>
    <updated>2019-10-24T06:24:56.091Z</updated>
    
    <content type="html"><![CDATA[<h1 id="docker-安装"><a href="#docker-安装" class="headerlink" title="docker 安装"></a>docker 安装</h1><p><a href="https://www.runoob.com/docker/centos-docker-install.html" target="_blank" rel="noopener">docker 安装地址</a></p><h1 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h1><pre><code>1、docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.See &#39;docker run --help&#39;.可能docker上次为正常关闭导致： systemctl start docker </code></pre><h1 id="验证docker-安装成功"><a href="#验证docker-安装成功" class="headerlink" title="验证docker 安装成功"></a>验证docker 安装成功</h1><pre><code>docker run hello-world</code></pre><p><img alt="image.png" data-src="/images/2019/10/23/dc94e130-f564-11e9-84ab-7fe2af665eb5.png" class="lazyload"></p><h1 id="常见镜像操作"><a href="#常见镜像操作" class="headerlink" title="常见镜像操作"></a>常见镜像操作</h1><pre><code>1.获取镜像docker pull ubuntu:16.042.运行镜像,进入容器docker run -it --rm \ubuntu:16.04 \bash3. 退出容器exit4.列出镜像docker image ls5.查看镜像、容器、数据卷所占用的空间docker system df6.虚悬镜像docker image ls -f dangling=true7.删除虚悬镜像docker image prune8.中间层镜像docker image ls -a9.部分镜像列出docker image ls ubuntudocker image ls ubuntu:16.04docker image ls -f since=mongo:3.2docker image ls -f label=com.example.version=0.110. 特定格式显示docker image ls -qdocker image ls --format &quot;{{.ID}}: {{.Repository}}&quot;docker image ls --format &quot;table {{.ID}}\t{{.Repository}}\t{{.Tag}}&quot;</code></pre><h1 id="镜像构建"><a href="#镜像构建" class="headerlink" title="镜像构建"></a>镜像构建</h1><pre><code>$ mkdir mynginx$ cd mynginx$ touch DockerfileDockerfile内容:FROM nginxRUN echo &#39;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#39; &gt; /usr/share/nginx/html/index.html在Dockerfile所在的目录执行docker build -t nginx:v3 .</code></pre><h1 id="常见容器操作"><a href="#常见容器操作" class="headerlink" title="常见容器操作"></a>常见容器操作</h1><pre><code>1.新建并启动容器docker run ubuntu:16.04 /bin/echo &#39;Hello World!&#39;2.在终端打开容器docker run -t -i  ubuntu:16.04 /bin/bash  （其中，-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开）3. 启动已经终止的容器docker container start --help4.后台运行容器docker run -d  ubuntu:16.04 /bin/sh -c &quot;while true ;do echo hello world;sleep 1;done&quot;5.查看运行容器docker container ls6.查看容器的运行日志docker logs container iddocker logs 583d0660cf1364e7e52a996e0ccab94c2c9e1b88a831233e5777adf6d46846017.终止一个容器docker container stop container iddocker container stop 583d0660cf138.查看所有的容器docker container ls -a9.启动已经停止的容器docker container start container iddocker container start 583d0660cf1310.重启容器docker container restart</code></pre><h1 id="进入容器两种方式"><a href="#进入容器两种方式" class="headerlink" title="进入容器两种方式"></a>进入容器两种方式</h1><pre><code>1. attach 命令docker run  -dit ubuntudocker container lsdocker attach 14bf可以看出在退出容器是这个容器也会被停止。2. exec命令 docker exec -it f2a8 bash这个命令退出时不会停止容器。所有推荐使用这个命令进入容器中进行操作。</code></pre><h1 id="数据卷"><a href="#数据卷" class="headerlink" title="数据卷"></a>数据卷</h1><pre><code>1.创建数据卷docker volume create my-vol2.查看所有数据卷docker volume ls3.查看某个数据卷的具体信息docker volume inspect my-vol4.启动一个挂载数据卷的容器docker run -d -P \&gt; --name web \&gt; --mount source=my-vol,target=/webapp \&gt; training/webapp \&gt; python app.py5.删除数据卷docker volume rm my-vol如果报错：Error response from daemon: remove my-vol: volume is in use - [e6520e12082e077ff427d2ec87dc32c61bf30414570009a82363c706d5e2072e]则需要先停止正在使用数据卷的容器，并删除它。docker stop e6520e12082edocker rm e6520e12082e6.清除无主的数据卷docker volume prune</code></pre><h1 id="docker-构建-Tomcat"><a href="#docker-构建-Tomcat" class="headerlink" title="docker 构建 Tomcat"></a>docker 构建 Tomcat</h1><pre><code>1.搜索Tomcat镜像docker search tomcat2.拉取Tomcat镜像docker pull tomcat3.运行容器docker run --name tomcat -p 8080:8080 -v $PWD/test:/usr/local/tomcat/webapps/test -d tomcat命令说明：-p 8080:8080：将容器的8080端口映射到主机的8080端口-v $PWD/test:/usr/local/tomcat/webapps/test：将主机中当前目录下的test挂载到容器的/test</code></pre><h1 id="docker-构建-mysql"><a href="#docker-构建-mysql" class="headerlink" title="docker 构建 mysql"></a>docker 构建 mysql</h1><pre><code>1.搜索mysql镜像docker search mysql2.拉取mysql镜像docker pull mysql3.运行mysql镜像，构建mysql容器docker run -p 3306:3306 --name mysql \-v /usr/local/docker/mysql/conf:/etc/mysql \-v /usr/local/docker/mysql/logs:/var/log/mysql \-v /usr/local/docker/mysql/data:/var/lib/mysql \-e MYSQL_ROOT_PASSWORD=123456 \-d mysql命令参数：-p 3306:3306：将容器的3306端口映射到主机的3306端口-v /usr/local/docker/mysql/conf:/etc/mysql：将主机当前目录下的 conf 挂载到容器的 /etc/mysql-v /usr/local/docker/mysql/logs:/var/log/mysql：将主机当前目录下的 logs 目录挂载到容器的 /var/log/mysql-v /usr/local/docker/mysql/data:/var/lib/mysql：将主机当前目录下的 data 目录挂载到容器的 /var/lib/mysql-e MYSQL\_ROOT\_PASSWORD=123456：初始化root用户的密码</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;docker-安装&quot;&gt;&lt;a href=&quot;#docker-安装&quot; class=&quot;headerlink&quot; title=&quot;docker 安装&quot;&gt;&lt;/a&gt;docker 安装&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.runoob.com/docker/cen
      
    
    </summary>
    
      <category term="docker" scheme="http://yoursite.com/categories/docker/"/>
    
    
      <category term="容器" scheme="http://yoursite.com/tags/%E5%AE%B9%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>HBase 常用工具类</title>
    <link href="http://yoursite.com/2019/10/17/HBase-%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%B1%BB/"/>
    <id>http://yoursite.com/2019/10/17/HBase-常用工具类/</id>
    <published>2019-10-17T02:13:36.000Z</published>
    <updated>2019-10-17T02:13:37.040Z</updated>
    
    <content type="html"><![CDATA[<pre><code>object HBaseUtils {  def logger: Logger = LoggerFactory.getLogger(getClass)  /**    * 获取配置参数信息    * @return    */  def getHBaseConf : Configuration = {    val conf : Configuration = HBaseConfiguration.create    conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;cdh-master01,cdh-master02,cdh-master03&quot;)    conf.set(&quot;hadoop.security.authentication&quot;, &quot;Kerberos&quot;)    conf  }  /**    * 获取连接    * @param conf 配置信息    * @return    */  def getConnection(conf:Configuration): Connection ={    ConnectionFactory.createConnection(conf)  }  /**    * 获取管理员权限    * @param conn 连接信息    * @return HBaseAdmin    */  def getAdmin(conn:Connection) : HBaseAdmin = {    conn.getAdmin.asInstanceOf[HBaseAdmin]  }  /**    * 创建表    * @param admin 管理员    * @param tableName 表名    * @param columnFamily 列族    * @throws org.apache.hadoop.hbase.MasterNotRunningException 异常    * @throws org.apache.hadoop.hbase.ZooKeeperConnectionException 异常    * @throws java.io.IOException 异常    */  @throws(classOf[MasterNotRunningException])  @throws(classOf[ZooKeeperConnectionException])  @throws(classOf[IOException])  def createTable(admin: HBaseAdmin,tableName: String,columnFamily:Array[String]):Unit = {    val createTableName = TableName.valueOf(tableName)    if (admin.tableExists(createTableName)){      logger.info(tableName + &quot;table exists!&quot;)    }else{      val tableDesc = new HTableDescriptor(createTableName)            tableDesc.addCoprocessor(&quot;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&quot;)      for (singleColumnFamily &lt;- columnFamily){        val columnDesc = new HColumnDescriptor(singleColumnFamily)        tableDesc.addFamily(columnDesc)      } admin.createTable(tableDesc)      logger.info(tableName + &quot; create table success!&quot;)    }admin.close()  } /**    * 载入数据    * @param table HBase表    * @param rowKey 行键    * @param columnFamily 列族    * @param quorum 分布式信息    * @param value 数值    */  def addRow(table: Table,rowKey:String,columnFamily:String ,quorum:String,value:String): Unit ={    val rowPut:Put = new Put(Bytes.toBytes(rowKey))    if (value == null){      rowPut.addColumn(columnFamily.getBytes,quorum.getBytes,&quot;&quot;.getBytes)    }else{      rowPut.addColumn(columnFamily.getBytes,quorum.getBytes,value.getBytes)    }    table.put(rowPut)  }  /**    * 获取表数据    * @param table HBase表    * @param rowKey 行键    * @return HBase result    */  def getRow(table: Table,rowKey: String ):Result = {    val get:Get = new Get(Bytes.toBytes(rowKey))    val result:Result = table.get(get)    for (rowKv &lt;- result.rawCells()){      println(&quot;Family:&quot; + new String(rowKv.getFamilyArray,rowKv.getFamilyOffset,rowKv.getFamilyLength,&quot;UT-8&quot;))      println(&quot;Qualifier:&quot; + new String(rowKv.getQualifierArray,rowKv.getQualifierOffset,rowKv.getQualifierLength,&quot;UT-8&quot;))      println(&quot;TimeStamp:&quot; + rowKv.getTimestamp)      println(&quot;rowKey:&quot; + new String(rowKv.getRowArray,rowKv.getRowOffset,rowKv.getRowLength,&quot;UT-8&quot;))      println(&quot;Value:&quot; + new String(rowKv.getValueArray,rowKv.getValueOffset,rowKv.getValueLength,&quot;UT-8&quot;))   }    result  }  /**    * 批量添加数据    * @param table HBase表    * @param list 数据列表    */  def addDataBatch(table: Table,list: java.util.List[Put]): Unit = {    try{      table.put(list)    }catch{        logger.error(e.getMessage)      case e: IOException =&gt;        logger.error(e.getMessage)    }  }  /**    * 查询所有记录    * @param table HBase表    * @return resultScanner    */  def queryAll(table: Table):ResultScanner = {    val scan: Scan = new Scan    try {      val result: ResultScanner = table.getScanner(scan)      result    }catch {      case e: IOException =&gt;      logger.error(e.getMessage)        null    }  } /**    * 单条记录查询    * @param table HBase表    * @param queryColumn 查询列     * @param value 数值    * @param columns 列集合    * @return ResultScanner    */  def queryBySingleColumn(table: Table, queryColumn: String, value: String, columns: Array[String]): ResultScanner = {    if (columns == null || queryColumn == null || value == null ){      null    }else{      try {        val filter: SingleColumnValueFilter = new SingleColumnValueFilter(Bytes.toBytes(queryColumn),Bytes.toBytes(queryColumn),CompareOperator.EQUAL,new SubstringComparator(value))        val scan: Scan = new Scan        for (column  logger.error(e.getMessage)          null      }    }  }  /**    * 删除表    * @param hBaseConnection 链接    * @param tableName HBase表    */  def dropTable(hBaseConnection: Connection,tableName: String): Unit = {    try {      val admin: HBaseAdmin = hBaseConnection.getAdmin.asInstanceOf[HBaseAdmin]      admin.disableTable(TableName.valueOf(tableName))      admin.deleteTable(TableName.valueOf(tableName))    }catch {      case e: MasterNotRunningException =&gt; logger.error(e.getMessage)      case e: ZooKeeperConnectionException =&gt; logger.error(e.getMessage)      case e: IOException =&gt; logger.error(e.getMessage)    }  }}org.apache.hbase hbase-client${hbase.version}org.apache.hadoop hadoop-annotations                                            </code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;object HBaseUtils {
  def logger: Logger = LoggerFactory.getLogger(getClass)
  /**
    * 获取配置参数信息
    * @return    
*/ 
 def getH
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Apache NiFi 简单介绍和使用</title>
    <link href="http://yoursite.com/2019/09/11/Apache-NiFi-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2019/09/11/Apache-NiFi-简单介绍和使用/</id>
    <published>2019-09-11T06:59:39.000Z</published>
    <updated>2019-09-11T07:59:02.983Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、什么是Apache-NiFi"><a href="#一、什么是Apache-NiFi" class="headerlink" title="一、什么是Apache NiFi"></a>一、什么是Apache NiFi</h2><pre><code>简单来讲，NIFI就是为了构建系统之间的数据自动化传输的简易操作工具。提供了一些可靠的数据流的传输工具，解决了大部分现代企业中的数据ETL中所遇到的挑战。</code></pre><h2 id="二、NIFI-核心概念"><a href="#二、NIFI-核心概念" class="headerlink" title="二、NIFI 核心概念"></a>二、NIFI 核心概念</h2><p><img alt="image.png" data-src="/images/2019/09/11/7c2f6a40-d45d-11e9-b427-5d1ae987f58a.png" class="lazyload"><br><img alt="image.png" data-src="/images/2019/09/11/a544b020-d45d-11e9-b427-5d1ae987f58a.png" class="lazyload"></p><h2 id="三、NIFI-架构"><a href="#三、NIFI-架构" class="headerlink" title="三、NIFI 架构"></a>三、NIFI 架构</h2><p><img alt="image.png" data-src="/images/2019/09/11/dd2a1340-d45d-11e9-b427-5d1ae987f58a.png" class="lazyload"></p><h2 id="四、-NIFI-使用界面介绍"><a href="#四、-NIFI-使用界面介绍" class="headerlink" title="四、 NIFI 使用界面介绍"></a>四、 NIFI 使用界面介绍</h2><p><img alt="image.png" data-src="/images/2019/09/11/79ee7e20-d461-11e9-b427-5d1ae987f58a.png" class="lazyload"></p><blockquote><p>主要包括工具栏、状态栏、总菜单栏，操作缩图和画布。平常用到的组件主要有<br>Processor:The Processor is the NiFi component that is used to listen for incoming data; pull data from external sources; publish data to external sources; and route, transform, or extract information from FlowFiles.</p></blockquote><blockquote><p>Processor Group:When a dataflow becomes complex, it often is beneficial to reason about the dataflow at a higher, more abstract level. NiFi allows multiple components, such as Processors, to be grouped together into a Process Group. The NiFi User Interface then makes it easy for a DFM to connect together multiple Process Groups into a logical dataflow, as well as allowing the DFM to enter a Process Group in order to see and manipulate the components within the Process Group.</p></blockquote><blockquote><p>我认为 Processor 就是一个数据处理器，它不仅可以从各种数据源中加载数据，转成Fiedflow基本nifi元素，还能够将数据处理成各种常见的格式以及将数据输出到各种数据库中<br>而Processor Group 则是一个装载 Processor的容器。</p></blockquote><h2 id="五、案例（JSON-TO-MYSQL）"><a href="#五、案例（JSON-TO-MYSQL）" class="headerlink" title="五、案例（JSON_TO_MYSQL）"></a>五、案例（JSON_TO_MYSQL）</h2><ol><li>创建一个Processor Group ,将其拖到画布当中<br><img alt="image.png" data-src="/images/2019/09/11/b8052b50-d466-11e9-b427-5d1ae987f58a.png" class="lazyload"></li><li>然后修改名字<br><img alt="image.png" data-src="/images/2019/09/11/f0cefb00-d466-11e9-b427-5d1ae987f58a.png" class="lazyload"><br><img alt="image.png" data-src="/images/2019/09/11/52a49ab0-d467-11e9-b427-5d1ae987f58a.png" class="lazyload"></li><li>读取json文件，GetFile –&gt; 将JSON 转为sql语句，ConvertJSONToSQL –&gt; 运行sql语句，PutSQL</li><li>GetFile 设置输入目录Input Directory，目标文件 File Filter<br><img alt="image.png" data-src="/images/2019/09/11/5e82d2b0-d468-11e9-b427-5d1ae987f58a.png" class="lazyload"></li><li>ConvertJSONToSQL  设置数据库连接池JDBC Connection Pool ,SQL 类型 Statement Type ,表名 Table Name,其他默认 ，注意 ConvertJSONToSQL 只适用一个json值的文件<br><img alt="image.png" data-src="/images/2019/09/11/3459dd70-d469-11e9-b427-5d1ae987f58a.png" class="lazyload"></li><li>PutSQL 设置 数据库连接池，SQL Statement ，其他默认。一般来说只需要设置加黑的参数，但这里需要设置sql语句。<br><img alt="image.png" data-src="/images/2019/09/11/c980d930-d469-11e9-b427-5d1ae987f58a.png" class="lazyload"></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、什么是Apache-NiFi&quot;&gt;&lt;a href=&quot;#一、什么是Apache-NiFi&quot; class=&quot;headerlink&quot; title=&quot;一、什么是Apache NiFi&quot;&gt;&lt;/a&gt;一、什么是Apache NiFi&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;简单来讲，N
      
    
    </summary>
    
      <category term="ETL" scheme="http://yoursite.com/categories/ETL/"/>
    
      <category term="NIFI" scheme="http://yoursite.com/categories/ETL/NIFI/"/>
    
    
      <category term="ETL" scheme="http://yoursite.com/tags/ETL/"/>
    
      <category term="NIFI" scheme="http://yoursite.com/tags/NIFI/"/>
    
  </entry>
  
  <entry>
    <title>NIFI 安装部署</title>
    <link href="http://yoursite.com/2019/09/09/NIFI-%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2019/09/09/NIFI-安装部署/</id>
    <published>2019-09-09T01:43:41.000Z</published>
    <updated>2019-09-11T06:14:07.169Z</updated>
    
    <content type="html"><![CDATA[<p>::: hljs-center</p><p>NIFI 安装部署</p><p>:::</p><h2 id="一、NIFI-下载"><a href="#一、NIFI-下载" class="headerlink" title="一、NIFI 下载"></a>一、NIFI 下载</h2><ul><li>NIFI 下载链接：<a href="http://nifi.apache.org/download.html" target="_blank" rel="noopener">NIFI Downloads</a></li><li>选择与自己环境匹配的安装包</li><li><img alt="image.png" data-src="/images/2019/09/09/b6d2a810-d2a0-11e9-aa7e-85cf9ceb3292.png" class="lazyload"></li><li>若是下载速度过慢，可以选择另一个下载地址：<a href="https://www.apache.org/dyn/closer.lua?path=/nifi/1.9.2/nifi-1.9.2-bin.zip" target="_blank" rel="noopener">NIFI 下载</a></li><li><img alt="image.png" data-src="/images/2019/09/09/7bb8b160-d2a1-11e9-aa7e-85cf9ceb3292.png" class="lazyload"></li></ul><h2 id="二、NIFI-安装"><a href="#二、NIFI-安装" class="headerlink" title="二、NIFI 安装"></a>二、NIFI 安装</h2><ol><li>将下载的安装包放到 E:\software\ ，然后解压安装，非常简单</li><li>修改配置文件E:\software\nifi-1.9.2-bin\nifi-1.9.2\conf\nifi.properties，更改端口号</li><li><img alt="image.png" data-src="/images/2019/09/09/6df7ebd0-d2a2-11e9-aa7e-85cf9ceb3292.png" class="lazyload"></li></ol><h2 id="三-、NIFI-运行"><a href="#三-、NIFI-运行" class="headerlink" title="三 、NIFI 运行"></a>三 、NIFI 运行</h2><ol><li>进入E:\software\nifi-1.9.2-bin\nifi-1.9.2\bin 目录，双击run-nifi.bat，若是linux则sh nifi.sh</li><li>打开浏览器，进入<a href="http://localhost:8081/nifi/，可能需要等会页面加载慢" target="_blank" rel="noopener">http://localhost:8081/nifi/，可能需要等会页面加载慢</a></li><li><img alt="image.png" data-src="/images/2019/09/09/2a0a96b0-d2a3-11e9-aa7e-85cf9ceb3292.png" class="lazyload"></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;::: hljs-center&lt;/p&gt;
&lt;p&gt;NIFI 安装部署&lt;/p&gt;
&lt;p&gt;:::&lt;/p&gt;
&lt;h2 id=&quot;一、NIFI-下载&quot;&gt;&lt;a href=&quot;#一、NIFI-下载&quot; class=&quot;headerlink&quot; title=&quot;一、NIFI 下载&quot;&gt;&lt;/a&gt;一、NIFI 下
      
    
    </summary>
    
      <category term="ETL" scheme="http://yoursite.com/categories/ETL/"/>
    
      <category term="NIFI" scheme="http://yoursite.com/categories/ETL/NIFI/"/>
    
    
      <category term="ETL" scheme="http://yoursite.com/tags/ETL/"/>
    
      <category term="NIFI" scheme="http://yoursite.com/tags/NIFI/"/>
    
  </entry>
  
  <entry>
    <title>maven 打包相关记录</title>
    <link href="http://yoursite.com/2019/09/05/maven-%E6%89%93%E5%8C%85%E7%9B%B8%E5%85%B3%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2019/09/05/maven-打包相关记录/</id>
    <published>2019-09-05T02:20:06.000Z</published>
    <updated>2019-09-05T02:23:17.349Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Maven项目打包"><a href="#Maven项目打包" class="headerlink" title="Maven项目打包"></a>Maven项目打包</h1><pre><code>这里主要记录常用的打包方式（将依赖打包进jar包）。</code></pre><h2 id="一、spring-boot-项目"><a href="#一、spring-boot-项目" class="headerlink" title="一、spring boot 项目"></a>一、spring boot 项目</h2><p>对于spring boot项目只需要添加spring boot 和maven整合的插件就可以</p><pre><code> &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;parent&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;        &lt;version&gt;2.1.5.RELEASE&lt;/version&gt;        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;    &lt;/parent&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;    &lt;/properties&gt;    &lt;!-- spring boot 依赖 --&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;            &lt;exclusions&gt;                &lt;exclusion&gt;                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                    &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;                &lt;/exclusion&gt;            &lt;/exclusions&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;!-- spring boot end --&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><h2 id="二、普通的java项目"><a href="#二、普通的java项目" class="headerlink" title="二、普通的java项目"></a>二、普通的java项目</h2><p>对于普通的java项目需要添加额外的依赖才能够将依赖打包进jar中</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;                &lt;version&gt;2.5.5&lt;/version&gt;                &lt;configuration&gt;                    &lt;archive&gt;                        &lt;manifest&gt;                            &lt;mainClass&gt;com.gree.cn.location.LocationMap&lt;/mainClass&gt;                        &lt;/manifest&gt;                    &lt;/archive&gt;                    &lt;descriptorRefs&gt;                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;                    &lt;/descriptorRefs&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;make-assembly&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;single&lt;/goal&gt;                        &lt;/goals&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                &lt;version&gt;2.3.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;source&gt;1.8&lt;/source&gt;                    &lt;target&gt;1.8&lt;/target&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><h2 id="三、scala-项目打包"><a href="#三、scala-项目打包" class="headerlink" title="三、scala 项目打包"></a>三、scala 项目打包</h2><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;                &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;                &lt;version&gt;2.15.2&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;scala-compile-first&lt;/id&gt;                        &lt;goals&gt;                            &lt;goal&gt;compile&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;includes&gt;                                &lt;include&gt;**/*.scala&lt;/include&gt;                            &lt;/includes&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                    &lt;execution&gt;                        &lt;id&gt;scala-test-compile&lt;/id&gt;                        &lt;goals&gt;                            &lt;goal&gt;testCompile&lt;/goal&gt;                        &lt;/goals&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;                    &lt;descriptorRefs&gt;                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;                    &lt;/descriptorRefs&gt;                    &lt;archive&gt;                        &lt;manifest&gt;                            &lt;mainClass&gt;com.gree.cn.write2mysql.Write2mysql&lt;/mainClass&gt;                        &lt;/manifest&gt;                    &lt;/archive&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;make-assembly&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;single&lt;/goal&gt;                        &lt;/goals&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Maven项目打包&quot;&gt;&lt;a href=&quot;#Maven项目打包&quot; class=&quot;headerlink&quot; title=&quot;Maven项目打包&quot;&gt;&lt;/a&gt;Maven项目打包&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;这里主要记录常用的打包方式（将依赖打包进jar包）。
&lt;/code&gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>flink 基础</title>
    <link href="http://yoursite.com/2019/08/30/flink-%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2019/08/30/flink-基础/</id>
    <published>2019-08-30T08:54:39.000Z</published>
    <updated>2019-09-04T09:31:17.064Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Apache-Flink"><a href="#一、Apache-Flink" class="headerlink" title="一、Apache Flink"></a>一、Apache Flink</h2><h3 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h3><pre><code>Apache Flink 是一个分布式大数据处理引擎，可以对有限流和无线流进行有状态和无状态进行计算，能够在各种集群环境上部署，对各种大小的数据集进行快速计算。</code></pre><h3 id="flink-启动命令"><a href="#flink-启动命令" class="headerlink" title="flink 启动命令"></a>flink 启动命令</h3><pre><code>linux./bin/start-cluster.shwindows.\start-cluster.bat</code></pre><p><img alt="image.png" data-src="/images/2019/08/30/063b0630-cb04-11e9-bf46-8baad6d0e743.png" class="lazyload"></p><h3 id="flink-maven-项目依赖"><a href="#flink-maven-项目依赖" class="headerlink" title="flink maven 项目依赖"></a>flink maven 项目依赖</h3><p>java版本：</p><pre><code>&lt;dependency&gt;  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;  &lt;artifactId&gt;flink-java&lt;/artifactId&gt;  &lt;version&gt;1.8.0&lt;/version&gt;  &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;  &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;  &lt;version&gt;1.8.0&lt;/version&gt;  &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;</code></pre><p>Scala版本：</p><pre><code>&lt;dependency&gt;  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;  &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;  &lt;version&gt;1.8.0&lt;/version&gt;  &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt;  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;  &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;  &lt;version&gt;1.8.0&lt;/version&gt;  &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;</code></pre><p>flink 关联 kafka 依赖</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;    &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt;    &lt;version&gt;1.8.0&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>flink 推荐使用shade插件打包maven项目</p><pre><code>&lt;build&gt;    &lt;plugins&gt;        &lt;plugin&gt;            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;            &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;            &lt;version&gt;3.0.0&lt;/version&gt;            &lt;executions&gt;                &lt;execution&gt;                    &lt;phase&gt;package&lt;/phase&gt;                    &lt;goals&gt;                        &lt;goal&gt;shade&lt;/goal&gt;                    &lt;/goals&gt;                    &lt;configuration&gt;                        &lt;artifactSet&gt;                            &lt;excludes&gt;                                &lt;exclude&gt;com.google.code.findbugs:jsr305&lt;/exclude&gt;                                &lt;exclude&gt;org.slf4j:*&lt;/exclude&gt;                                &lt;exclude&gt;log4j:*&lt;/exclude&gt;                            &lt;/excludes&gt;                        &lt;/artifactSet&gt;                        &lt;filters&gt;                            &lt;filter&gt;                                &lt;!-- Do not copy the signatures in the META-INF folder.                                Otherwise, this might cause SecurityExceptions when using the JAR. --&gt;                                &lt;artifact&gt;*:*&lt;/artifact&gt;                                &lt;excludes&gt;                                    &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;                                    &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;                                    &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;                                &lt;/excludes&gt;                            &lt;/filter&gt;                        &lt;/filters&gt;                        &lt;transformers&gt;                            &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;                                &lt;mainClass&gt;my.programs.main.clazz&lt;/mainClass&gt;                            &lt;/transformer&gt;                        &lt;/transformers&gt;                    &lt;/configuration&gt;                &lt;/execution&gt;            &lt;/executions&gt;        &lt;/plugin&gt;    &lt;/plugins&gt;&lt;/build&gt;</code></pre><h3 id="flink-基础-API-概念"><a href="#flink-基础-API-概念" class="headerlink" title="flink 基础 API 概念"></a>flink 基础 API 概念</h3><blockquote><p>DataSet and DataStream:<br>    Flink具有特殊类DataSet并DataStream在程序中表示数据。您可以将它们视为可以包含重复项的不可变数据集合。在DataSet数据有限的情况下，对于一个DataStream元素的数量可以是无界的。</p></blockquote><h3 id="flink-一般处理步骤"><a href="#flink-一般处理步骤" class="headerlink" title="flink 一般处理步骤"></a>flink 一般处理步骤</h3><ol><li>获得一个execution environment，</li><li>加载/创建初始数据，</li><li>指定此数据的转换，</li><li>指定放置计算结果的位置，</li><li>触发程序执行</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、Apache-Flink&quot;&gt;&lt;a href=&quot;#一、Apache-Flink&quot; class=&quot;headerlink&quot; title=&quot;一、Apache Flink&quot;&gt;&lt;/a&gt;一、Apache Flink&lt;/h2&gt;&lt;h3 id=&quot;定义：&quot;&gt;&lt;a href=&quot;#定义
      
    
    </summary>
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="流处理" scheme="http://yoursite.com/tags/%E6%B5%81%E5%A4%84%E7%90%86/"/>
    
      <category term="flink" scheme="http://yoursite.com/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>spark structed streaming 读取kafka数据</title>
    <link href="http://yoursite.com/2019/08/27/spark-structed-streaming-%E8%AF%BB%E5%8F%96kafka%E6%95%B0%E6%8D%AE/"/>
    <id>http://yoursite.com/2019/08/27/spark-structed-streaming-读取kafka数据/</id>
    <published>2019-08-27T02:34:36.000Z</published>
    <updated>2019-08-27T03:09:29.917Z</updated>
    
    <content type="html"><![CDATA[<p>structed streaming :</p><blockquote><p>Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine.</p></blockquote><p>Structed streaming 官方文档 <a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">2.4.3官方文档</a></p><h3 id="一、部分关于如何连接kafka数据，并做计算。"><a href="#一、部分关于如何连接kafka数据，并做计算。" class="headerlink" title="一、部分关于如何连接kafka数据，并做计算。"></a>一、部分关于如何连接kafka数据，并做计算。</h3><p>首先引入pom.xml</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;com.gree.cn&lt;/groupId&gt;    &lt;artifactId&gt;structstreaminglocation&lt;/artifactId&gt;    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;    &lt;name&gt;StructStreamingLocation&lt;/name&gt;    &lt;description&gt;Demo project for StructStreamingLocation&lt;/description&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!-- spark 相关依赖 --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;            &lt;version&gt;2.4.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;            &lt;version&gt;2.4.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- sparkStreaming kafka 整合--&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;            &lt;version&gt;2.4.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;            &lt;version&gt;2.4.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- sparkStreaming kafka end --&gt;        &lt;!-- struct streaming kafka --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt;            &lt;version&gt;2.4.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- struct streaming kafka 结束--&gt;        &lt;!-- spark 相关依赖 --&gt;        &lt;!-- 数据库连接--&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;8.0.12&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;druid&lt;/artifactId&gt;            &lt;version&gt;1.1.12&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- 数据库连接 end --&gt;        &lt;!-- Redis客户端 --&gt;        &lt;dependency&gt;            &lt;groupId&gt;redis.clients&lt;/groupId&gt;            &lt;artifactId&gt;jedis&lt;/artifactId&gt;            &lt;version&gt;2.7.1&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- 常用工具 --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;version&gt;1.18.2&lt;/version&gt;            &lt;optional&gt;true&lt;/optional&gt;        &lt;/dependency&gt;        &lt;!-- 常用工具  end --&gt;    &lt;/dependencies&gt;    &lt;build&gt;       &lt;plugins&gt;           &lt;plugin&gt;               &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;               &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;               &lt;version&gt;2.5.5&lt;/version&gt;               &lt;configuration&gt;                   &lt;archive&gt;                       &lt;manifest&gt;                           &lt;mainClass&gt;com.gree.cn.location.LocationMap&lt;/mainClass&gt;                       &lt;/manifest&gt;                   &lt;/archive&gt;                   &lt;descriptorRefs&gt;                       &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;                   &lt;/descriptorRefs&gt;               &lt;/configuration&gt;               &lt;executions&gt;                   &lt;execution&gt;                       &lt;id&gt;make-assembly&lt;/id&gt;                       &lt;phase&gt;package&lt;/phase&gt;                       &lt;goals&gt;                           &lt;goal&gt;single&lt;/goal&gt;                       &lt;/goals&gt;                   &lt;/execution&gt;               &lt;/executions&gt;           &lt;/plugin&gt;           &lt;plugin&gt;               &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;               &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;               &lt;version&gt;2.3.1&lt;/version&gt;               &lt;configuration&gt;                   &lt;source&gt;1.8&lt;/source&gt;                   &lt;target&gt;1.8&lt;/target&gt;               &lt;/configuration&gt;           &lt;/plugin&gt;       &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><p>然后编写相应的代码逻辑：</p><pre><code>package com.gree.cn.location;import com.gree.cn.entity.*;import com.gree.cn.sink.BaseHeartSink;import com.gree.cn.sink.LabelSink;import com.gree.cn.sink.MaterialInfoSink;import com.gree.cn.sink.MaterialRedisSink;import com.gree.cn.utils.DruidPoolUtils;import org.apache.spark.api.java.function.MapFunction;import org.apache.spark.sql.*;import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;import org.apache.spark.sql.streaming.StreamingQuery;import org.apache.spark.sql.streaming.StreamingQueryException;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.util.ArrayList;import java.util.Collections;import java.util.Properties;import static org.apache.spark.sql.functions.*;public class LocationMap {    public static void main(String[] args) {        //创建SparkSession        SparkSession spark = SparkSession                .builder()                .appName(&quot;小车定位程序&quot;)                //.master(&quot;local[2]&quot;)                .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)                .config(&quot;spark.local.dir&quot;, &quot;/lvm/data1/bluetooth_location/spark/location&quot;)                .config(&quot;spark.ui.port&quot;, &quot;8083&quot;)                .config(&quot;spark.sql.shuffle.partitions&quot;, &quot;50&quot;)                .config(&quot;spark.default.parallelism&quot;, &quot;12&quot;)                .getOrCreate();        //读取kafka数据        Dataset&lt;Row&gt; kafkaValue = spark.readStream()                .format(&quot;kafka&quot;)                .option(&quot;kafka.bootstrap.servers&quot;, &quot;10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092&quot;)                .option(&quot;subscribe&quot;, &quot;topic0619&quot;)                //.option(&quot;kafka.bootstrap.servers&quot;,&quot;172.16.247.129:9092&quot;)                //.option(&quot;subscribe&quot;,&quot;test06&quot;)                .option(&quot;startingOffsets&quot;, &quot;latest&quot;)                .load()                .selectExpr(&quot;CAST(value AS STRING)&quot;);        //基站数据包括 接收时间，基站ID，信标ID，信号强度        Dataset&lt;BaseTable&gt; baseTableDs = kafkaValue.as(Encoders.STRING()).map(                (MapFunction&lt;String, BaseTable&gt;) values -&gt; {                    BaseTable baseTable = new BaseTable();            //具体逻辑                      ...            ...            ...                    return baseTable;                } , ExpressionEncoder.javaBean(BaseTable.class));        //获取信标电量表数据        Dataset&lt;LabelData&gt; labelData = baseTableDs                .filter(col(&quot;orderName&quot;).equalTo(&quot;66&quot;))                .select(col(&quot;labelID&quot;), col(&quot;electricity&quot;),                        col(&quot;baseID&quot;),col(&quot;recDatetime&quot;))                .as(ExpressionEncoder.javaBean(LabelData.class));        /*StreamingQuery labelDataStart =*/ labelData.repartition(2, col(&quot;labelID&quot;))                .writeStream()                .outputMode(&quot;append&quot;)                .foreach(new LabelSink())                .start();        //labelData.writeStream().format(&quot;console&quot;).start();}</code></pre><h3 id="二、如何将数据sink到mysql"><a href="#二、如何将数据sink到mysql" class="headerlink" title="二、如何将数据sink到mysql"></a>二、如何将数据sink到mysql</h3><pre><code>package com.gree.cn.sink;import com.gree.cn.entity.LabelData;import com.gree.cn.utils.DruidPoolUtils;import org.apache.spark.sql.ForeachWriter;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.SQLException;public class LabelSink extends ForeachWriter&lt;LabelData&gt; {    /**     * sink 信标数据到MySQL中     *     */    private PreparedStatement statement;    private Connection connection;    public boolean open(long arg0, long arg1) {        return true;    }    public void process(LabelData labelData) {        try {            connection = DruidPoolUtils.getConnection();            String insertSql = &quot; REPLACE INTO bluetooth_location.beacon_info (beaconId,electricity,stationId,updateTime) VALUES(?,?,?,?) &quot;;            statement = connection.prepareStatement(insertSql);            statement.setString(1,labelData.getLabelID());            statement.setInt(2,labelData.getElectricity());            statement.setString(3,labelData.getBaseID());            statement.setString(4,labelData.getRecDatetime());            statement.executeUpdate();        } catch (SQLException e) {            e.printStackTrace();        }finally {            DruidPoolUtils.close(connection,statement);        }    }    @Override    public void close(Throwable arg0) {    }}</code></pre><h3 id="三、数据库连接池配置"><a href="#三、数据库连接池配置" class="headerlink" title="三、数据库连接池配置"></a>三、数据库连接池配置</h3><pre><code>package com.gree.cn.utils;import com.alibaba.druid.pool.DruidDataSourceFactory;import javax.sql.DataSource;import java.sql.Connection;import java.sql.ResultSet;import java.sql.SQLException;import java.sql.Statement;import java.util.Properties;/** * 数据库连接池工具类 */public class DruidPoolUtils {    //创建成员变量    private static DataSource dataSource;    //加载配置文件    static {        try {            Properties properties = new Properties();            //加载类路径            properties.load(DruidPoolUtils.class.getResourceAsStream(&quot;/druid.properties&quot;));            //读取属性文件，创建连接池            dataSource = DruidDataSourceFactory.createDataSource(properties);        }catch (Exception e){            e.printStackTrace();        }    }    //获取数据源    public static DataSource getDataSource(){        return dataSource;    }    //获取连接对象    public static Connection getConnection(){        try {            return dataSource.getConnection();        }catch (SQLException e){            throw new RuntimeException(e);        }    }    //释放资源    public static void close(Connection connection, Statement statement, ResultSet resultSet){        if (resultSet != null ){            try {                resultSet.close();            }catch (SQLException e){                e.printStackTrace();            }        }        if (statement != null){            try {                statement.close();            }catch (SQLException e){                e.printStackTrace();            }        }        if (connection != null){            try {                connection.close();            }catch (SQLException e){                e.printStackTrace();            }        }    }    public static void close(Connection connection,Statement statement){        close(connection,statement,null);    }}</code></pre><p>druid.properties</p><blockquote><p>driverClassName=com.mysql.cj.jdbc.Driver<br>url=jdbc:mysql://localhost:3306?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false<br>username=root<br>password=lyxbdw<br>initialSize=5<br>maxActive=20<br>maxWait=3000<br>minIdle=3</p></blockquote><h2 id="Scala编写structed-stremaing"><a href="#Scala编写structed-stremaing" class="headerlink" title="Scala编写structed stremaing"></a>Scala编写structed stremaing</h2><h3 id="引入pom-xml"><a href="#引入pom-xml" class="headerlink" title="引入pom.xml"></a>引入pom.xml</h3><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;com.gree.cn&lt;/groupId&gt;    &lt;artifactId&gt;write2mysql&lt;/artifactId&gt;    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;    &lt;name&gt;write2mysql&lt;/name&gt;    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;!-- 常用插件 --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;            &lt;version&gt;1.18.6&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- 常用插件 结束 --&gt;        &lt;!-- spark 相关依赖 --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;            &lt;version&gt;2.4.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;            &lt;version&gt;2.4.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- spark streaming kafka --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;            &lt;version&gt;2.4.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;            &lt;version&gt;2.4.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- spark streaming kafka 结束--&gt;        &lt;!-- struct streaming kafka --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;            &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt;            &lt;version&gt;2.4.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- struct streaming kafka 结束--&gt;        &lt;!-- spark 相关依赖 结束--&gt;        &lt;!-- 数据库连接--&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;8.0.15&lt;/version&gt;            &lt;scope&gt;runtime&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;            &lt;artifactId&gt;druid&lt;/artifactId&gt;            &lt;version&gt;1.1.12&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- 数据库连接 end --&gt;        &lt;!-- log 日志--&gt;        &lt;dependency&gt;            &lt;groupId&gt;log4j&lt;/groupId&gt;            &lt;artifactId&gt;log4j&lt;/artifactId&gt;            &lt;version&gt;1.2.17&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- log 日志结束--&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;                &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;                &lt;version&gt;2.15.2&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;scala-compile-first&lt;/id&gt;                        &lt;goals&gt;                            &lt;goal&gt;compile&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;includes&gt;                                &lt;include&gt;**/*.scala&lt;/include&gt;                            &lt;/includes&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                    &lt;execution&gt;                        &lt;id&gt;scala-test-compile&lt;/id&gt;                        &lt;goals&gt;                            &lt;goal&gt;testCompile&lt;/goal&gt;                        &lt;/goals&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;                &lt;configuration&gt;                    &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;                    &lt;descriptorRefs&gt;                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;                    &lt;/descriptorRefs&gt;                    &lt;archive&gt;                        &lt;manifest&gt;                            &lt;mainClass&gt;com.gree.cn.write2mysql.Write2mysql&lt;/mainClass&gt;                        &lt;/manifest&gt;                    &lt;/archive&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;make-assembly&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;single&lt;/goal&gt;                        &lt;/goals&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><p>与java的差别主要是打包插件不同，编写相应逻辑</p><pre><code>package com.gree.cn.write2mysqlimport com.gree.cn.sink.MysqlSinkimport org.apache.spark.sql.SparkSession/**  * 原始数据保存  * @author sfz  * Time 2019-08-08  */object Write2mysql {  def main(args: Array[String]): Unit = {    //创建SparkSession    val spark = SparkSession      .builder()      .appName(&quot;write2mysql&quot;)      .master(&quot;local[*]&quot;)      .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)      .config(&quot;spark.local.dir&quot;, &quot;/lvm/data1/bluetooth_location/spark/write2mysql&quot;)      .config(&quot;spark.ui.port&quot;, &quot;8084&quot;)      .getOrCreate()    //引入隐式转换，保证String可以序列化    import spark.implicits._    //读取kafka数据    val kafkaValue = spark.readStream      .format(&quot;kafka&quot;)      //.option(&quot;kafka.bootstrap.servers&quot;, &quot;10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092&quot;)     // .option(&quot;subscribe&quot;, &quot;topic0619&quot;)      .option(&quot;kafka.bootstrap.servers&quot;,&quot;localhost:9092&quot;)      .option(&quot;subscribe&quot;,&quot;test06&quot;)      .option(&quot;startingOffsets&quot;, &quot;latest&quot;)      .load()      .selectExpr(&quot;CAST(value AS STRING)&quot;)    //将kafka数据转化为BaseTable结构    val baseData = kafkaValue.as[String].map(values =&gt; {//具体的解析数据逻辑。。。。。。。。。        BaseTable.apply(frameHeader, equipment, frameLength, orderName, baseID, labelID, signalPower, electricity, datetime, checkName, recDatetime)      }      else {        BaseTable.apply(frameHeader, equipment, frameLength, orderName, baseID77, labelID, signalPower, electricity, datetime, checkName, recDatetime)      }    }).as[BaseTable]    //将数据sink到mysql数据库中    val query =  baseData.writeStream      .foreach(new MysqlSink)      .outputMode(&quot;append&quot;)      .start()    query.awaitTermination()  }  //创建BaseTable结构  case class BaseTable(frameHeader: String, equipment: String, frameLength: String, orderName: String, baseId: String,                       labelId: String, signalPower: Integer, electricity: Integer, datetime: String, checkName: String, recDatetime: String)}</code></pre><p>接着sink到mysql 数据库中</p><pre><code>package com.gree.cn.sinkimport java.sql.{Connection, PreparedStatement, SQLException}import com.gree.cn.utils.{DateUtils, MysqlPoolUtils}import com.gree.cn.write2mysql.Write2mysql.BaseTableimport org.apache.spark.sql.ForeachWriterclass MysqlSink() extends ForeachWriter[BaseTable](){  var conn:Connection = _  var statement : PreparedStatement = _  override def open(partitionId: Long, epochId: Long): Boolean = {     conn = MysqlPoolUtils.getConnection.get     conn.setAutoCommit(false)     true  }  override def process(baseTable: BaseTable): Unit = {    val insertSql = &quot; &quot;    statement = conn.prepareStatement(insertSql)    statement.setString(1, baseTable.frameHeader) 。。。。。。    statement.setString(11, baseTable.recDatetime)    statement.executeUpdate()  }  override def close(errorOrNull: Throwable): Unit = {    try{      if ( statement != null &amp;&amp; conn != null) {        statement.close()        conn.close()      }    } catch {      case e: SQLException =&gt;        e.printStackTrace()    }  }}</code></pre><p>最后配上数据库连接池</p><pre><code>package com.gree.cn.utilsimport java.sql.Connectionimport java.util.Propertiesimport com.alibaba.druid.pool.DruidDataSourceFactoryimport javax.sql.DataSourceimport org.apache.log4j.Logger/**  * @author sfz  * MySql 数据库连接池  */object MysqlPoolUtils {private val log = Logger.getLogger(MysqlPoolUtils.getClass.getName)val dataSource:Option[DataSource] = {  try {    val druidProps = new Properties()    //获取Druid连接池配置文件    val druidConfig = getClass.getResourceAsStream(&quot;/application.properties&quot;)    //加载配置文件    druidProps.load(druidConfig)    Some(DruidDataSourceFactory.createDataSource(druidProps))  }catch {    case error :Exception =&gt;      log.error(&quot;Error Create Mysql Connection &quot;,error)      None  }}  //获取连接  def getConnection:Option[Connection] = {    dataSource match {      case Some(ds) =&gt; Some(ds.getConnection)      case None =&gt; None    }  }}</code></pre><p>application.properties</p><blockquote><p>driver-class-name = com.mysql.cj.jdbc.Driver<br>url=jdbc:mysql://localhost:3306/base_data?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false<br>username=root<br>password=lyxbdw<br>filters=stat,config<br>initialSize=2<br>maxActive=20<br>minIdle=2<br>maxWait=60000<br>timeBetweenEvictionRunsMillis=60000<br>minEvictableIdleTimeMillis=300000<br>validationQuery=SELECT 1<br>testWhileIdle=true<br>testOnBorrow=false<br>testOnReturn=false<br>removeAbandoned=true<br>logAbandoned=true<br>poolPreparedStatements=false<br>removeAbandonedTimeout=1800<br>maxOpenPreparedStatements=100</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;structed streaming :&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spar
      
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>蓝牙信标定位项目总结</title>
    <link href="http://yoursite.com/2019/08/26/%E8%93%9D%E7%89%99%E4%BF%A1%E6%A0%87%E5%AE%9A%E4%BD%8D%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2019/08/26/蓝牙信标定位项目总结/</id>
    <published>2019-08-26T07:54:34.000Z</published>
    <updated>2019-08-27T02:55:50.848Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、项目简介："><a href="#一、项目简介：" class="headerlink" title="一、项目简介："></a>一、项目简介：</h2><blockquote><p>  蓝牙信标定位项目又称物流容器信息化管理系统。主要是为了解决缩短物理配送周期，提高物流配送效率问题，<br>采用蓝牙定位技术,实时定位小车的位置以及监控小车的状态信息，达到及时优化和调整配送流程的效果。<br>以下是蓝牙基站数据处理时序图：</p></blockquote><p><img alt="image.png" data-src="/images/2019/08/26/d389bdf0-c7d2-11e9-82cd-b1912fd86647.png" class="lazyload"></p><p>数据处理流程：</p><ol><li>作为服务端，需要接受来自客户端tcp/ip数据。(netty)</li><li>数据接收并校验成功后作出相应的动作，一部分数据需要返回时间戳，所有校验成功的数据都写入kafka。(kafka producer)</li><li>消费kafka中的数据，主要有三个部分，一是信标电量信息采集，二是基站状态信息的采集，三是小车位置的计算和物料信息的绑定（structured streaming/flink）</li><li>读取redis小车位置以及状态信息，通过websocket每隔五秒准实时推送一次最新数据到前端。(websocket)</li></ol><p><strong>说明</strong>：<br>1.使用netty作为数据接入使用可以参考：<a href="https://sfzsjx.github.io/2019/05/22/netty-%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/" target="_blank" rel="noopener">netty整合spring boot</a><br>2.数据接收后需要写入kafka集群中，关于kafka集群部署可以参考：<a href="https://sfzsjx.github.io/2019/04/30/kafka-%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85-%E9%83%A8%E7%BD%B2/" target="_blank" rel="noopener">kafka集群安装部署</a><br>3.kafka安装后，数据写入kafka,需要创建kafka producer，可以参考：<a href="https://sfzsjx.github.io/2019/04/30/kafka%E7%9B%B8%E5%85%B3%E7%9A%84%E5%B8%B8%E8%A7%84%E6%93%8D%E4%BD%9C/" target="_blank" rel="noopener">kafka相关操作</a><br>4.读取kafka数据，并做相应的计算，需要用到structed streaming/flink技术，相应的需要部署spark 集群或flink集群，spark集群部署可以参考：<a href="https://sfzsjx.github.io/2019/08/12/Spark-Windows10-%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/" target="_blank" rel="noopener">spark 集群安装部署</a><br>5.structed streaming 读取kafka 数据并做实时计算可以参考 ：<a href="https://sfzsjx.github.io/2019/08/27/spark-structed-streaming-%E8%AF%BB%E5%8F%96kafka%E6%95%B0%E6%8D%AE/" target="_blank" rel="noopener">structed streaming 读取kafka 数据</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、项目简介：&quot;&gt;&lt;a href=&quot;#一、项目简介：&quot; class=&quot;headerlink&quot; title=&quot;一、项目简介：&quot;&gt;&lt;/a&gt;一、项目简介：&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;  蓝牙信标定位项目又称物流容器信息化管理系统。主要是为了解决缩短物理配送
      
    
    </summary>
    
      <category term="蓝牙定位" scheme="http://yoursite.com/categories/%E8%93%9D%E7%89%99%E5%AE%9A%E4%BD%8D/"/>
    
    
      <category term="蓝牙定位" scheme="http://yoursite.com/tags/%E8%93%9D%E7%89%99%E5%AE%9A%E4%BD%8D/"/>
    
  </entry>
  
  <entry>
    <title>spark standalone  运行原理</title>
    <link href="http://yoursite.com/2019/08/26/spark-standalone-%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2019/08/26/spark-standalone-运行原理/</id>
    <published>2019-08-26T02:31:37.000Z</published>
    <updated>2019-08-26T07:03:21.372Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一-、spark-standalone-运行模式"><a href="#一-、spark-standalone-运行模式" class="headerlink" title="一 、spark standalone 运行模式"></a>一 、spark standalone 运行模式</h3><p><img alt="image.png" data-src="/images/2019/08/26/2f6b2e50-c7a7-11e9-b720-45f763a73cbf.png" class="lazyload"></p><blockquote><p>standalone 运行模式 是由客户端，主节点和多个worker节点组成。<br>Worker节点可以通过ExecutorRunner运行在当前节点上的CoarseGrainedExecutorBackend进程，每个Worker节点上存在一个或多个CoarseGrainedExecutorBackend进程，每个进程包含一个Executor对象。 该对象持有一个线程池,每个线程可以执行一个task。<br><code>`</code><br>1.启动应用程序，在SparkContext启动过程中，先初始化DAGScheduler 和 TaskSchedulerImpl两个调度器， 同时初始化SparkDeploySchedulerBackend，并在其内部启动DriverEndpoint 和 ClientEndpoint<br>2.ClientEndpoint向Master注册应用程序。Master收到注册消息后把应用放到待运行应用列表，使用自己的资源调度算法分配Worker资源给应用程序。<br>3.应用程序获得Worker时，Master会通知Worker中的WorkerEndpoint创建CoarseGrainedExecutorBackend进程，在该进程中创建执行容器Executor。<br>4.Executor创建完毕后发送消息到Master 和 DriverEndpoint。在SparkContext创建成功后， 等待Driver端发过来的任务。<br>5.SparkContext分配任务给CoarseGrainedExecutorBackend执行，在Executor上按照一定调度执行任务(这些任务就是自己写的代码)<br>6.CoarseGrainedExecutorBackend在处理任务的过程中把任务状态发送给SparkContext，SparkContext根据任务不同的结果进行处理。如果任务集处理完毕后，则继续发送其他任务集。<br>7.应用程序运行完成后，SparkContext会进行资源回收。</p></blockquote><pre><code>### 二、spark standalone两种提交模式#### 1. standalone-client 任务提交模式提交命令：</code></pre><h1 id="Run-on-a-Spark-standalone-cluster-in-client-deploy-mode"><a href="#Run-on-a-Spark-standalone-cluster-in-client-deploy-mode" class="headerlink" title="Run on a Spark standalone cluster in client deploy mode"></a>Run on a Spark standalone cluster in client deploy mode</h1><p>./bin/spark-submit \<br>  –class org.apache.spark.examples.SparkPi \<br>  –master spark://207.184.161.138:7077 \<br>  –executor-memory 20G \<br>  –total-executor-cores 100 \<br>  /path/to/examples.jar \<br>  1000</p><pre><code>例子：</code></pre><p>#!/bin/sh<br>result_time=$(date +%Y%m%d)<br>echo “小车定位程序”</p><p>nohup spark-submit –class com.gree.cn.location.LocationMap –master spark://lyxbdw-02:7077 –packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 –executor-memory 4G –total-executor-cores 12  /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar &gt; /lvm/data1/spark/log/location/${result_time}_result.log &amp;</p><pre><code>standalone client 模式任务流程![image.png](/images/2019/08/26/183089c0-c7ab-11e9-b720-45f763a73cbf.png)执行流程1. client 模式提交任务后，会在客户端启动Driver进程。2. Driver 会向Master申请启动Application启动资源。3. 资源申请成功后，Driver端会将task发送到worker端执行。4. worker端执行成功后将执行结果返回给Driver端#### 1. standalone-cluster 任务提交模式提交命令：</code></pre><h1 id="Run-on-a-Spark-standalone-cluster-in-cluster-deploy-mode-with-supervise"><a href="#Run-on-a-Spark-standalone-cluster-in-cluster-deploy-mode-with-supervise" class="headerlink" title="Run on a Spark standalone cluster in cluster deploy mode with supervise"></a>Run on a Spark standalone cluster in cluster deploy mode with supervise</h1><p>./bin/spark-submit \<br>  –class org.apache.spark.examples.SparkPi \<br>  –master spark://207.184.161.138:7077 \<br>  –deploy-mode cluster \<br>  –supervise \<br>  –executor-memory 20G \<br>  –total-executor-cores 100 \<br>  /path/to/examples.jar \<br>  1000</p><pre><code>例子： </code></pre><p>#!/bin/sh<br>result_time=$(date +%Y%m%d)<br>echo “小车定位程序”</p><p>spark-submit –class com.gree.cn.location.LocationMap –master spark://lyxbdw-02:7077 –deploy-mode cluster –supervise  –packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 –executor-memory 4G –total-executor-cores 12  /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar</p><p><code>`</code></p><blockquote><p>注意：Standalone-cluster提交方式，应用程序使用的所有jar包和文件，必须保证所有的worker节点都要有，因为此种方式，spark不会自动上传包。</p></blockquote><p>standalone-cluster模式提交任务<br><img alt="image.png" data-src="/images/2019/08/26/b76b5dc0-c7ac-11e9-b720-45f763a73cbf.png" class="lazyload"></p><p>执行流程：</p><ol><li>客户端使用命令spark-submit –deploy-mode cluster 后会启动spark-submit进程</li><li>此进程为Driver向Master 申请资源。</li><li>Master会随机在一台Worker节点来启动Driver进程。</li><li>Driver启动成功后，spark-submit关闭，然后Driver向Master申请资源。</li><li>Master接收到请求后，会在资源充足的Worker节点上启动Executor进程。</li><li>Driver分发Task到Executor中执行。</li></ol><p>建议：<strong>在平时调试时使用client模式，在正式生产环境时建议使用cluster模式</strong></p><p>参考文章：<a href="https://www.cnblogs.com/wangtcc/p/da-huaSpark-5san-tu-xiang-shuSpark-StandaloneClien.html" target="_blank" rel="noopener">1、大话Spark(5)-三图详述Spark Standalone/Client/Cluster运行模式</a><br><a href="https://blog.csdn.net/qq_39131779/article/details/83539608" target="_blank" rel="noopener">2、Spark中Standalone的两种提交模式（Standalone-client模式与Standalone-cluster模式）</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一-、spark-standalone-运行模式&quot;&gt;&lt;a href=&quot;#一-、spark-standalone-运行模式&quot; class=&quot;headerlink&quot; title=&quot;一 、spark standalone 运行模式&quot;&gt;&lt;/a&gt;一 、spark stand
      
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>redis 数据库</title>
    <link href="http://yoursite.com/2019/08/22/redis-%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>http://yoursite.com/2019/08/22/redis-数据库/</id>
    <published>2019-08-22T11:00:10.000Z</published>
    <updated>2019-09-23T07:14:47.615Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Redis 数据库</strong></p><blockquote><p>Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。<br>它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。</p></blockquote><p>Redis 安装参考<a href="https://www.runoob.com/redis/redis-install.html" target="_blank" rel="noopener">redis基础教程</a></p><h3 id="一、客户端模式："><a href="#一、客户端模式：" class="headerlink" title="一、客户端模式："></a>一、客户端模式：</h3><pre><code>cd /usr/local/redis/redis-5.0.5/src./redis-cli --rawhvals materialinfo</code></pre><h3 id="二、springboot-与-redis-整合"><a href="#二、springboot-与-redis-整合" class="headerlink" title="二、springboot 与 redis 整合"></a>二、springboot 与 redis 整合</h3><p>1.引入依赖</p><pre><code>&lt;dependency&gt;  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;  &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;  &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;  &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt;</code></pre><p>2.配置依赖文件</p><pre><code># redisspring.redis.host = 172.80.122.33spring.redis.port = 6379spring.redis.database = 0spring.redis.timeout = 100msspring.redis.lettuce.pool.max-active = 8spring.redis.lettuce.pool.max-idle = 8spring.redis.lettuce.pool.min-idle = 0spring.redis.lettuce.pool.max-wait = 100ms</code></pre><p>3.使用redisTemplate</p><pre><code>  @Autowired   private RedisTemplate&lt;String,String&gt; redisTemplate;  redisTemplate.opsForHash().entries(&quot;scada:dashboard:enamelling_line_status&quot;);</code></pre><h3 id="redis-client操作redis数据"><a href="#redis-client操作redis数据" class="headerlink" title="redis-client操作redis数据"></a>redis-client操作redis数据</h3><p>1.引入依赖</p><pre><code> &lt;!-- Redis客户端 --&gt;        &lt;dependency&gt;            &lt;groupId&gt;redis.clients&lt;/groupId&gt;            &lt;artifactId&gt;jedis&lt;/artifactId&gt;            &lt;version&gt;2.7.1&lt;/version&gt;        &lt;/dependency&gt;</code></pre><p>2.配置数据库连接池</p><pre><code>private static JedisPool jedisPool;    private Jedis jedis;    static {        JedisPoolConfig config = new JedisPoolConfig();        config.setMaxTotal(20);        config.setMaxIdle(5);        config.setMaxWaitMillis(1000);        config.setMinIdle(2);        config.setTestOnBorrow(false);        jedisPool = new JedisPool(config, &quot;127.0.0.1&quot;, 6379);    }    private static synchronized Jedis getJedis(){        return jedisPool.getResource();    }</code></pre><p>3.操作数据库</p><pre><code> jedis = getJedis(); jedis.hvals(&quot;materialinfo&quot;);</code></pre><h3 id="redis-client-命令操作"><a href="#redis-client-命令操作" class="headerlink" title="redis client 命令操作"></a>redis client 命令操作</h3><pre><code>对String操作命令</code></pre><ul><li>set(Key,value) :给数据库中名称为Key的String赋值value</li><li>get(Key):返回数据库中名称为Key的String的value</li><li>getset(key, value)：给名称为 key 的 string 赋予上一次的 value</li><li>mget(key1, key2,…, key N)：返回库中多个 string 的 value</li><li>setnx(key, value)：添加 string，名称为 key，值为 value</li><li>setex(key, time, value)：向库中添加 string，设定过期时间 time</li><li>mset(key N, value N)：批量设置多个 string 的值</li><li>msetnx(key N, value N)：如果所有名称为 key i 的 string 都不存在</li><li>incr(key)：名称为 key 的 string 增 1 操作</li><li>incrby(key, integer)：名称为 key 的 string 增加 integer</li><li>decr(key)：名称为 key 的 string 减 1 操作</li><li>decrby(key, integer)：名称为 key 的 string 减少 integer</li><li>append(key, value)：名称为 key 的 string 的值附加 value</li><li>substr(key, start, end)：返回名称为 key 的 string 的 value 的子串</li></ul><pre><code>对List操作命令</code></pre><ul><li>rpush(key, value)：在名称为 key 的 list 尾添加一个值为 value 的元素</li><li>lpush(key, value)：在名称为 key 的 list 头添加一个值为 value 的元素</li><li>llen(key)：返回名称为 key 的 list 的长度</li><li>lrange(key, start, end)：返回名称为 key 的 list 中 start 至 end 之间的元素</li><li>ltrim(key, start, end)：截取名称为 key 的 list</li><li>lindex(key, index)：返回名称为 key 的 list 中 index 位置的元素</li><li>lset(key, index, value)：给名称为 key 的 list 中 index 位置的元素赋值</li><li>lrem(key, count, value)：删除 count 个 key 的 list 中值为 value 的元素</li><li>lpop(key)：返回并删除名称为 key 的 list 中的首元素</li><li>rpop(key)：返回并删除名称为 key 的 list 中的尾元素</li><li>blpop(key1, key2,… key N, timeout)：lpop 命令的 block 版本。</li><li>brpop(key1, key2,… key N, timeout)：rpop 的 block 版本。</li><li><p>rpoplpush(srckey, dstkey)：返回并删除名称为 srckey 的 list 的尾元素，并将该元素添加到名称为 dstkey 的 list 的头部</p><p>  对Set操作命令    </p></li><li>sadd(key, member)：向名称为 key 的 set 中添加元素 member</li><li>srem(key, member) ：删除名称为 key 的 set 中的元素 member</li><li>spop(key) ：随机返回并删除名称为 key 的 set 中一个元素</li><li>smove(srckey, dstkey, member) ：移到集合元素</li><li>scard(key) ：返回名称为 key 的 set 的基数</li><li>sismember(key, member) ：member 是否是名称为 key 的 set 的元素</li><li>sinter(key1, key2,…key N) ：求交集</li><li>sinterstore(dstkey, (keys)) ：求交集并将交集保存到 dstkey 的集合</li><li>sunion(key1, (keys)) ：求并集</li><li>sunionstore(dstkey, (keys)) ：求并集并将并集保存到 dstkey 的集合</li><li>sdiff(key1, (keys)) ：求差集</li><li>sdiffstore(dstkey, (keys)) ：求差集并将差集保存到 dstkey 的集合</li><li>smembers(key) ：返回名称为 key 的 set 的所有元素</li><li><p>srandmember(key) ：随机返回名称为 key 的 set 的一个元素</p><p>  对 Hash 操作的命令</p></li><li>hset(key, field, value)：向名称为 key 的 hash 中添加元素 field</li><li>hget(key, field)：返回名称为 key 的 hash 中 field 对应的 value</li><li>hmget(key, (fields))：返回名称为 key 的 hash 中 field i 对应的 value</li><li>hmset(key, (fields))：向名称为 key 的 hash 中添加元素 field</li><li>hincrby(key, field, integer)：将名称为 key 的 hash 中 field 的 value 增加 integer</li><li>hexists(key, field)：名称为 key 的 hash 中是否存在键为 field 的域</li><li>hdel(key, field)：删除名称为 key 的 hash 中键为 field 的域</li><li>hlen(key)：返回名称为 key 的 hash 中元素个数</li><li>hkeys(key)：返回名称为 key 的 hash 中所有键</li><li>hvals(key)：返回名称为 key 的 hash 中所有键对应的 value</li><li>hgetall(key)：返回名称为 key 的 hash 中所有的键（field）及其对应的 value</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Redis 数据库&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。&lt;br&gt;它通常被称为数据结
      
    
    </summary>
    
      <category term="数据库 " scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Intellij IDEA 问题记录</title>
    <link href="http://yoursite.com/2019/08/15/Intellij-IDEA-%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2019/08/15/Intellij-IDEA-问题记录/</id>
    <published>2019-08-15T08:03:58.000Z</published>
    <updated>2019-08-15T08:03:58.470Z</updated>
    
    <content type="html"><![CDATA[<p>Intellij IDEA 问题记录 </p><p>问题一、</p><pre><code>Intellij idea Language level和Java Compiler版本自动变化问题该问题主要是由于刷新pom.xml文件时，IDEA的这两个参数就会恢复成默认值解决办法：在pom.xml中加入以下配置&lt;build&gt;    &lt;plugins&gt;        &lt;plugin&gt;            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;            &lt;version&gt;3.5.1&lt;/version&gt;            &lt;configuration&gt;                &lt;source&gt;1.8&lt;/source&gt;                &lt;target&gt;1.8&lt;/target&gt;            &lt;/configuration&gt;        &lt;/plugin&gt;    &lt;/plugins&gt;&lt;/build&gt;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Intellij IDEA 问题记录 &lt;/p&gt;
&lt;p&gt;问题一、&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Intellij idea Language level和Java Compiler版本自动变化问题
该问题主要是由于刷新pom.xml文件时，IDEA的这两个参数就会恢复成默认值
      
    
    </summary>
    
      <category term="IDEA" scheme="http://yoursite.com/categories/IDEA/"/>
    
    
      <category term="IDEA" scheme="http://yoursite.com/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>mysql 8.0以后授权</title>
    <link href="http://yoursite.com/2019/08/14/mysql-8-0%E4%BB%A5%E5%90%8E%E6%8E%88%E6%9D%83/"/>
    <id>http://yoursite.com/2019/08/14/mysql-8-0以后授权/</id>
    <published>2019-08-14T08:25:52.000Z</published>
    <updated>2019-08-14T08:49:04.520Z</updated>
    
    <content type="html"><![CDATA[<p>mysql 授权操作</p><pre><code>首先创建一个用户create user &#39;root&#39;@&#39;172.16.247.129&#39; identified by &#39;lyxbdw&#39;;然后进行授权 grant all privileges on *.* to &#39;root&#39;@&#39;172.16.247.129&#39;    -&gt; ;刷新权限flush privileges;开启管理员权限，重启mysql![image.png](/images/2019/08/14/6a9ca990-be6f-11e9-8d50-b3f800759cdf.png)</code></pre><p><img alt="image.png" data-src="/images/2019/08/14/74cdc340-be6f-11e9-8d50-b3f800759cdf.png" class="lazyload"></p><p>查看某个用户权限</p><pre><code>show grants for  &#39;root&#39;@&#39;172.16.247.129&#39;;</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;mysql 授权操作&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;首先创建一个用户
create user &amp;#39;root&amp;#39;@&amp;#39;172.16.247.129&amp;#39; identified by &amp;#39;lyxbdw&amp;#39;;

然后进行授权
 grant all
      
    
    </summary>
    
      <category term="mysql，数据库" scheme="http://yoursite.com/categories/mysql%EF%BC%8C%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="mysql" scheme="http://yoursite.com/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>Spark Windows10 安装部署</title>
    <link href="http://yoursite.com/2019/08/12/Spark-Windows10-%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    <id>http://yoursite.com/2019/08/12/Spark-Windows10-安装部署/</id>
    <published>2019-08-12T01:18:10.000Z</published>
    <updated>2019-08-24T09:12:41.028Z</updated>
    
    <content type="html"><![CDATA[<p>Spark centos 安装部署 </p><h2 id="一、Scala-安装"><a href="#一、Scala-安装" class="headerlink" title="一、Scala 安装"></a>一、Scala 安装</h2><blockquote><p>jdk安装不用多说，一般没什么问题，下载Scala安装包，<a href="https://www.scala-lang.org/download/all.html" target="_blank" rel="noopener">Scala安装包下载链接</a>，选择自己安装的版本。</p><pre><code>cd /usr/localmkdir scalatar -zxvf scala-2.11.8.tgz</code></pre><p>解压安装后配置环境变量<br><code>`</code><br>vim /etc/profile</p></blockquote><p> export JAVA_HOME=/usr/local/java/jdk1.8.0_11<br> export SCALA_HOME=/usr/local/scala/scala-2.11.8<br> export SPARK_HOME=/usr/local/spark/spark-2.4.3-bin-hadoop2.7<br> export JRE_HOME=${JAVA_HOME}/jre<br> export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:${SCALA_HOME}/lib:${SPARK_HOME}/lib<br> export PATH=${JAVA_HOME}/bin:$PATH:${SCALA_HOME}/bin:$PATH:${SPARK_HOME}/bin<br> export SPARK_SSH_OPTS=”-p 2122”</p><pre><code>&gt; 主要是配置SCALA_HOME参数，然后运行source /etc/profile,并执行scala![image.png](/images/2019/08/21/5e145a30-c3e5-11e9-b19d-dbb0f4ee7101.png)## 二、Spark安装&gt; Spark安装也没什么技术含量，直接下载spark安装包，[spark下载链接](https://archive.apache.org/dist/spark/spark-2.4.3/)，选择自己安装版本。</code></pre><p>cd /usr/local<br>mkdir spark<br>cd spark<br>tar -zxvf spark-2.4.3-bin-hadoop2.7.tgz</p><pre><code>&gt; 同样解压后配置环境变量，主要是SPARK_HOME配置，再次执行source /etc/profile,并执行spark-shell.![image.png](/images/2019/08/21/c1fd7b20-c3e6-11e9-b19d-dbb0f4ee7101.png)## 三、spark standalone模式部署&gt;进入到配置目录</code></pre><p>cd /usr/local/spark/spark-2.4.3-bin-hadoop2.7/conf<br>mv slaves.template slaves<br>mv spark-env.sh.template spark-env.sh<br>vim spark-env.sh<br>添加以下配置</p><h1 id="spark-env-sh"><a href="#spark-env-sh" class="headerlink" title="spark-env.sh"></a>spark-env.sh</h1><p>export SCALA_HOME=/usr/local/scala/scala-2.11.8<br>export JAVA_HOME=/usr/local/java/jdk1.8.0_11</p><h1 id="本地安装绑定"><a href="#本地安装绑定" class="headerlink" title="本地安装绑定"></a>本地安装绑定</h1><p>export SPARK_MASTER_IP=127.0.0.1</p><p>#export SPARK_LOCAL_IP=127.0.0.1<br>export SPARK_MASTER_PORT=7077<br>export SPARK_WORKER_MEMORY=8G<br>export SPARK_EXECUTOR_CORES=4<br>export SPARK_LOG_DIR=/lvm/data1/spark/log</p><p>#export master=spark://10.7.20.191:7077</p><pre><code>&gt; 然后进入sbin目录,启动集群</code></pre><p>cd /usr/local/spark/spark-2.4.3-bin-hadoop2.7/sbin<br>./start-all.sh</p><pre><code>&gt;注意若是服务器之间的免密登录端口不是22端口，则需要在 /etc/profile文件添加配置，改到对应端口</code></pre><p>export SPARK_SSH_OPTS=”-p 2122”</p><pre><code>至此spark安装完成。## Spark命令提交程序</code></pre><p>nohup spark-submit –class com.gree.cn.location.LocationMap –master spark://lyxbdw-02:7077 –packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar &gt; /lvm/data1/spark/log/location.log &amp;</p><pre><code>执行脚本</code></pre><p>#!/bin/sh<br>result_time=$(date +%Y%m%d)<br>echo “小车定位程序”<br>nohup spark-submit –class com.gree.cn.location.LocationMap –master spark://lyxbdw-02:7077 –packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar &gt; /lvm/data1/spark/log/location/${result_time}_result.log &amp;<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Spark centos 安装部署 &lt;/p&gt;
&lt;h2 id=&quot;一、Scala-安装&quot;&gt;&lt;a href=&quot;#一、Scala-安装&quot; class=&quot;headerlink&quot; title=&quot;一、Scala 安装&quot;&gt;&lt;/a&gt;一、Scala 安装&lt;/h2&gt;&lt;blockquote&gt;
&lt;p
      
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>spark 常用参数设置</title>
    <link href="http://yoursite.com/2019/08/09/spark-%E5%B8%B8%E7%94%A8%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/"/>
    <id>http://yoursite.com/2019/08/09/spark-常用参数设置/</id>
    <published>2019-08-09T08:50:32.000Z</published>
    <updated>2019-10-07T00:56:12.011Z</updated>
    
    <content type="html"><![CDATA[<pre><code>// task数量设置spark.sql.shuffle.partitions 50//并行度设置spark.default.parallelism 10</code></pre><h4 id="spark提交部署程序脚本"><a href="#spark提交部署程序脚本" class="headerlink" title="spark提交部署程序脚本"></a>spark提交部署程序脚本</h4><pre><code>#!/bin/shresult_time=$(date +%Y%m%d)echo &quot;小车定位程序&quot;spark-submit --class com.gree.cn.location.LocationMap --master spark://lyxbdw-02:7077 --deploy-mode cluster --supervise  --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 --executor-memory 4G --total-executor-cores 12  /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;// task数量设置
spark.sql.shuffle.partitions 50
//并行度设置
spark.default.parallelism 10

&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&quot;spark提交部署程序脚本&quot;&gt;&lt;a href=&quot;#s
      
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spark sql  java 自定义函数</title>
    <link href="http://yoursite.com/2019/07/18/spark-sql-java-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/07/18/spark-sql-java-自定义函数/</id>
    <published>2019-07-18T08:22:27.000Z</published>
    <updated>2019-07-18T08:22:27.928Z</updated>
    
    <content type="html"><![CDATA[<pre><code>    ::: hljs-center</code></pre><p>java自定义函数</p><p>:::</p><p>//自定义函数将信号强度转距离<br>        //其中udf+数字中数字代表入参个数，最后一个参数代表出参的数据类型<br>        //出现序列化问题</p><pre><code>  ```  </code></pre><p>spark.udf().register(“transsinglepower”, new UDF1&lt;Integer,Double&gt;() {<br>            @Override<br>            public Double call(Integer single_poer) throws Exception {<br>                Double mesure_distinct = Math.pow(10, (Math.abs(single_poer) - 59) / (10 * 2.0));<br>                return mesure_distinct;<br>            }}, DataTypes.DoubleType);</p><p><code>`</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;    ::: hljs-center
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;java自定义函数&lt;/p&gt;
&lt;p&gt;:::&lt;/p&gt;
&lt;p&gt;//自定义函数将信号强度转距离&lt;br&gt;        //其中udf+数字中数字代表入参个数，最后一个参数代表出参的数据类型&lt;br
      
    
    </summary>
    
      <category term="spark" scheme="http://yoursite.com/categories/spark/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>spring boot 注解</title>
    <link href="http://yoursite.com/2019/07/16/spring-boot-%E6%B3%A8%E8%A7%A3/"/>
    <id>http://yoursite.com/2019/07/16/spring-boot-注解/</id>
    <published>2019-07-16T09:05:47.000Z</published>
    <updated>2019-07-16T09:09:46.592Z</updated>
    
    <content type="html"><![CDATA[<p>问题一、</p><p>Could not autowire. No beans of ‘RestTemplate’ type found</p><p>在mapper 类添加注解@Repository 即可解决，简单有效</p><p>方法2：在mapper文件上加@Component注解，把普通pojo实例化到spring容器中</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;问题一、&lt;/p&gt;
&lt;p&gt;Could not autowire. No beans of ‘RestTemplate’ type found&lt;/p&gt;
&lt;p&gt;在mapper 类添加注解@Repository 即可解决，简单有效&lt;/p&gt;
&lt;p&gt;方法2：在mapper文件上加@Co
      
    
    </summary>
    
      <category term="spring boot" scheme="http://yoursite.com/categories/spring-boot/"/>
    
    
      <category term="springboot" scheme="http://yoursite.com/tags/springboot/"/>
    
  </entry>
  
</feed>
