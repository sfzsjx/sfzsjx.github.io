<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>上善若水博客</title>
  
  <subtitle>大数据</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-02-23T06:45:58.063Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>上善若水</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>17-数据分析实战笔记： 决策树</title>
    <link href="http://yoursite.com/2019/02/23/17-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%EF%BC%9A-%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>http://yoursite.com/2019/02/23/17-数据分析实战笔记：-决策树/</id>
    <published>2019-02-23T06:16:23.000Z</published>
    <updated>2019-02-23T06:45:58.063Z</updated>
    
    <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h2 id="决策树的工作原理"><a href="#决策树的工作原理" class="headerlink" title="决策树的工作原理"></a>决策树的工作<strong>原理</strong></h2><blockquote><p>  在现实生活，我们做的各种决策，都是基于以往的经验来判断的。如果将背后的逻辑整理成一个结构图，这实际上就是决策树。<br><img src="/images/2019/02/23/4dbc8110-3733-11e9-928d-e15e87e90364.png" alt="image.png"><br>上图就是一个典型的决策树。而在实现决策树时会经历两个阶段：<strong>构造和剪枝。</strong></p></blockquote><h3 id="构造"><a href="#构造" class="headerlink" title="构造"></a>构造</h3><blockquote><p>构造的过程就是选择什么样的属性作为节点的过程，一般在构造中存在三种类型的节点：<br>1.根节点：树的顶端，最开始的那个节点<br>2.内部节点：树的中间节点<br>3.叶节点：树最底部的节点，也是决策树结果</p></blockquote><p>构造过程中，需要解决的三个重要问题：<br>1.选择哪个属性作为根节点；<br>2.选择哪些属性作为子节点；<br>3.什么时候停止并得到目标状态，即叶子节点。</p><h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><blockquote><p>剪枝就是给决策树瘦身。也即不需要过多的判断也能得到不错的结果。主要是为了防止“过拟合（Overfitting）”现象的发生。<br>过拟合现象会导致得到的模型虽然训练结果好，但是泛化能力差。<br>剪枝一般分为两种：“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）</p></blockquote><p><img src="/images/2019/02/23/98412a80-3736-11e9-928d-e15e87e90364.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;决策树&quot;&gt;&lt;a href=&quot;#决策树&quot; class=&quot;headerlink&quot; title=&quot;决策树&quot;&gt;&lt;/a&gt;决策树&lt;/h1&gt;&lt;h2 id=&quot;决策树的工作原理&quot;&gt;&lt;a href=&quot;#决策树的工作原理&quot; class=&quot;headerlink&quot; title=&quot;决策树的工
      
    
    </summary>
    
      <category term="决策树" scheme="http://yoursite.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="决策树" scheme="http://yoursite.com/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>02 数据分析实战笔记--学习数据挖掘的最佳路径</title>
    <link href="http://yoursite.com/2019/02/18/02-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0-%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%9A%84%E6%9C%80%E4%BD%B3%E8%B7%AF%E5%BE%84/"/>
    <id>http://yoursite.com/2019/02/18/02-数据分析实战笔记-学习数据挖掘的最佳路径/</id>
    <published>2019-02-18T14:26:38.000Z</published>
    <updated>2019-02-22T05:59:14.544Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2019/02/22/dc70e0e0-3666-11e9-8304-57f818b3fae9.png" alt="image.png"><br><strong>数据挖掘的基本流程（六大步骤）</strong>：<br>1、商业理解：先从商业的角度理解项目的需求，然后再对数据挖掘的目标进行定义。<br>2、数据理解：尝试收集部分数据，然后对数据进行探索，包括数据描述、数据质量验证<br>3、数据准备：收集数据，并对数据进行清洗、数据集成等操作<br>4、模型建立：选择和应用各种数据挖掘模型，并进行优化<br>5、模型评估：对模型进行评价，并检查构建模型的每个步骤，确认模型是否实现预定的商业目标<br>6、上线发布：项目只有落地实施才能体现价值。当然后序还需要一定的日常运维。</p><p><strong>数据挖掘的十大算法</strong><br>按照不同的目的，算法主要分为四类：<br>|分类算法：C4.5,朴素贝叶斯（Naive Bayes）,SVM,KNN,Adaboost,CART<br>|聚类算法：K-Means,EM<br>|关联分析：Apriori<br>|连接分析：PageRank</p><p><strong>数据挖掘的数学原理</strong></p><p>1、概率论与数理统计<br>2、线性代数<br>3、图论<br>4、最优化方法</p><p><img src="/images/2019/02/22/ec89f8e0-3666-11e9-8304-57f818b3fae9.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/2019/02/22/dc70e0e0-3666-11e9-8304-57f818b3fae9.png&quot; alt=&quot;image.png&quot;&gt;&lt;br&gt;&lt;strong&gt;数据挖掘的基本流程（六大步骤）&lt;/strong&gt;：&lt;br&gt;1、商业理解：先从
      
    
    </summary>
    
      <category term="数据分析" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
      <category term="数据挖掘" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="数据分析" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>修炼指南</title>
    <link href="http://yoursite.com/2019/02/18/%E4%BF%AE%E7%82%BC%E6%8C%87%E5%8D%97/"/>
    <id>http://yoursite.com/2019/02/18/修炼指南/</id>
    <published>2019-02-18T14:04:46.000Z</published>
    <updated>2019-02-22T05:57:28.340Z</updated>
    
    <content type="html"><![CDATA[<p><strong>MAS学习法</strong></p><p>认知：我们只有把知识转化为自己的语言，它才真正编程我们自己的东西。这个转换的过程就叫做认知的过程。<br><img src="/images/2019/02/22/ab30d8a0-3666-11e9-8304-57f818b3fae9.png" alt="image.png"><br>如何提高我们的学习吸收能力，就要做到知行合一。如果说认知是大脑，那么工具就是我们的双手。所以我们需要把握两点原则:</p><p>1、不要重复造轮子<br>   也即尽量选择已有的第三方工具来完成我们的项目，因为大部分的业务场景都能找到相关的工具来解决，这样既省时又省力。<br>2、工具决定效率<br>   在工作中选择工具的原则是选择使用者最多的工具。因为：Bug少、文档全、案例多。同时找到适合的工具，这样就能大大提高我们的工作效率。</p><p>选择好工具后就需要大量的积累。一般而言，我们很难记住大段的知识和相关指令。但是我们能够记住自己曾经做过的相关的项目，题目和故事。这就需要多练，练熟练透它。正所谓熟能生巧。量变引起质变。</p><p><strong>总结一下几点</strong></p><p>1、记录下自己每天的认知<br>2、这些认知对应工具的那些操作<br>3、勤加练习，巩固知识。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;MAS学习法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;认知：我们只有把知识转化为自己的语言，它才真正编程我们自己的东西。这个转换的过程就叫做认知的过程。&lt;br&gt;&lt;img src=&quot;/images/2019/02/22/ab30d8a0-3666-11e9-8304-
      
    
    </summary>
    
      <category term="数据分析" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="数据分析" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>数据分析全景图笔记</title>
    <link href="http://yoursite.com/2019/02/18/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%85%A8%E6%99%AF%E5%9B%BE%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/02/18/数据分析全景图笔记/</id>
    <published>2019-02-18T06:29:35.000Z</published>
    <updated>2019-02-19T00:48:28.981Z</updated>
    
    <content type="html"><![CDATA[<p>数据分析的<strong>三个</strong>重要组成部分：</p><pre><code>1、数据采集。（源头）2、数据挖掘。（核心，主要是挖掘数据的商业价值）3、数据可视化。（直观感受数据分析结果）</code></pre><p><img src="/images/2019/02/18/45a3d120-3343-11e9-b0b3-2d63e1557176.png" alt="image.png"></p><p><strong>数据采集</strong></p><pre><code>主要是各种各样的数据源打交道，然后使用不用的工具来进行采集。</code></pre><p><img src="/images/2019/02/18/0f33da80-3344-11e9-b0b3-2d63e1557176.png" alt="image.png"></p><p><strong>数据挖掘</strong></p><pre><code>第二部分主要熟悉数据挖掘的基本流程、十大算法、以及背后的数学基础。</code></pre><p><img src="/images/2019/02/18/d1c7dd80-3344-11e9-b0b3-2d63e1557176.png" alt="image.png"></p><p><strong>数据可视化</strong></p><pre><code>数据可视化可以帮我们很好的理解数据的结构，以及分析结果的呈现。主要有两种方法实现数据的可视化。第一种就是使用Python。在Python对数据进行清洗、挖掘的过程中，可以使用Matplotlib,Seaborn等第三方库进行呈现。第二种就是使用第三方工具。若已经生成了csv文件，想采用所见即所得的方式呈现，可以采用微图、DataV、Data GIF Maker等第三方工具。</code></pre><p><img src="/images/2019/02/18/b23122d0-3347-11e9-b0b3-2d63e1557176.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;数据分析的&lt;strong&gt;三个&lt;/strong&gt;重要组成部分：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1、数据采集。（源头）
2、数据挖掘。（核心，主要是挖掘数据的商业价值）
3、数据可视化。（直观感受数据分析结果）
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&quot;/image
      
    
    </summary>
    
      <category term="数据分析" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="数据分析" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>jupyter 安装</title>
    <link href="http://yoursite.com/2019/01/21/jupyter-%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2019/01/21/jupyter-安装/</id>
    <published>2019-01-21T01:11:39.000Z</published>
    <updated>2019-02-22T02:54:25.057Z</updated>
    
    <content type="html"><![CDATA[<p>选择管理员运行cmd,执行以下命令：</p><pre><code>pip install jupyter   </code></pre><p>进入目录 C:\Program Files\Python37\Scripts&gt;，运行命令：</p><pre><code>cd C:\Program Files\Python37\Scripts             jupyter notebook</code></pre><p>注意管理员权限问题。</p><pre><code>http://localhost:8888/tree</code></pre><p>第三方库numpy安装 </p><pre><code>pip install numpypip install -U scikit-learnpip install -U pandasqlpip install graphvizpip install pydotplus</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;选择管理员运行cmd,执行以下命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install jupyter   
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;进入目录 C:\Program Files\Python37\Scripts&amp;gt;，运行命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记</title>
    <link href="http://yoursite.com/2019/01/18/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/01/18/学习笔记/</id>
    <published>2019-01-18T08:56:37.000Z</published>
    <updated>2019-01-18T08:56:37.581Z</updated>
    
    <content type="html"><![CDATA[<p>chcp    查看windows系统编码格式</p><p>chcp 65001   将Windows格式编码设为utf-8模式</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;chcp    查看windows系统编码格式&lt;/p&gt;
&lt;p&gt;chcp 65001   将Windows格式编码设为utf-8模式&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>kylin jdbc 连接</title>
    <link href="http://yoursite.com/2019/01/18/kylin-jdbc-%E8%BF%9E%E6%8E%A5/"/>
    <id>http://yoursite.com/2019/01/18/kylin-jdbc-连接/</id>
    <published>2019-01-18T01:13:24.000Z</published>
    <updated>2019-02-19T02:53:03.689Z</updated>
    
    <content type="html"><![CDATA[<p>kylin jdbc 连接</p><p>一 、kylin 简介</p><pre><code>Kylin是ebay开发的一套OLAP系统，与Mondrian不同的是，它是一个MOLAP系统，主要用于支持大数据生态圈的数据分析业务，它主要是通过预计算的方式将用户设定的多维立方体缓存到HBase中。</code></pre><p>二、引入pom文件</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.apache.kylin&lt;/groupId&gt;    &lt;artifactId&gt;kylin-jdbc&lt;/artifactId&gt;    &lt;version&gt;1.6.0&lt;/version&gt;&lt;/dependency&gt;</code></pre><p>三 、例子</p><pre><code>import org.apache.kylin.jdbc.Driver;import org.datanucleus.state.LifeCycleState;import java.sql.Connection;import java.sql.ResultSet;import java.sql.Statement;import java.util.ArrayList;import java.util.List;import java.util.Properties;public class KylinJdbc {    public static void main(String[] args) throws Exception {        Driver driver = (Driver) Class.forName(&quot;org.apache.kylin.jdbc.Driver&quot;).newInstance();        Properties info = new Properties();info.put(&quot;user&quot;, &quot;BI&quot;);info.put(&quot;password&quot;, &quot;16de#+ui9&quot;);        Connection conn = driver.connect(&quot;jdbc:kylin://10.214.234.111:7070/Test_kylin&quot;, info);        Statement state = conn.createStatement();        String sqlStr = &quot;&quot;;        ResultSet resultSet = state.executeQuery(sqlStr);        List list = new ArrayList();        while (resultSet.next()){            list.add(resultSet.getString(&quot;1&quot;));        }        list.forEach(citg -&gt; System.out.println(citg));    }}</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;kylin jdbc 连接&lt;/p&gt;
&lt;p&gt;一 、kylin 简介&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Kylin是ebay开发的一套OLAP系统，与Mondrian不同的是，它是一个MOLAP系统，主要用于支持大数据生态圈的数据分析业务，它主要是通过预计算的方式将用户设定的多维立
      
    
    </summary>
    
      <category term="kylin" scheme="http://yoursite.com/categories/kylin/"/>
    
    
      <category term="kylin" scheme="http://yoursite.com/tags/kylin/"/>
    
  </entry>
  
  <entry>
    <title>CDH 5.15 简易版离线安装完整版</title>
    <link href="http://yoursite.com/2019/01/14/CDH-5-15-%E7%AE%80%E6%98%93%E7%89%88%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E5%AE%8C%E6%95%B4%E7%89%88/"/>
    <id>http://yoursite.com/2019/01/14/CDH-5-15-简易版离线安装完整版/</id>
    <published>2019-01-14T06:34:23.000Z</published>
    <updated>2019-01-14T06:34:23.838Z</updated>
    
    <content type="html"><![CDATA[<pre><code>CDH 简易版离线安装</code></pre><p>一、虚拟机搭建</p><p>准备一台32G内存的电脑，安装虚拟机VMware-workstation。虚拟机下载地址：<a href="http://download3.vmware.com/software/wkst/file/VMware-Workstation-Full-14.1.2-8497320.x86_64.bundle。根据自己的电脑系统下载不同的版本，我下载的是VMware-Workstation-Full-14.1.2-8497320.x86_64.bundle。安装完虚拟机后，下载操作系统镜像CentOS-7-x86_64-DVD-1804.iso（这是我选择的版本，你们可以选择不同的版本），创建一个新的虚拟机，至于虚拟机如何创建请自行解决。" target="_blank" rel="noopener">http://download3.vmware.com/software/wkst/file/VMware-Workstation-Full-14.1.2-8497320.x86_64.bundle。根据自己的电脑系统下载不同的版本，我下载的是VMware-Workstation-Full-14.1.2-8497320.x86_64.bundle。安装完虚拟机后，下载操作系统镜像CentOS-7-x86_64-DVD-1804.iso（这是我选择的版本，你们可以选择不同的版本），创建一个新的虚拟机，至于虚拟机如何创建请自行解决。</a></p><p>经过上面的一系列的操作，目前拥有三台虚拟机</p><p>master  内存 16G 磁盘 150G  </p><p>slave1   内存   6G 磁盘 150G</p><p>slave2   内存    6G 磁盘 150G</p><p>二、虚拟机配置</p><p>1.修改所有的主机名，这样便于管理。</p><p>hostnamectl set-hostname master<br>hostnamectl set-hostname slave1<br>hostnamectl set-hostname slave2<br>2.配置静态IP</p><p>首先，选择NAT网络连接模式</p><p> 然后，点击Edit编辑虚拟机网络设置，进入VMware network edit ,选中vmnet8 ,将Use local DHCP service to distribute IP addresses to VMs 前面的勾去掉。</p><p>接着，进入 /etc/sysconfig/network-scripts中查看现有的配置文件然后修改其中的配置文件，其中有个类似ifcfg-enth0的文件是你的网络名字</p><p>TYPE=Ethernet<br>PROXY_METHOD=none<br>BROWSER_ONLY=no<br>BOOTPROTO=static<br>DEFROUTE=yes<br>IPV4_FAILURE_FATAL=no<br>IPV6INIT=yes<br>IPV6_AUTOCONF=yes<br>IPV6_DEFROUTE=yes<br>IPV6_FAILURE_FATAL=no<br>IPV6_ADDR_GEN_MODE=stable-privacy<br>NAME=$’\751\605\615\747\675\656 1’<br>UUID=2bfdf6df-9fd6-44e3-ade7-5a397cf8d2e4<br>ONBOOT=yes<br>IPADDR=172.16.247.135<br>GATEWAY=172.16.247.2<br>NETMASK=255.255.255.0<br>PREFIX=24</p><p>上面主要修改红色字体部分，其中BOOTPROTO=static 表示静态，IPADDR=172.16.247.135 表示静态IP地址</p><p>最后，保存退出，执行</p><p>重启网络<br>service network restart<br>查看IP<br>ifconfig<br>ping网络<br>ping <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a><br>3.编辑hosts 文件 添加ip地址</p><p>vi /etc/hosts<br> 添加以下配置，你对应的三台机器的IP地址和对应的主机名</p><p>172.16.247.135 master<br>172.16.247.132 slave1<br>172.16.247.136 slave2</p><p>然后将这个文件分别拷贝到各个节点上</p><p>scp /etc/hosts root@slave1:/etc/hosts<br>scp /etc/hosts root@slave2:/etc/hosts</p><p>4.配置SSH免密登陆</p><p>主要分为两个步骤：首先在所有的节点生成公钥</p><p>ssh-keygen -t rsa<br>然后将所有的节点执行拷贝公钥</p><p>ssh-copy-id root@master<br>ssh-copy-id root@slave1<br>ssh-copy-id root@slave2<br>当然也可以公钥添加到认证文件中，并设置authorized_keys的访问权限：<a href="https://blog.csdn.net/johnzhc/article/details/81119030" target="_blank" rel="noopener">https://blog.csdn.net/johnzhc/article/details/81119030</a></p><p>5.关闭selinux和防火墙</p><p>vi /etc/selinux/config</p><p>SELINUX=disabled</p><p>[hadoop@master network-scripts]$ cat /etc/selinux/config</p><h1 id="This-file-controls-the-state-of-SELinux-on-the-system"><a href="#This-file-controls-the-state-of-SELinux-on-the-system" class="headerlink" title="This file controls the state of SELinux on the system."></a>This file controls the state of SELinux on the system.</h1><h1 id="SELINUX-can-take-one-of-these-three-values"><a href="#SELINUX-can-take-one-of-these-three-values" class="headerlink" title="SELINUX= can take one of these three values:"></a>SELINUX= can take one of these three values:</h1><h1 id="enforcing-SELinux-security-policy-is-enforced"><a href="#enforcing-SELinux-security-policy-is-enforced" class="headerlink" title="enforcing - SELinux security policy is enforced."></a>enforcing - SELinux security policy is enforced.</h1><h1 id="permissive-SELinux-prints-warnings-instead-of-enforcing"><a href="#permissive-SELinux-prints-warnings-instead-of-enforcing" class="headerlink" title="permissive - SELinux prints warnings instead of enforcing."></a>permissive - SELinux prints warnings instead of enforcing.</h1><h1 id="disabled-No-SELinux-policy-is-loaded"><a href="#disabled-No-SELinux-policy-is-loaded" class="headerlink" title="disabled - No SELinux policy is loaded."></a>disabled - No SELinux policy is loaded.</h1><p>SELINUX=disabled</p><h1 id="SELINUXTYPE-can-take-one-of-three-two-values"><a href="#SELINUXTYPE-can-take-one-of-three-two-values" class="headerlink" title="SELINUXTYPE= can take one of three two values:"></a>SELINUXTYPE= can take one of three two values:</h1><h1 id="targeted-Targeted-processes-are-protected"><a href="#targeted-Targeted-processes-are-protected" class="headerlink" title="targeted - Targeted processes are protected,"></a>targeted - Targeted processes are protected,</h1><h1 id="minimum-Modification-of-targeted-policy-Only-selected-processes-are-protected"><a href="#minimum-Modification-of-targeted-policy-Only-selected-processes-are-protected" class="headerlink" title="minimum - Modification of targeted policy. Only selected processes are protected."></a>minimum - Modification of targeted policy. Only selected processes are protected.</h1><h1 id="mls-Multi-Level-Security-protection"><a href="#mls-Multi-Level-Security-protection" class="headerlink" title="mls - Multi Level Security protection."></a>mls - Multi Level Security protection.</h1><p>SELINUXTYPE=targeted</p><p> 关闭防火墙和查看防火墙状态：</p><p>systemctl stop firewalld<br>systemctl disable firewalld<br>systemctl status firewalld<br>6.安装NTP时间同步</p><p>yum install  -y ntp     #安装ntp服务（所有节点）</p><p>vi /etc/ntp.conf          #编辑ntp服务的配置文件（所有节点）</p><p>主节点master的ntp.conf修改红色部分，蓝色要注释掉</p><h1 id="Note-Monitoring-will-not-be-disabled-with-the-limited-restriction-flag"><a href="#Note-Monitoring-will-not-be-disabled-with-the-limited-restriction-flag" class="headerlink" title="Note: Monitoring will not be disabled with the limited restriction flag."></a>Note: Monitoring will not be disabled with the limited restriction flag.</h1><p>#disable monitor<br>restrict default nomodify<br>restrict default nomodify notrap<br>server 127.127.1.0<br>fudge 127.127.1.0 stratum 10<br>includefile /etc/ntp/crypto/pw<br>keys /etc/ntp/keys</p><h1 id="中国这边最活跃的时间服务器-http-www-pool-ntp-org-zone-cn"><a href="#中国这边最活跃的时间服务器-http-www-pool-ntp-org-zone-cn" class="headerlink" title="中国这边最活跃的时间服务器 : http://www.pool.ntp.org/zone/cn"></a>中国这边最活跃的时间服务器 : <a href="http://www.pool.ntp.org/zone/cn" target="_blank" rel="noopener">http://www.pool.ntp.org/zone/cn</a></h1><p>server 0.cn.pool.ntp.org<br>server 0.asia.pool.ntp.org<br>server 3.asia.pool.ntp.org</p><h1 id="allow-update-time-by-the-upper-server"><a href="#allow-update-time-by-the-upper-server" class="headerlink" title="allow update time by the upper server"></a>allow update time by the upper server</h1><h1 id="允许上层时间服务器主动修改本机时间"><a href="#允许上层时间服务器主动修改本机时间" class="headerlink" title="允许上层时间服务器主动修改本机时间"></a>允许上层时间服务器主动修改本机时间</h1><p>restrict 0.cn.pool.ntp.org nomodify notrap noquery<br>restrict 0.asia.pool.ntp.org nomodify notrap noquery<br>restrict 3.asia.pool.ntp.org nomodify notrap noquery</p><h1 id="Undisciplined-Local-Clock-This-is-a-fake-driver-intended-for-backup"><a href="#Undisciplined-Local-Clock-This-is-a-fake-driver-intended-for-backup" class="headerlink" title="Undisciplined Local Clock. This is a fake driver intended for backup"></a>Undisciplined Local Clock. This is a fake driver intended for backup</h1><h1 id="and-when-no-outside-source-of-synchronized-time-is-available"><a href="#and-when-no-outside-source-of-synchronized-time-is-available" class="headerlink" title="and when no outside source of synchronized time is available."></a>and when no outside source of synchronized time is available.</h1><h1 id="外部时间服务器不可用时，以本地时间作为时间服务"><a href="#外部时间服务器不可用时，以本地时间作为时间服务" class="headerlink" title="外部时间服务器不可用时，以本地时间作为时间服务"></a>外部时间服务器不可用时，以本地时间作为时间服务</h1><p> 从节点slave的ntp.conf修改红色部分，紫色要注释掉</p><h1 id="with-symmetric-key-cryptography"><a href="#with-symmetric-key-cryptography" class="headerlink" title="with symmetric key cryptography."></a>with symmetric key cryptography.</h1><p>keys /etc/ntp/keys</p><h1 id="Specify-the-key-identifiers-which-are-trusted"><a href="#Specify-the-key-identifiers-which-are-trusted" class="headerlink" title="Specify the key identifiers which are trusted."></a>Specify the key identifiers which are trusted.</h1><p>#trustedkey 4 8 42</p><h1 id="Specify-the-key-identifier-to-use-with-the-ntpdc-utility"><a href="#Specify-the-key-identifier-to-use-with-the-ntpdc-utility" class="headerlink" title="Specify the key identifier to use with the ntpdc utility."></a>Specify the key identifier to use with the ntpdc utility.</h1><p>#requestkey 8</p><h1 id="Specify-the-key-identifier-to-use-with-the-ntpq-utility"><a href="#Specify-the-key-identifier-to-use-with-the-ntpq-utility" class="headerlink" title="Specify the key identifier to use with the ntpq utility."></a>Specify the key identifier to use with the ntpq utility.</h1><p>#controlkey 8</p><h1 id="Enable-writing-of-statistics-records"><a href="#Enable-writing-of-statistics-records" class="headerlink" title="Enable writing of statistics records."></a>Enable writing of statistics records.</h1><p>#statistics clockstats cryptostats loopstats peerstats</p><h1 id="Disable-the-monitoring-facility-to-prevent-amplification-attacks-using-ntpdc"><a href="#Disable-the-monitoring-facility-to-prevent-amplification-attacks-using-ntpdc" class="headerlink" title="Disable the monitoring facility to prevent amplification attacks using ntpdc"></a>Disable the monitoring facility to prevent amplification attacks using ntpdc</h1><h1 id="monlist-command-when-default-restrict-does-not-include-the-noquery-flag-See"><a href="#monlist-command-when-default-restrict-does-not-include-the-noquery-flag-See" class="headerlink" title="monlist command when default restrict does not include the noquery flag. See"></a>monlist command when default restrict does not include the noquery flag. See</h1><h1 id="CVE-2013-5211-for-more-details"><a href="#CVE-2013-5211-for-more-details" class="headerlink" title="CVE-2013-5211 for more details."></a>CVE-2013-5211 for more details.</h1><h1 id="Note-Monitoring-will-not-be-disabled-with-the-limited-restriction-flag-1"><a href="#Note-Monitoring-will-not-be-disabled-with-the-limited-restriction-flag-1" class="headerlink" title="Note: Monitoring will not be disabled with the limited restriction flag."></a>Note: Monitoring will not be disabled with the limited restriction flag.</h1><p>#disable monitor<br>server master prefer     #master 是指你的主机名<br>restrict default nomodify notrap nopeer noquery<br>restrict -6 default kod nomodify notrap nopeer noquery</p><p>设置开机启动ntp服务</p><p>#关闭chronyd服务<br>systemctl disable chronyd.service</p><p>#开机自启动<br> systemctl enable ntpd.service</p><p>以上都配置完成后，执行</p><p>systemctl start ntpd    #开启ntp服务</p><p>ntpstat                         #查看ntp运行状态</p><p>synchronised to NTP server (172.16.247.135) at stratum 3<br>   time correct to within 350 ms<br>   polling server every 1024 s         #出现这个表示成功同步</p><p>7.卸载Centos 系统自带的JDK</p><p>rpm -qa | grep jdk  #查看系统自带的jdk</p><p>yum -y remove   xxjdk   #删除所有的jdk</p><p>8.CM 和CDH下载以及JDK和java驱动</p><p>Cloudera Manager下载地址：<a href="http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.15.0_x86_64.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.15.0_x86_64.tar.gz</a></p><p>CDH安装包地址：<a href="http://archive.cloudera.com/cdh5/parcels/latest/，由于我们的操作系统为CentOS7.2，需要下载以下文件：" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/parcels/latest/，由于我们的操作系统为CentOS7.2，需要下载以下文件：</a></p><p>CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel</p><p>CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha1</p><p>manifest.json</p><p>JDK 可以去官网下载 下载地址：<a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p><p>  下载版本为： jdk-8u172-linux-x64.rpm</p><p>mysql的java驱动 下载地址：<a href="https://dev.mysql.com/downloads/connector/j/5.1.html" target="_blank" rel="noopener">https://dev.mysql.com/downloads/connector/j/5.1.html</a> </p><pre><code>下载版本为：mysql-connector-java-5.1.46.tar.gz</code></pre><p>9.安装CM和jdk以及mysql驱动</p><p>将下载好的安装包分发到各个节点上，并解压缩。</p><p>1.将和cloudera-manager-daemons-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm以及cloudera-manager-server-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm三个安装包传入管理节点（master节点）/tmp 目录下，当然其他目录也可以。但是tmp目录可以使得解压后的rpm包重启后删除不占内存。</p><p>2.将cloudera-manager-agent-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm和cloudera-manager-daemons-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm两个安装包传入所有从节点上（slave1和slave2节点）的/tmp目录下</p><p>3.将jdk-8u172-linux-x64.rpm安装包传入所有节点上/tmp目录,复制语句类似下面：</p><p>scp cloudera-manager-agent-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm root@master:/tmp<br>4.然后解压所有对应的安装包（所有节点）</p><p>yum localinstall *.rpm<br>5.配置JAVA_HOME变量（所有节点）</p><p>echo “JAVA_HOME=/usr/java/latest/“ &gt;&gt; /etc/environment<br>6.安装mysql驱动程序</p><p>将mysql-connector-java-5.1.46.tar.gz解压mysql-connector-java-5.1.46后将解压后包中的mysql-connector-java-5.1.46-bin.jar重命名为mysql-connector-java.jar传入 /usr/share/java目录里面。</p><p>tar mysql-connector-java-5.1.46.tar.gz<br>sudo mkdir -p /usr/share/java<br>cd mysql-connector-java-5.1.46<br>sudo cp mysql-connector-java-5.1.46-bin.jar /usr/share/java/mysql-connector-java.jar</p><p>10.数据库安装</p><p>1.在master节点安装MariaDB(Mysql)这里安装MariDB,若要安装mysql，可参考<a href="https://blog.csdn.net/johnzhc/article/details/81119030。" target="_blank" rel="noopener">https://blog.csdn.net/johnzhc/article/details/81119030。</a></p><p>sudo yum install mariadb-server  #安装maridb<br>sudo systemctl enable mariadb    #设置开机启动<br>sudo systemctl start mariadb     #启动mariadb<br>sudo /usr/bin/mysql_secure_installation  #配置mariadb<br> 可参考文档<a href="https://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/CDH5-Installation-Guide.html" target="_blank" rel="noopener">https://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/CDH5-Installation-Guide.html</a></p><p>2.为CDH创建数据库和用户</p><p>mysql -u root -p<br>输入密码登陆mysql ，然后创建多个数据库，并完成授权。</p><p>CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</p><p>GRANT ALL ON scm.* TO ‘scm‘@’%’ IDENTIFIED BY ‘scm’;</p><p>CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</p><p>GRANT ALL ON amon.* TO ‘amon‘@’%’ IDENTIFIED BY ‘amon’;</p><p>CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</p><p>GRANT ALL ON rman.* TO ‘rman‘@’%’ IDENTIFIED BY ‘rman’;</p><p>CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</p><p>GRANT ALL ON hue.* TO ‘hue‘@’%’ IDENTIFIED BY ‘hue’;</p><p>CREATE DATABASE hive DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</p><p>GRANT ALL ON hive.* TO ‘hive‘@’%’ IDENTIFIED BY ‘hive’;</p><p>CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</p><p>GRANT ALL ON sentry.* TO ‘sentry‘@’%’ IDENTIFIED BY ‘sentry’;</p><p>CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</p><p>GRANT ALL ON oozie.* TO ‘oozie‘@’%’ IDENTIFIED BY ‘oozie’;<br>最后，退出数据库进行数据库的初始化，执行语句类似下面。</p><p>exit #退出数据库<br>/user/share/cmf/schema/scm_prepare_database.sh (databaseType) (databaseName) (databaseuser) (databasepassword)<br>例如scm数据库： /usr/share/cmf/schema/scm_prepare_database.sh mysql scm scm scm</p><p>11.安装CDH</p><p>1.在master节点创建parcel-repo仓库</p><p>mkdir -p /opt/cloudera/parcel-repo<br>chown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo<br>2.将CDH安装包复制到/opt/cloudera/parcel-repo 目录下。</p><p>CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel</p><p>CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha1</p><p>manifest.json</p><p>然后将CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha1 重命名CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha</p><p>3.修改slave1和slave2的/etc/cloudera-scm-agent/config.ini</p><p>将server_host改为管理节点的网络名本例为master</p><p>4.分别启动cloudera-scm-server和cloudera-scm-agent</p><p>在主节点启动agent和server执行以下命令</p><p>systemctl start cloudera-scm-agent</p><p>systemctl start cloudera-scm-server<br>在slave1和slave2执行以下代码</p><p>systemctl start cloudera-scm-agent<br>5.进入<a href="http://master:7180,默认的用户名和密码均为admin开始添加集群，下面是一种添加服务的顺序。" target="_blank" rel="noopener">http://master:7180,默认的用户名和密码均为admin开始添加集群，下面是一种添加服务的顺序。</a></p><p>hdfs-&gt; yarn-&gt; hive-&gt; impala-&gt; zookeeper-&gt; hbase-&gt; oozie-&gt; hue-&gt;sqoop-&gt;kafka-&gt;spark</p><p> 12.总结</p><p>安装过程主要遇到的坑：</p><p>1.静态IP的配置</p><p>2.时间同步ntp服务</p><p>3.安装服务的对应文件夹的权限问题。主要就是查看日志更改对应文件的权限：</p><p>chmod 777 xxx<br>chown root XXX<br>4.kafka安装</p><p>在安装界面点击主机</p><p>点击parcel</p><p>点击KAFKA分配，并激活</p><p>然后添加kafka服务，并再配置的设置如下的参数：</p><p> kafak mirrormaker：</p><p>Destination Broker list slave1:9092</p><p>source list slave1:9092</p><p>topical whitelist slave1:9092</p><p>kafak Broker</p><p>Advertiesd Host   slave1</p><p>java heap size of broker 256</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;CDH 简易版离线安装
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;一、虚拟机搭建&lt;/p&gt;
&lt;p&gt;准备一台32G内存的电脑，安装虚拟机VMware-workstation。虚拟机下载地址：&lt;a href=&quot;http://download3.vmware.com/soft
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>NEO4J 图数据库使用APOC数据导入</title>
    <link href="http://yoursite.com/2019/01/14/NEO4J-%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8APOC%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5/"/>
    <id>http://yoursite.com/2019/01/14/NEO4J-图数据库使用APOC数据导入/</id>
    <published>2019-01-14T06:33:35.000Z</published>
    <updated>2019-01-14T06:33:35.829Z</updated>
    
    <content type="html"><![CDATA[<pre><code>Neo4j 数据导入</code></pre><p>一、安装与部署</p><pre><code>直接在官网下载安装包安装，解压即可。</code></pre><p>二、下载相应的jar包</p><p>1.sqlserver 数据导入neo4j的jar包</p><p>apoc-3.4.0.1-all.jar     mssql-jdbc-6.2.2.jre8.jar     sqljdbc4-4.0.jar</p><p>2.mysql 数据导入neo4j的jar包</p><p>apoc-3.3.0.1-all.jar    mysql-connector-java-8.0.8-dmr.jar</p><p>3.将对应jar包放在安装目录plugins文件目录里，然后conf目录里的neo4j.conf的后面加上</p><p>dbms.security.procedures.unrestricted=apoc.*</p><p>apoc.import.file.enabled=true<br>apoc.export.file.enabled=true</p><p>4.restart neo4j,运行return apoc.version(),若有版本号，则成功。</p><p>三、导数据</p><p>import org.neo4j.driver.v1.<em>;<br>public class Connect{<br>public static void main(String[] args){<br> Driver driver = GraphDatabase.driver(“bolt://localhost:7687”,AuthTokens.basic(“neo4j”,”neo4j”));<br> Session session = driver.session();<br> String cypher=”create constraint on (n:ITEM) ASSERT n.itemid is unique”; //创建唯一索引，这样可以更快的导入数据<br> Session.run(cypher);<br> cypher=”CALL apoc.periodic.iterate(\”CALL apoc.load.jdbc(‘jdbc:sqlserver://localhost;username=name;password=word;database=db;characterEncoding=utf-8’,\\”SELECT </em> FROM TABLE1\\”)\”,\”MERGE(n:ITEM{itemid:row.mitemid}) with <em> MERGE(m:ITEM{itemid:row.itemid}) with </em> create p=(n)-[r:rel{rels:row.rels}]-&gt;(m)\”,{batchSize:10000,iterateList:true})”;  //连接sqlserve数据库和设计创建neo4j图数据库数据模型<br> Session.run(cypher);<br> session.close();<br> driver.close();<br>   }<br>}<br>mysql数据库类似,不再赘述。</p><p>补充：1.使用neo4j-import导入数据的命令</p><p>neo4j-admin import –nodes:item  “nodes.csv”  –relationships:rel “rel_header.csv,rel.csv” –ignore-missing-nodes</p><p>2.apoc 导出命令</p><p>call apoc.export.cypher.query(<br>“MATCH (p1:Person)-[r:KNOWS]-&gt;(p2:Person) RETURN p1,r,p2”,<br>“/tmp/friendships.cypher”,<br>{format:’plain’,cypherFormat:’updateStructure’})`<br>参考： <a href="http://neo4j-contrib.github.io/neo4j-apoc-procedures/#_export_import" target="_blank" rel="noopener">http://neo4j-contrib.github.io/neo4j-apoc-procedures/#_export_import</a></p><p>call apoc.export.cypher.query(“match (n:lable) where not (n)–() and n.properties = ‘400’ return distinct(n)”,”C://User/Desktop/test”,{format:’plain’,cypherFormat:’create’})<br> 3.不用解压也能导数据load csv</p><p>load csv from “file:/twitter-2010.txt.gz” as line fieldterminator ‘ ‘ with toInt(line[0]) as id,toInt(line[1]) as id1 return id,id1 limit 10<br>using periodic commit 1000<br>load csv from “file:/twitter-2010.txt.gz” as line fieldterminator ‘ ‘ create (item:ITEM{id:line[0],item:line[1]})</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;Neo4j 数据导入
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;一、安装与部署&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;直接在官网下载安装包安装，解压即可。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;二、下载相应的jar包&lt;/p&gt;
&lt;p&gt;1.sqlserver 数据导入neo4j的jar
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>RDD常用算子</title>
    <link href="http://yoursite.com/2018/08/15/RDD%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90/"/>
    <id>http://yoursite.com/2018/08/15/RDD常用算子/</id>
    <published>2018-08-15T06:15:14.000Z</published>
    <updated>2018-08-15T10:52:32.586Z</updated>
    
    <content type="html"><![CDATA[<p><strong>1.mapPartitionsWithIndex:独立运行在每个分片上，并带有分区的编号。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9),2)</span><br><span class="line"> rdd1.saveAsTextFile(&quot;hdfs://master:8020/sfz&quot;) def func1(index:</span><br><span class="line">Int,iter: Iterator[(Int)]) : Iterator[String] = &#123;iter.toList.map(x =&gt;</span><br><span class="line"> &quot;[partID:&quot; + index + &quot;,val: &quot; + x +&quot;]&quot;).iterator&#125;</span><br><span class="line"> rdd1.mapPartitionsWithIndex(func1).collect</span><br></pre></td></tr></table></figure><p><strong><em>2.aggregate:先局部操作，再整体操作。</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val rdd2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;),2)</span><br><span class="line">def func2(index: Int,iter: Iterator[(String)]) : Iterator[String] = &#123;iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;,val: &quot; + x +&quot;]&quot;).iterator&#125;</span><br><span class="line">rdd2.mapPartitionsWithIndex(func2).collect</span><br><span class="line">rdd2.aggregate(&quot;&quot;)(_+_,_+_)</span><br><span class="line"></span><br><span class="line">val rdd3 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;4567&quot;),2)</span><br><span class="line">rdd3.aggregate(&quot;&quot;)((x,y) =&gt; math.max(x.length,y.length).toString,(x,y) =&gt; x+y)</span><br><span class="line"></span><br><span class="line">val rdd5 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;&quot;,&quot;345&quot;),2)</span><br><span class="line">rdd5.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length,y.length).toString,(x,y) =&gt; x+y)</span><br></pre></td></tr></table></figure><p><strong><em>3.aggregateByKey：将key值相同的，先局部操作，再整体操作。和reduceByKey内部实现差不多</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;,    4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 2)    pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;1.mapPartitionsWithIndex:独立运行在每个分片上，并带有分区的编号。&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;sp
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>SPARK DATAFRAME</title>
    <link href="http://yoursite.com/2018/08/14/SPARK-DATAFRAME/"/>
    <id>http://yoursite.com/2018/08/14/SPARK-DATAFRAME/</id>
    <published>2018-08-14T02:23:52.000Z</published>
    <updated>2018-08-14T02:23:52.880Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>图数据集群搭建</title>
    <link href="http://yoursite.com/2018/08/14/%E5%9B%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2018/08/14/图数据集群搭建/</id>
    <published>2018-08-14T02:15:50.000Z</published>
    <updated>2018-08-16T00:46:15.748Z</updated>
    
    <content type="html"><![CDATA[<pre><code>** NEO4J高可用集群搭建 **</code></pre><p>高可用的neo4j集群主要采用了主从的结构，来保证集群的容错能力和应变能力，同时也保证了了集群在读取密集型的数据的场景下可横向的扩展能力。同时，它还支持缓存分区，使得NEO4J高可用性集群比neo4j单实例具有更大的负载能力。但HA集群很快要不支持了。</p><p>好了，话不多说，如果看过前一篇文章<a href="https://blog.csdn.net/fffsssfff6/article/details/81215416" target="_blank" rel="noopener">https://blog.csdn.net/fffsssfff6/article/details/81215416</a>, 完成了前半部分的一些基本准备，那么就可以直接进行HA集群搭建。若没有准备，则需要完成至JDK安装的步骤。下面就开始了：</p><p>一、首先 下载neo4j企业版的安装包。可以参考<a href="https://blog.csdn.net/xubo245/article/details/50033003" target="_blank" rel="noopener">https://blog.csdn.net/xubo245/article/details/50033003</a>.  执行下面命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  http://dist.neo4j.org/neo4j-enterprise-3.4.0-unix.tar.gz</span><br><span class="line">```  </span><br><span class="line"> 或者前往neo4j官网下载 ：https://neo4j.com/download/ .</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">二、然后将安装包解压后分别传入到 /opt/neo4j 目录下。</span><br><span class="line">``` bash</span><br><span class="line"></span><br><span class="line">tar -zxvf neo4j-enterprise-3.4.0-unix.tar.gz</span><br><span class="line">scp -r neo4j-enterprise-3.4.0 root@master: /opt/neo4j</span><br><span class="line">scp -r neo4j-enterprise-3.4.0 root@slave1: /opt/neo4j</span><br><span class="line">scp -r neo4j-enterprise-3.4.0 root@slave2: /opt/neo4j</span><br></pre></td></tr></table></figure></p><p>三 、修改配置文件neo4j.conf(重要)</p><p>master节点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dbms.mode=HA</span><br><span class="line">ha.server_id=1</span><br><span class="line">ha.initial_hosts=172.16.247.135:5001,172.16.247.132:5001,172.16.247.136:5001</span><br><span class="line">dbms.connectors.default_listen_address=0.0.0.0</span><br></pre></td></tr></table></figure></p><p>slave1节点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dbms.mode=HA</span><br><span class="line">ha.server_id=2</span><br><span class="line">ha.initial_hosts=172.16.247.135:5001,172.16.247.132:5001,172.16.247.136:5001</span><br><span class="line">dbms.connectors.default_listen_address=0.0.0.0</span><br></pre></td></tr></table></figure><p>slave2节点：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dbms.mode=HA</span><br><span class="line">ha.server_id=3</span><br><span class="line">ha.initial_hosts=172.16.247.135:5001,172.16.247.132:5001,172.16.247.136:5001</span><br><span class="line">dbms.connectors.default_listen_address=0.0.0.0</span><br></pre></td></tr></table></figure><p>四、启动HA集群,分别进入neo4j 目录下执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">./bin/neo4j start</span><br><span class="line">./bin/neo4j start</span><br><span class="line">./bin/neo4j start</span><br></pre></td></tr></table></figure><p> 五、进入localhost：7474查看集群信息<br><img src="https://img-blog.csdn.net/20180803153649681?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ZGRlNTU0ZGRjY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="Image text"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;code&gt;** NEO4J高可用集群搭建 **
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;高可用的neo4j集群主要采用了主从的结构，来保证集群的容错能力和应变能力，同时也保证了了集群在读取密集型的数据的场景下可横向的扩展能力。同时，它还支持缓存分区，使得NEO4J高可用性集群
      
    
    </summary>
    
      <category term="图数据库" scheme="http://yoursite.com/categories/%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="neo4j" scheme="http://yoursite.com/tags/neo4j/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/08/13/hello-world/"/>
    <id>http://yoursite.com/2018/08/13/hello-world/</id>
    <published>2018-08-13T08:48:53.448Z</published>
    <updated>2018-08-13T08:48:53.448Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
