<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="天道酬勤">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    
    上善若水博客</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
    <link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">
  
  <script src="/js/pace.min.js"></script>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body>
<main class="content">
  <section class="jumbotron">
  <div class="video">
    
      <div class="video-frame">
        <img src="/images/ocean/overlay-hero.png" alt="Decorative image frame">
      </div>
    
    <div class="video-media">
      <video playsinline="" autoplay="" loop="" muted="" data-autoplay=""
             poster="/images/ocean/ocean.png" x5-video-player-type="h5">
        <source src="/images/ocean/ocean.mp4" type="video/mp4">
        <source src="/images/ocean/ocean.ogv" type="video/ogg">
        <source src="/images/ocean/ocean.webm" type="video/webm">
        <p>Your user agent does not support the HTML5 Video element.</p>
      </video>
      <div class="video-overlay"></div>
    </div>
    <div class="video-inner text-center text-white">
      <h1><a href="/">上善若水博客</a></h1>
      <p>大数据</p>
      <div><img src="/images/hexo-inverted.svg" class="brand" alt="上善若水博客"></div>
    </div>
    <div class="video-learn-more">
      <a class="anchor" href="#landingpage"><i class="fe fe-mouse"></i></a>
    </div>
  </div>
</section>

<div id="landingpage">
  <section class="outer">
  <article class="articles">
    
    <h1 class="page-type-title"></h1>

    
      
        

<article id="post-spring-boot-jpa" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/11/30/spring-boot-jpa/">spring boot jpa</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/11/30/spring-boot-jpa/" class="article-date">
  <time datetime="2019-11-30T01:04:38.000Z" itemprop="datePublished">2019-11-30</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/spring-boot/">spring boot</a>
  </div>

      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>spring boot jpa<br> jpa 常用于数据库访问</p>
<p>1.依赖</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.1.5.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;
    &lt;groupId&gt;com.gree.bdc&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-jpa&lt;/artifactId&gt;
    &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;jpa-test&lt;/name&gt;
    &lt;description&gt;jpa-test&lt;/description&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!-- spring boot begin--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;!--mysql 依赖--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;version&gt;8.0.11&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- log4j--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt;
            &lt;version&gt;1.3.8.RELEASE&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- spring boot jpa --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;
        &lt;/dependency&gt;


    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;


</code></pre><p>2.application.properties 配置，创建入口类。</p>
<pre><code>#通用数据源配置

spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
spring.datasource.url=jdbc:mysql://localhost:3306/springboot_jpa?charset=utf8mb4&amp;useSSL=false&amp;serverTimezone=GMT
spring.datasource.username=root
spring.datasource.password=123456
# Hikari 数据源专用配置
spring.datasource.hikari.maximum-pool-size=20
spring.datasource.hikari.minimum-idle=5
# JPA 相关配置
spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialect
spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto=create

</code></pre><pre><code>package com.gree.bdc.jpa;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class JpaApplication {
    public static void main(String[] args) {
        SpringApplication.run(JpaApplication.class,args);
    }
}

</code></pre><p>3.创建实体</p>
<pre><code>package com.gree.bdc.jpa.entity;

import javax.persistence.Column;
import javax.persistence.Entity;
import javax.persistence.Id;
import javax.persistence.Table;

@Entity
@Table(name = &quot;AUTH_USER&quot;)
public class UserDO {
        @Id
        private Long id;
        @Column(length = 32)
        private String name;
        @Column(length = 32)
        private String account;
        @Column(length = 64)
        private String pwd;

        public Long getId() {
            return id;
        }

        public void setId(Long id) {
            this.id = id;
        }

        public String getName() {
            return name;
        }

        public void setName(String name) {
            this.name = name;
        }

        public String getAccount() {
            return account;
        }

        public void setAccount(String account) {
            this.account = account;
        }

        public String getPwd() {
            return pwd;
        }

        public void setPwd(String pwd) {
            this.pwd = pwd;
        }
}


</code></pre><p>4.写一个DAO继承Repository或他的子类，执行入口类创建数据库表。后注释掉配置文件</p>
<p>spring.jpa.hibernate.ddl-auto=create</p>
<pre><code>package com.gree.bdc.jpa.repository;

import com.gree.bdc.jpa.entity.UserDO;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface UserDAO extends JpaRepository&lt;UserDO,Long&gt; {
}


</code></pre><p>5.使用DAO调用其操作数据库方法</p>
<pre><code>
package com.gree.bdc.jpa;

import com.gree.bdc.jpa.entity.UserDO;
import com.gree.bdc.jpa.repository.UserDAO;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.junit.runner.RunWith;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.junit4.SpringRunner;

import java.util.List;

@RunWith(SpringRunner.class)
@SpringBootTest
public class UserDOTest {

    @Autowired
    private UserDAO userDAO;

    @Before
    public void before(){
        UserDO userDO = new UserDO();
        userDO.setId(1L);
        userDO.setName(&quot;风清扬&quot;);
        userDO.setAccount(&quot;fengqy&quot;);
        userDO.setPwd(&quot;123456&quot;);
        userDAO.save(userDO);
        userDO = new UserDO();
        userDO.setId(3L);
        userDO.setName(&quot;东方不败&quot;);
        userDO.setAccount(&quot;bubai&quot;);
        userDO.setPwd(&quot;123456&quot;);
        userDAO.save(userDO);
        userDO.setId(5L);
        userDO.setName(&quot;向问天&quot;);
        userDO.setAccount(&quot;wentian&quot;);
        userDO.setPwd(&quot;123456&quot;);
        userDAO.save(userDO);
    }

    @Test
    public void testAdd(){
        UserDO userDO = new UserDO();
        userDO.setId(2L);
        userDO.setName(&quot;任我行&quot;);
        userDO.setAccount(&quot;renwox&quot;);
        userDO.setPwd(&quot;123456&quot;);
        userDAO.save(userDO);
        userDO = new UserDO();
        userDO.setId(4L);
        userDO.setName(&quot;令狐冲&quot;);
        userDO.setAccount(&quot;linghuc&quot;);
        userDO.setPwd(&quot;123456&quot;);
        userDAO.save(userDO);
    }
    @After
    public void after(){
        userDAO.deleteById(1L);
        List&lt;UserDO&gt; users = userDAO.findAll();
        for(UserDO user: users){
            System.out.println(user.getName());
        }

    }

}

</code></pre><p>参考文档：<a href="https://www.jianshu.com/p/c14640b63653" target="_blank" rel="noopener">spring boot jpa</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/30/spring-boot-jpa/" data-id="ck3o5xx86003a8wuji1r1yacb"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/springboot/">springboot</a></li></ul>

    </footer>

  </div>

  

  

</article>



      
        

<article id="post-java-基本知识" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/11/29/java-基本知识/">java 基本知识</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/11/29/java-基本知识/" class="article-date">
  <time datetime="2019-11-29T02:00:58.000Z" itemprop="datePublished">2019-11-29</time>
</a>
        
      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>1.泛型<br>  通过泛型可以定义类型安全的数据结构（类型安全），而无须使用实际的数据类型（可扩展）。这能够显著提高性能并得到更高质量的代码（高性能），因为您可以重用数据处理算法，而无须复制类型特定的代码（可重用）。</p>
<p>2.java.sql.SQLException: Unknown system variable ‘query_cache_size’</p>
<pre><code> &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;

</code></pre><p>3.java.sql.SQLException: The server time zone value ‘ÖÐ¹ú±ê×¼Ê±¼ä’ is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the serverTimezone configuration property) to use a more specifc time zone value if you want to utilize time zone support.</p>
<pre><code>jdbc:mysql://localhost:3306/springboot_jpa?charset=utf8mb4&amp;useSSL=false&amp;serverTimezone=GMT

</code></pre><p>4.java.lang.Exception: No runnable methods</p>
<blockquote>
<p>测试时 未在test方法上加上 @test 注解，同时保证注解依赖org.junit.test</p>
</blockquote>
<p>5.STRING…</p>
<blockquote>
<p>类型后面三个点(String…)，是从Java 5开始，Java语言对方法参数支持一种新写法，叫可变长度参数列表，其语法就是类型后跟…，表示此处接受的参数为0到多个Object类型的对象，或者是一个Object[]。</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/29/java-基本知识/" data-id="ck3o5xx6x001o8wujjxtuaxng"
         class="article-share-link">Share</a>
      
    </footer>

  </div>

  

  

</article>



      
        

<article id="post-spark-常用-api" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/11/26/spark-常用-api/">spark 常用 api</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/11/26/spark-常用-api/" class="article-date">
  <time datetime="2019-11-26T03:20:57.000Z" itemprop="datePublished">2019-11-26</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>1.spark 中采用部分关联字段</p>
<pre><code>//可以看出Temp_khcp 字段包含 first_chart 字段 ，通过部分关联来实现表连接
  val alphabet_Beign = number_Beign.join(custom_file_regular, col(&quot;Temp_khcp&quot;).startsWith(col(&quot;first_chart&quot;)), &quot;left&quot;).drop(&quot;first_chart&quot;).na.fill(&quot;&quot;, Seq(&quot;before_file&quot;))

</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/11/26/spark-常用-api/" data-id="ck3o5xx7y002z8wuj5q8dk5zp"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>

  </div>

  

  

</article>



      
        

<article id="post-Docker-三剑客之-Docker-Compose" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/10/25/Docker-三剑客之-Docker-Compose/">Docker 三剑客之 Docker Compose</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/10/25/Docker-三剑客之-Docker-Compose/" class="article-date">
  <time datetime="2019-10-25T00:55:10.000Z" itemprop="datePublished">2019-10-25</time>
</a>
        
      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><pre><code>Docker Compose 定位是 定义和运行多个docker容器应用（Defining and running multi-container Docker applications）

它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）

Compose 中有两个重要的概念：

服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。
项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。

</code></pre><h1 id="安装与卸载"><a href="#安装与卸载" class="headerlink" title="安装与卸载"></a>安装与卸载</h1><pre><code>1.安装
$ sudo curl -L https://github.com/docker/compose/releases/download/1.17.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose
$ sudo chmod +x /usr/local/bin/docker-compose

2.卸载
sudo rm /usr/local/bin/docker-compose

</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/25/Docker-三剑客之-Docker-Compose/" data-id="ck3o5xx6c000r8wujdv8tgaw8"
         class="article-share-link">Share</a>
      
    </footer>

  </div>

  

  

</article>



      
        

<article id="post-Docker-学习笔记" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/10/23/Docker-学习笔记/">Docker 学习笔记</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/10/23/Docker-学习笔记/" class="article-date">
  <time datetime="2019-10-23T07:13:40.000Z" itemprop="datePublished">2019-10-23</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/docker/">docker</a>
  </div>

      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <h1 id="docker-安装"><a href="#docker-安装" class="headerlink" title="docker 安装"></a>docker 安装</h1><p><a href="https://www.runoob.com/docker/centos-docker-install.html" target="_blank" rel="noopener">docker 安装地址</a></p>
<h1 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h1><pre><code>1、docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.
See &#39;docker run --help&#39;.

可能docker上次为正常关闭导致： systemctl start docker 

</code></pre><h1 id="验证docker-安装成功"><a href="#验证docker-安装成功" class="headerlink" title="验证docker 安装成功"></a>验证docker 安装成功</h1><pre><code>docker run hello-world

</code></pre><p><img src="/images/2019/10/23/dc94e130-f564-11e9-84ab-7fe2af665eb5.png" alt="image.png"></p>
<h1 id="常见镜像操作"><a href="#常见镜像操作" class="headerlink" title="常见镜像操作"></a>常见镜像操作</h1><pre><code>1.获取镜像
docker pull ubuntu:16.04

2.运行镜像,进入容器
docker run -it --rm \
ubuntu:16.04 \
bash

3. 退出容器
exit

4.列出镜像
docker image ls

5.查看镜像、容器、数据卷所占用的空间
docker system df

6.虚悬镜像
docker image ls -f dangling=true

7.删除虚悬镜像
docker image prune

8.中间层镜像
docker image ls -a

9.部分镜像列出
docker image ls ubuntu
docker image ls ubuntu:16.04
docker image ls -f since=mongo:3.2
docker image ls -f label=com.example.version=0.1

10. 特定格式显示
docker image ls -q
docker image ls --format &quot;{{.ID}}: {{.Repository}}&quot;
docker image ls --format &quot;table {{.ID}}\t{{.Repository}}\t{{.Tag}}&quot;

</code></pre><h1 id="镜像构建"><a href="#镜像构建" class="headerlink" title="镜像构建"></a>镜像构建</h1><pre><code>$ mkdir mynginx
$ cd mynginx
$ touch Dockerfile

Dockerfile内容:

FROM nginx
RUN echo &#39;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&#39; &gt; /usr/share/nginx/html/index.html

在Dockerfile所在的目录执行
docker build -t nginx:v3 .

</code></pre><h1 id="常见容器操作"><a href="#常见容器操作" class="headerlink" title="常见容器操作"></a>常见容器操作</h1><pre><code>1.新建并启动容器
docker run ubuntu:16.04 /bin/echo &#39;Hello World!&#39;

2.在终端打开容器
docker run -t -i  ubuntu:16.04 /bin/bash  （其中，-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开）

3. 启动已经终止的容器
docker container start --help

4.后台运行容器
docker run -d  ubuntu:16.04 /bin/sh -c &quot;while true ;do echo hello world;sleep 1;done&quot;
5.查看运行容器
docker container ls

6.查看容器的运行日志
docker logs container id
docker logs 583d0660cf1364e7e52a996e0ccab94c2c9e1b88a831233e5777adf6d4684601

7.终止一个容器
docker container stop container id
docker container stop 583d0660cf13

8.查看所有的容器
docker container ls -a

9.启动已经停止的容器
docker container start container id
docker container start 583d0660cf13

10.重启容器
docker container restart

</code></pre><h1 id="进入容器两种方式"><a href="#进入容器两种方式" class="headerlink" title="进入容器两种方式"></a>进入容器两种方式</h1><pre><code>1. attach 命令
docker run  -dit ubuntu
docker container ls
docker attach 14bf

可以看出在退出容器是这个容器也会被停止。

2. exec命令
 docker exec -it f2a8 bash
这个命令退出时不会停止容器。所有推荐使用这个命令进入容器中进行操作。

</code></pre><h1 id="数据卷"><a href="#数据卷" class="headerlink" title="数据卷"></a>数据卷</h1><pre><code>1.创建数据卷
docker volume create my-vol
2.查看所有数据卷
docker volume ls
3.查看某个数据卷的具体信息
docker volume inspect my-vol
4.启动一个挂载数据卷的容器

docker run -d -P \
&gt; --name web \
&gt; --mount source=my-vol,target=/webapp \
&gt; training/webapp \
&gt; python app.py

5.删除数据卷
docker volume rm my-vol
如果报错：
Error response from daemon: remove my-vol: volume is in use - [e6520e12082e077ff427d2ec87dc32c61bf30414570009a82363c706d5e2072e]

则需要先停止正在使用数据卷的容器，并删除它。
docker stop e6520e12082e
docker rm e6520e12082e

6.清除无主的数据卷
docker volume prune

</code></pre><h1 id="docker-构建-Tomcat"><a href="#docker-构建-Tomcat" class="headerlink" title="docker 构建 Tomcat"></a>docker 构建 Tomcat</h1><pre><code>1.搜索Tomcat镜像
docker search tomcat
2.拉取Tomcat镜像
docker pull tomcat
3.运行容器
docker run --name tomcat -p 8080:8080 -v $PWD/test:/usr/local/tomcat/webapps/test -d tomcat
命令说明：

-p 8080:8080：将容器的8080端口映射到主机的8080端口
-v $PWD/test:/usr/local/tomcat/webapps/test：将主机中当前目录下的test挂载到容器的/test
</code></pre><h1 id="docker-构建-mysql"><a href="#docker-构建-mysql" class="headerlink" title="docker 构建 mysql"></a>docker 构建 mysql</h1><pre><code>1.搜索mysql镜像
docker search mysql
2.拉取mysql镜像
docker pull mysql
3.运行mysql镜像，构建mysql容器
docker run -p 3306:3306 --name mysql \
-v /usr/local/docker/mysql/conf:/etc/mysql \
-v /usr/local/docker/mysql/logs:/var/log/mysql \
-v /usr/local/docker/mysql/data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=123456 \
-d mysql

命令参数：

-p 3306:3306：将容器的3306端口映射到主机的3306端口
-v /usr/local/docker/mysql/conf:/etc/mysql：将主机当前目录下的 conf 挂载到容器的 /etc/mysql
-v /usr/local/docker/mysql/logs:/var/log/mysql：将主机当前目录下的 logs 目录挂载到容器的 /var/log/mysql
-v /usr/local/docker/mysql/data:/var/lib/mysql：将主机当前目录下的 data 目录挂载到容器的 /var/lib/mysql
-e MYSQL\_ROOT\_PASSWORD=123456：初始化root用户的密码

</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/23/Docker-学习笔记/" data-id="ck3o5xx61000f8wujap5e1f28"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/容器/">容器</a></li></ul>

    </footer>

  </div>

  

  

</article>



      
        

<article id="post-HBase-常用工具类" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/10/17/HBase-常用工具类/">HBase 常用工具类</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/10/17/HBase-常用工具类/" class="article-date">
  <time datetime="2019-10-17T02:13:36.000Z" itemprop="datePublished">2019-10-17</time>
</a>
        
      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <pre><code>object HBaseUtils {
  def logger: Logger = LoggerFactory.getLogger(getClass)
  /**
    * 获取配置参数信息
    * @return    
*/ 
 def getHBaseConf : Configuration = {
    val conf : Configuration = HBaseConfiguration.create
    conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;cdh-master01,cdh-master02,cdh-master03&quot;)
    conf.set(&quot;hadoop.security.authentication&quot;, &quot;Kerberos&quot;)
    conf
  }
  /**
    * 获取连接
    * @param conf 配置信息
    * @return
    */
  def getConnection(conf:Configuration): Connection ={
    ConnectionFactory.createConnection(conf)
  }
  /**
    * 获取管理员权限
    * @param conn 连接信息
    * @return HBaseAdmin
    */
  def getAdmin(conn:Connection) : HBaseAdmin = {
    conn.getAdmin.asInstanceOf[HBaseAdmin]
  }
  /**
    * 创建表
    * @param admin 管理员
    * @param tableName 表名
    * @param columnFamily 列族
    * @throws org.apache.hadoop.hbase.MasterNotRunningException 异常
    * @throws org.apache.hadoop.hbase.ZooKeeperConnectionException 异常
    * @throws java.io.IOException 异常
    */
  @throws(classOf[MasterNotRunningException])
  @throws(classOf[ZooKeeperConnectionException])
  @throws(classOf[IOException])
  def createTable(admin: HBaseAdmin,tableName: String,columnFamily:Array[String]):Unit = {
    val createTableName = TableName.valueOf(tableName)
    if (admin.tableExists(createTableName)){
      logger.info(tableName + &quot;table exists!&quot;)
    }else{
      val tableDesc = new HTableDescriptor(createTableName)      
      tableDesc.addCoprocessor(&quot;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&quot;)
      for (singleColumnFamily &lt;- columnFamily){
        val columnDesc = new HColumnDescriptor(singleColumnFamily)
        tableDesc.addFamily(columnDesc)
      } 
admin.createTable(tableDesc)
      logger.info(tableName + &quot; create table success!&quot;)
    }
admin.close() 
 }

 /**
    * 载入数据
    * @param table HBase表
    * @param rowKey 行键
    * @param columnFamily 列族
    * @param quorum 分布式信息
    * @param value 数值
    */
  def addRow(table: Table,rowKey:String,columnFamily:String ,quorum:String,value:String): Unit ={
    val rowPut:Put = new Put(Bytes.toBytes(rowKey))
    if (value == null){
      rowPut.addColumn(columnFamily.getBytes,quorum.getBytes,&quot;&quot;.getBytes)
    }else{
      rowPut.addColumn(columnFamily.getBytes,quorum.getBytes,value.getBytes)
    }
    table.put(rowPut)
  }

  /**
    * 获取表数据
    * @param table HBase表
    * @param rowKey 行键
    * @return HBase result
    */
  def getRow(table: Table,rowKey: String ):Result = {
    val get:Get = new Get(Bytes.toBytes(rowKey))
    val result:Result = table.get(get)
    for (rowKv &lt;- result.rawCells()){
      println(&quot;Family:&quot; + new String(rowKv.getFamilyArray,rowKv.getFamilyOffset,rowKv.getFamilyLength,&quot;UT-8&quot;))      println(&quot;Qualifier:&quot; + new String(rowKv.getQualifierArray,rowKv.getQualifierOffset,rowKv.getQualifierLength,&quot;UT-8&quot;))      println(&quot;TimeStamp:&quot; + rowKv.getTimestamp)
      println(&quot;rowKey:&quot; + new String(rowKv.getRowArray,rowKv.getRowOffset,rowKv.getRowLength,&quot;UT-8&quot;))
      println(&quot;Value:&quot; + new String(rowKv.getValueArray,rowKv.getValueOffset,rowKv.getValueLength,&quot;UT-8&quot;))
   }
    result
  }

  /**
    * 批量添加数据
    * @param table HBase表
    * @param list 数据列表
    */
  def addDataBatch(table: Table,list: java.util.List[Put]): Unit = {
    try{
      table.put(list)
    }catch{
        logger.error(e.getMessage)
      case e: IOException =&gt; 
       logger.error(e.getMessage)
    }
  }
  /**
    * 查询所有记录
    * @param table HBase表
    * @return resultScanner
    */
  def queryAll(table: Table):ResultScanner = {
    val scan: Scan = new Scan
    try {
      val result: ResultScanner = table.getScanner(scan)
      result
    }catch {
      case e: IOException =&gt;
      logger.error(e.getMessage)
        null
    }
  }

 /**
    * 单条记录查询
    * @param table HBase表
    * @param queryColumn 查询列 
    * @param value 数值
    * @param columns 列集合
    * @return ResultScanner
    */
  def queryBySingleColumn(table: Table, queryColumn: String, value: String, columns: Array[String]): ResultScanner = {
    if (columns == null || queryColumn == null || value == null ){
      null
    }else{
      try {
        val filter: SingleColumnValueFilter = new SingleColumnValueFilter(Bytes.toBytes(queryColumn),Bytes.toBytes(queryColumn),CompareOperator.EQUAL,new SubstringComparator(value))
        val scan: Scan = new Scan
        for (column  logger.error(e.getMessage)
          null
      }
    }
  }
  /**
    * 删除表
    * @param hBaseConnection 链接
    * @param tableName HBase表
    */
  def dropTable(hBaseConnection: Connection,tableName: String): Unit = {
    try {
      val admin: HBaseAdmin = hBaseConnection.getAdmin.asInstanceOf[HBaseAdmin]      admin.disableTable(TableName.valueOf(tableName))
      admin.deleteTable(TableName.valueOf(tableName))
    }catch {
      case e: MasterNotRunningException =&gt; logger.error(e.getMessage)
      case e: ZooKeeperConnectionException =&gt; logger.error(e.getMessage)
      case e: IOException =&gt; logger.error(e.getMessage)
    }
  }
}

org.apache.hbase 
hbase-client
${hbase.version}
org.apache.hadoop 
hadoop-annotations                                            

</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/17/HBase-常用工具类/" data-id="ck3o5xx5x000d8wujkadwpb9r"
         class="article-share-link">Share</a>
      
    </footer>

  </div>

  

  

</article>



      
        

<article id="post-Apache-NiFi-简单介绍和使用" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/09/11/Apache-NiFi-简单介绍和使用/">Apache NiFi 简单介绍和使用</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/09/11/Apache-NiFi-简单介绍和使用/" class="article-date">
  <time datetime="2019-09-11T06:59:39.000Z" itemprop="datePublished">2019-09-11</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/ETL/">ETL</a> / <a class="article-category-link" href="/categories/ETL/NIFI/">NIFI</a>
  </div>

      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <h2 id="一、什么是Apache-NiFi"><a href="#一、什么是Apache-NiFi" class="headerlink" title="一、什么是Apache NiFi"></a>一、什么是Apache NiFi</h2><pre><code>简单来讲，NIFI就是为了构建系统之间的数据自动化传输的简易操作工具。提供了一些可靠的数据流的传输工具，解决了大部分现代企业中的数据ETL中所遇到的挑战。
</code></pre><h2 id="二、NIFI-核心概念"><a href="#二、NIFI-核心概念" class="headerlink" title="二、NIFI 核心概念"></a>二、NIFI 核心概念</h2><p><img src="/images/2019/09/11/7c2f6a40-d45d-11e9-b427-5d1ae987f58a.png" alt="image.png"><br><img src="/images/2019/09/11/a544b020-d45d-11e9-b427-5d1ae987f58a.png" alt="image.png"></p>
<h2 id="三、NIFI-架构"><a href="#三、NIFI-架构" class="headerlink" title="三、NIFI 架构"></a>三、NIFI 架构</h2><p><img src="/images/2019/09/11/dd2a1340-d45d-11e9-b427-5d1ae987f58a.png" alt="image.png"></p>
<h2 id="四、-NIFI-使用界面介绍"><a href="#四、-NIFI-使用界面介绍" class="headerlink" title="四、 NIFI 使用界面介绍"></a>四、 NIFI 使用界面介绍</h2><p><img src="/images/2019/09/11/79ee7e20-d461-11e9-b427-5d1ae987f58a.png" alt="image.png"></p>
<blockquote>
<p>主要包括工具栏、状态栏、总菜单栏，操作缩图和画布。平常用到的组件主要有<br>Processor:The Processor is the NiFi component that is used to listen for incoming data; pull data from external sources; publish data to external sources; and route, transform, or extract information from FlowFiles.</p>
</blockquote>
<blockquote>
<p>Processor Group:When a dataflow becomes complex, it often is beneficial to reason about the dataflow at a higher, more abstract level. NiFi allows multiple components, such as Processors, to be grouped together into a Process Group. The NiFi User Interface then makes it easy for a DFM to connect together multiple Process Groups into a logical dataflow, as well as allowing the DFM to enter a Process Group in order to see and manipulate the components within the Process Group.</p>
</blockquote>
<blockquote>
<p>我认为 Processor 就是一个数据处理器，它不仅可以从各种数据源中加载数据，转成Fiedflow基本nifi元素，还能够将数据处理成各种常见的格式以及将数据输出到各种数据库中<br>而Processor Group 则是一个装载 Processor的容器。</p>
</blockquote>
<h2 id="五、案例（JSON-TO-MYSQL）"><a href="#五、案例（JSON-TO-MYSQL）" class="headerlink" title="五、案例（JSON_TO_MYSQL）"></a>五、案例（JSON_TO_MYSQL）</h2><ol>
<li>创建一个Processor Group ,将其拖到画布当中<br><img src="/images/2019/09/11/b8052b50-d466-11e9-b427-5d1ae987f58a.png" alt="image.png"></li>
<li>然后修改名字<br><img src="/images/2019/09/11/f0cefb00-d466-11e9-b427-5d1ae987f58a.png" alt="image.png"><br><img src="/images/2019/09/11/52a49ab0-d467-11e9-b427-5d1ae987f58a.png" alt="image.png"></li>
<li>读取json文件，GetFile –&gt; 将JSON 转为sql语句，ConvertJSONToSQL –&gt; 运行sql语句，PutSQL</li>
<li>GetFile 设置输入目录Input Directory，目标文件 File Filter<br><img src="/images/2019/09/11/5e82d2b0-d468-11e9-b427-5d1ae987f58a.png" alt="image.png"></li>
<li>ConvertJSONToSQL  设置数据库连接池JDBC Connection Pool ,SQL 类型 Statement Type ,表名 Table Name,其他默认 ，注意 ConvertJSONToSQL 只适用一个json值的文件<br><img src="/images/2019/09/11/3459dd70-d469-11e9-b427-5d1ae987f58a.png" alt="image.png"></li>
<li>PutSQL 设置 数据库连接池，SQL Statement ，其他默认。一般来说只需要设置加黑的参数，但这里需要设置sql语句。<br><img src="/images/2019/09/11/c980d930-d469-11e9-b427-5d1ae987f58a.png" alt="image.png"></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/09/11/Apache-NiFi-简单介绍和使用/" data-id="ck3o5xx5q00068wuj5d1ouvh3"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ETL/">ETL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NIFI/">NIFI</a></li></ul>

    </footer>

  </div>

  

  

</article>



      
        

<article id="post-NIFI-安装部署" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/09/09/NIFI-安装部署/">NIFI 安装部署</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/09/09/NIFI-安装部署/" class="article-date">
  <time datetime="2019-09-09T01:43:41.000Z" itemprop="datePublished">2019-09-09</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/ETL/">ETL</a> / <a class="article-category-link" href="/categories/ETL/NIFI/">NIFI</a>
  </div>

      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>::: hljs-center</p>
<p>NIFI 安装部署</p>
<p>:::</p>
<h2 id="一、NIFI-下载"><a href="#一、NIFI-下载" class="headerlink" title="一、NIFI 下载"></a>一、NIFI 下载</h2><ul>
<li>NIFI 下载链接：<a href="http://nifi.apache.org/download.html" target="_blank" rel="noopener">NIFI Downloads</a></li>
<li>选择与自己环境匹配的安装包</li>
<li><img src="/images/2019/09/09/b6d2a810-d2a0-11e9-aa7e-85cf9ceb3292.png" alt="image.png"></li>
<li>若是下载速度过慢，可以选择另一个下载地址：<a href="https://www.apache.org/dyn/closer.lua?path=/nifi/1.9.2/nifi-1.9.2-bin.zip" target="_blank" rel="noopener">NIFI 下载</a></li>
<li><img src="/images/2019/09/09/7bb8b160-d2a1-11e9-aa7e-85cf9ceb3292.png" alt="image.png"></li>
</ul>
<h2 id="二、NIFI-安装"><a href="#二、NIFI-安装" class="headerlink" title="二、NIFI 安装"></a>二、NIFI 安装</h2><ol>
<li>将下载的安装包放到 E:\software\ ，然后解压安装，非常简单</li>
<li>修改配置文件E:\software\nifi-1.9.2-bin\nifi-1.9.2\conf\nifi.properties，更改端口号</li>
<li><img src="/images/2019/09/09/6df7ebd0-d2a2-11e9-aa7e-85cf9ceb3292.png" alt="image.png"></li>
</ol>
<h2 id="三-、NIFI-运行"><a href="#三-、NIFI-运行" class="headerlink" title="三 、NIFI 运行"></a>三 、NIFI 运行</h2><ol>
<li>进入E:\software\nifi-1.9.2-bin\nifi-1.9.2\bin 目录，双击run-nifi.bat，若是linux则sh nifi.sh</li>
<li>打开浏览器，进入<a href="http://localhost:8081/nifi/，可能需要等会页面加载慢" target="_blank" rel="noopener">http://localhost:8081/nifi/，可能需要等会页面加载慢</a></li>
<li><img src="/images/2019/09/09/2a0a96b0-d2a3-11e9-aa7e-85cf9ceb3292.png" alt="image.png"></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/09/09/NIFI-安装部署/" data-id="ck3o5xx6j00138wujzxemav2i"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ETL/">ETL</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NIFI/">NIFI</a></li></ul>

    </footer>

  </div>

  

  

</article>



      
        

<article id="post-maven-打包相关记录" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/09/05/maven-打包相关记录/">maven 打包相关记录</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/09/05/maven-打包相关记录/" class="article-date">
  <time datetime="2019-09-05T02:20:06.000Z" itemprop="datePublished">2019-09-05</time>
</a>
        
      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <h1 id="Maven项目打包"><a href="#Maven项目打包" class="headerlink" title="Maven项目打包"></a>Maven项目打包</h1><pre><code>这里主要记录常用的打包方式（将依赖打包进jar包）。
</code></pre><h2 id="一、spring-boot-项目"><a href="#一、spring-boot-项目" class="headerlink" title="一、spring boot 项目"></a>一、spring boot 项目</h2><p>对于spring boot项目只需要添加spring boot 和maven整合的插件就可以</p>
<pre><code> &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;parent&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
        &lt;version&gt;2.1.5.RELEASE&lt;/version&gt;
        &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
    &lt;/parent&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;

    &lt;!-- spring boot 依赖 --&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                    &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
            &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;!-- spring boot end --&gt;


    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
                &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

&lt;/project&gt;


</code></pre><h2 id="二、普通的java项目"><a href="#二、普通的java项目" class="headerlink" title="二、普通的java项目"></a>二、普通的java项目</h2><p>对于普通的java项目需要添加额外的依赖才能够将依赖打包进jar中</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;


    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.5.5&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;archive&gt;
                        &lt;manifest&gt;
                            &lt;mainClass&gt;com.gree.cn.location.LocationMap&lt;/mainClass&gt;
                        &lt;/manifest&gt;
                    &lt;/archive&gt;
                    &lt;descriptorRefs&gt;
                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                    &lt;/descriptorRefs&gt;
                &lt;/configuration&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;make-assembly&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;single&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.3.1&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.8&lt;/source&gt;
                    &lt;target&gt;1.8&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;


</code></pre><h2 id="三、scala-项目打包"><a href="#三、scala-项目打包" class="headerlink" title="三、scala 项目打包"></a>三、scala 项目打包</h2><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;

    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;
                &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.15.2&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;scala-compile-first&lt;/id&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;compile&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;includes&gt;
                                &lt;include&gt;**/*.scala&lt;/include&gt;
                            &lt;/includes&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                    &lt;execution&gt;
                        &lt;id&gt;scala-test-compile&lt;/id&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;testCompile&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;
                    &lt;descriptorRefs&gt;
                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                    &lt;/descriptorRefs&gt;
                    &lt;archive&gt;
                        &lt;manifest&gt;
                            &lt;mainClass&gt;com.gree.cn.write2mysql.Write2mysql&lt;/mainClass&gt;
                        &lt;/manifest&gt;
                    &lt;/archive&gt;
                &lt;/configuration&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;make-assembly&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;single&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;

        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;


</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/09/05/maven-打包相关记录/" data-id="ck3o5xx7500218wujz4969zq6"
         class="article-share-link">Share</a>
      
    </footer>

  </div>

  

  

</article>



      
        

<article id="post-flink-基础" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/08/30/flink-基础/">flink 基础</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/08/30/flink-基础/" class="article-date">
  <time datetime="2019-08-30T08:54:39.000Z" itemprop="datePublished">2019-08-30</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>

      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <h2 id="一、Apache-Flink"><a href="#一、Apache-Flink" class="headerlink" title="一、Apache Flink"></a>一、Apache Flink</h2><h3 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h3><pre><code>Apache Flink 是一个分布式大数据处理引擎，可以对有限流和无线流进行有状态和无状态进行计算，能够在各种集群环境上部署，对各种大小的数据集进行快速计算。
</code></pre><h3 id="flink-启动命令"><a href="#flink-启动命令" class="headerlink" title="flink 启动命令"></a>flink 启动命令</h3><pre><code>linux
./bin/start-cluster.sh
windows
.\start-cluster.bat
</code></pre><p><img src="/images/2019/08/30/063b0630-cb04-11e9-bf46-8baad6d0e743.png" alt="image.png"></p>
<h3 id="flink-maven-项目依赖"><a href="#flink-maven-项目依赖" class="headerlink" title="flink maven 项目依赖"></a>flink maven 项目依赖</h3><p>java版本：</p>
<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
  &lt;artifactId&gt;flink-java&lt;/artifactId&gt;
  &lt;version&gt;1.8.0&lt;/version&gt;
  &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
  &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;
  &lt;version&gt;1.8.0&lt;/version&gt;
  &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre><p>Scala版本：</p>
<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
  &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;
  &lt;version&gt;1.8.0&lt;/version&gt;
  &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
  &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;
  &lt;version&gt;1.8.0&lt;/version&gt;
  &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
</code></pre><p>flink 关联 kafka 依赖</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
    &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt;
    &lt;version&gt;1.8.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><p>flink 推荐使用shade插件打包maven项目</p>
<pre><code>&lt;build&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
            &lt;version&gt;3.0.0&lt;/version&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;phase&gt;package&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;shade&lt;/goal&gt;
                    &lt;/goals&gt;
                    &lt;configuration&gt;
                        &lt;artifactSet&gt;
                            &lt;excludes&gt;
                                &lt;exclude&gt;com.google.code.findbugs:jsr305&lt;/exclude&gt;
                                &lt;exclude&gt;org.slf4j:*&lt;/exclude&gt;
                                &lt;exclude&gt;log4j:*&lt;/exclude&gt;
                            &lt;/excludes&gt;
                        &lt;/artifactSet&gt;
                        &lt;filters&gt;
                            &lt;filter&gt;
                                &lt;!-- Do not copy the signatures in the META-INF folder.
                                Otherwise, this might cause SecurityExceptions when using the JAR. --&gt;
                                &lt;artifact&gt;*:*&lt;/artifact&gt;
                                &lt;excludes&gt;
                                    &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;
                                    &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;
                                    &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;
                                &lt;/excludes&gt;
                            &lt;/filter&gt;
                        &lt;/filters&gt;
                        &lt;transformers&gt;
                            &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;
                                &lt;mainClass&gt;my.programs.main.clazz&lt;/mainClass&gt;
                            &lt;/transformer&gt;
                        &lt;/transformers&gt;
                    &lt;/configuration&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;

</code></pre><h3 id="flink-基础-API-概念"><a href="#flink-基础-API-概念" class="headerlink" title="flink 基础 API 概念"></a>flink 基础 API 概念</h3><blockquote>
<p>DataSet and DataStream:<br>    Flink具有特殊类DataSet并DataStream在程序中表示数据。您可以将它们视为可以包含重复项的不可变数据集合。在DataSet数据有限的情况下，对于一个DataStream元素的数量可以是无界的。</p>
</blockquote>
<h3 id="flink-一般处理步骤"><a href="#flink-一般处理步骤" class="headerlink" title="flink 一般处理步骤"></a>flink 一般处理步骤</h3><ol>
<li>获得一个execution environment，</li>
<li>加载/创建初始数据，</li>
<li>指定此数据的转换，</li>
<li>指定放置计算结果的位置，</li>
<li>触发程序执行</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/30/flink-基础/" data-id="ck3o5xx6q001e8wujx9g7n87o"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/flink/">flink</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/流处理/">流处理</a></li></ul>

    </footer>

  </div>

  

  

</article>



      
        

<article id="post-spark-structed-streaming-读取kafka数据" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/08/27/spark-structed-streaming-读取kafka数据/">spark structed streaming 读取kafka数据</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/08/27/spark-structed-streaming-读取kafka数据/" class="article-date">
  <time datetime="2019-08-27T02:34:36.000Z" itemprop="datePublished">2019-08-27</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>structed streaming :</p>
<blockquote>
<p>Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine.</p>
</blockquote>
<p>Structed streaming 官方文档 <a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">2.4.3官方文档</a></p>
<h3 id="一、部分关于如何连接kafka数据，并做计算。"><a href="#一、部分关于如何连接kafka数据，并做计算。" class="headerlink" title="一、部分关于如何连接kafka数据，并做计算。"></a>一、部分关于如何连接kafka数据，并做计算。</h3><p>首先引入pom.xml</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.gree.cn&lt;/groupId&gt;
    &lt;artifactId&gt;structstreaminglocation&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;StructStreamingLocation&lt;/name&gt;
    &lt;description&gt;Demo project for StructStreamingLocation&lt;/description&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!-- spark 相关依赖 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- sparkStreaming kafka 整合--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- sparkStreaming kafka end --&gt;

        &lt;!-- struct streaming kafka --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- struct streaming kafka 结束--&gt;

        &lt;!-- spark 相关依赖 --&gt;

        &lt;!-- 数据库连接--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;version&gt;8.0.12&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
            &lt;artifactId&gt;druid&lt;/artifactId&gt;
            &lt;version&gt;1.1.12&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- 数据库连接 end --&gt;

        &lt;!-- Redis客户端 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;redis.clients&lt;/groupId&gt;
            &lt;artifactId&gt;jedis&lt;/artifactId&gt;
            &lt;version&gt;2.7.1&lt;/version&gt;
        &lt;/dependency&gt;


        &lt;!-- 常用工具 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;version&gt;1.18.2&lt;/version&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
        &lt;!-- 常用工具  end --&gt;

    &lt;/dependencies&gt;
    &lt;build&gt;
       &lt;plugins&gt;
           &lt;plugin&gt;
               &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
               &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
               &lt;version&gt;2.5.5&lt;/version&gt;
               &lt;configuration&gt;
                   &lt;archive&gt;
                       &lt;manifest&gt;
                           &lt;mainClass&gt;com.gree.cn.location.LocationMap&lt;/mainClass&gt;
                       &lt;/manifest&gt;
                   &lt;/archive&gt;
                   &lt;descriptorRefs&gt;
                       &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                   &lt;/descriptorRefs&gt;
               &lt;/configuration&gt;
               &lt;executions&gt;
                   &lt;execution&gt;
                       &lt;id&gt;make-assembly&lt;/id&gt;
                       &lt;phase&gt;package&lt;/phase&gt;
                       &lt;goals&gt;
                           &lt;goal&gt;single&lt;/goal&gt;
                       &lt;/goals&gt;
                   &lt;/execution&gt;
               &lt;/executions&gt;
           &lt;/plugin&gt;
           &lt;plugin&gt;
               &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
               &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
               &lt;version&gt;2.3.1&lt;/version&gt;
               &lt;configuration&gt;
                   &lt;source&gt;1.8&lt;/source&gt;
                   &lt;target&gt;1.8&lt;/target&gt;
               &lt;/configuration&gt;
           &lt;/plugin&gt;
       &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;


</code></pre><p>然后编写相应的代码逻辑：</p>
<pre><code>package com.gree.cn.location;

import com.gree.cn.entity.*;
import com.gree.cn.sink.BaseHeartSink;
import com.gree.cn.sink.LabelSink;
import com.gree.cn.sink.MaterialInfoSink;
import com.gree.cn.sink.MaterialRedisSink;
import com.gree.cn.utils.DruidPoolUtils;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.*;
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;

import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Properties;

import static org.apache.spark.sql.functions.*;

public class LocationMap {
    public static void main(String[] args) {
        //创建SparkSession
        SparkSession spark = SparkSession
                .builder()
                .appName(&quot;小车定位程序&quot;)
                //.master(&quot;local[2]&quot;)
                .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)
                .config(&quot;spark.local.dir&quot;, &quot;/lvm/data1/bluetooth_location/spark/location&quot;)
                .config(&quot;spark.ui.port&quot;, &quot;8083&quot;)
                .config(&quot;spark.sql.shuffle.partitions&quot;, &quot;50&quot;)
                .config(&quot;spark.default.parallelism&quot;, &quot;12&quot;)
                .getOrCreate();

        //读取kafka数据
        Dataset&lt;Row&gt; kafkaValue = spark.readStream()
                .format(&quot;kafka&quot;)
                .option(&quot;kafka.bootstrap.servers&quot;, &quot;10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092&quot;)
                .option(&quot;subscribe&quot;, &quot;topic0619&quot;)
                //.option(&quot;kafka.bootstrap.servers&quot;,&quot;172.16.247.129:9092&quot;)
                //.option(&quot;subscribe&quot;,&quot;test06&quot;)
                .option(&quot;startingOffsets&quot;, &quot;latest&quot;)
                .load()
                .selectExpr(&quot;CAST(value AS STRING)&quot;);




        //基站数据包括 接收时间，基站ID，信标ID，信号强度
        Dataset&lt;BaseTable&gt; baseTableDs = kafkaValue.as(Encoders.STRING()).map(
                (MapFunction&lt;String, BaseTable&gt;) values -&gt; {
                    BaseTable baseTable = new BaseTable();
            //具体逻辑
                      ...
            ...
            ...
                    return baseTable;
                } , ExpressionEncoder.javaBean(BaseTable.class));

        //获取信标电量表数据
        Dataset&lt;LabelData&gt; labelData = baseTableDs
                .filter(col(&quot;orderName&quot;).equalTo(&quot;66&quot;))
                .select(col(&quot;labelID&quot;), col(&quot;electricity&quot;),
                        col(&quot;baseID&quot;),col(&quot;recDatetime&quot;))
                .as(ExpressionEncoder.javaBean(LabelData.class));

        /*StreamingQuery labelDataStart =*/ labelData.repartition(2, col(&quot;labelID&quot;))
                .writeStream()
                .outputMode(&quot;append&quot;)
                .foreach(new LabelSink())
                .start();

        //labelData.writeStream().format(&quot;console&quot;).start();
}

</code></pre><h3 id="二、如何将数据sink到mysql"><a href="#二、如何将数据sink到mysql" class="headerlink" title="二、如何将数据sink到mysql"></a>二、如何将数据sink到mysql</h3><pre><code>package com.gree.cn.sink;


import com.gree.cn.entity.LabelData;
import com.gree.cn.utils.DruidPoolUtils;
import org.apache.spark.sql.ForeachWriter;

import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;


public class LabelSink extends ForeachWriter&lt;LabelData&gt; {
    /**
     * sink 信标数据到MySQL中
     *
     */
    private PreparedStatement statement;
    private Connection connection;


    public boolean open(long arg0, long arg1) {

        return true;
    }


    public void process(LabelData labelData) {
        try {
            connection = DruidPoolUtils.getConnection();

            String insertSql = &quot; REPLACE INTO bluetooth_location.beacon_info (beaconId,electricity,stationId,updateTime) VALUES(?,?,?,?) &quot;;

            statement = connection.prepareStatement(insertSql);
            statement.setString(1,labelData.getLabelID());
            statement.setInt(2,labelData.getElectricity());
            statement.setString(3,labelData.getBaseID());
            statement.setString(4,labelData.getRecDatetime());
            statement.executeUpdate();

        } catch (SQLException e) {
            e.printStackTrace();
        }finally {
            DruidPoolUtils.close(connection,statement);
        }
    }

    @Override
    public void close(Throwable arg0) {

    }
}


</code></pre><h3 id="三、数据库连接池配置"><a href="#三、数据库连接池配置" class="headerlink" title="三、数据库连接池配置"></a>三、数据库连接池配置</h3><pre><code>package com.gree.cn.utils;

import com.alibaba.druid.pool.DruidDataSourceFactory;

import javax.sql.DataSource;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.Properties;

/**
 * 数据库连接池工具类
 */

public class DruidPoolUtils {

    //创建成员变量
    private static DataSource dataSource;
    //加载配置文件
    static {
        try {
            Properties properties = new Properties();
            //加载类路径
            properties.load(DruidPoolUtils.class.getResourceAsStream(&quot;/druid.properties&quot;));

            //读取属性文件，创建连接池
            dataSource = DruidDataSourceFactory.createDataSource(properties);
        }catch (Exception e){
            e.printStackTrace();
        }
    }

    //获取数据源
    public static DataSource getDataSource(){
        return dataSource;
    }

    //获取连接对象
    public static Connection getConnection(){
        try {
            return dataSource.getConnection();
        }catch (SQLException e){
            throw new RuntimeException(e);
        }
    }

    //释放资源

    public static void close(Connection connection, Statement statement, ResultSet resultSet){
        if (resultSet != null ){
            try {
                resultSet.close();
            }catch (SQLException e){
                e.printStackTrace();
            }
        }
        if (statement != null){
            try {
                statement.close();
            }catch (SQLException e){
                e.printStackTrace();
            }
        }
        if (connection != null){
            try {
                connection.close();
            }catch (SQLException e){
                e.printStackTrace();
            }
        }
    }

    public static void close(Connection connection,Statement statement){
        close(connection,statement,null);
    }
}

</code></pre><p>druid.properties</p>
<blockquote>
<p>driverClassName=com.mysql.cj.jdbc.Driver<br>url=jdbc:mysql://localhost:3306?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false<br>username=root<br>password=lyxbdw<br>initialSize=5<br>maxActive=20<br>maxWait=3000<br>minIdle=3</p>
</blockquote>
<h2 id="Scala编写structed-stremaing"><a href="#Scala编写structed-stremaing" class="headerlink" title="Scala编写structed stremaing"></a>Scala编写structed stremaing</h2><h3 id="引入pom-xml"><a href="#引入pom-xml" class="headerlink" title="引入pom.xml"></a>引入pom.xml</h3><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.gree.cn&lt;/groupId&gt;
    &lt;artifactId&gt;write2mysql&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;write2mysql&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!-- 常用插件 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;version&gt;1.18.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- 常用插件 结束 --&gt;

        &lt;!-- spark 相关依赖 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- spark streaming kafka --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- spark streaming kafka 结束--&gt;
        &lt;!-- struct streaming kafka --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- struct streaming kafka 结束--&gt;
        &lt;!-- spark 相关依赖 结束--&gt;

        &lt;!-- 数据库连接--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;version&gt;8.0.15&lt;/version&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
            &lt;artifactId&gt;druid&lt;/artifactId&gt;
            &lt;version&gt;1.1.12&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- 数据库连接 end --&gt;

        &lt;!-- log 日志--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j&lt;/artifactId&gt;
            &lt;version&gt;1.2.17&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- log 日志结束--&gt;

    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;
                &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.15.2&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;scala-compile-first&lt;/id&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;compile&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;includes&gt;
                                &lt;include&gt;**/*.scala&lt;/include&gt;
                            &lt;/includes&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                    &lt;execution&gt;
                        &lt;id&gt;scala-test-compile&lt;/id&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;testCompile&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;
                    &lt;descriptorRefs&gt;
                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                    &lt;/descriptorRefs&gt;
                    &lt;archive&gt;
                        &lt;manifest&gt;
                            &lt;mainClass&gt;com.gree.cn.write2mysql.Write2mysql&lt;/mainClass&gt;
                        &lt;/manifest&gt;
                    &lt;/archive&gt;
                &lt;/configuration&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;make-assembly&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;single&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;

        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
</code></pre><p>与java的差别主要是打包插件不同，编写相应逻辑</p>
<pre><code>package com.gree.cn.write2mysql


import com.gree.cn.sink.MysqlSink
import org.apache.spark.sql.SparkSession

/**
  * 原始数据保存
  * @author sfz
  * Time 2019-08-08
  */
object Write2mysql {
  def main(args: Array[String]): Unit = {

    //创建SparkSession
    val spark = SparkSession
      .builder()
      .appName(&quot;write2mysql&quot;)
      .master(&quot;local[*]&quot;)
      .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)
      .config(&quot;spark.local.dir&quot;, &quot;/lvm/data1/bluetooth_location/spark/write2mysql&quot;)
      .config(&quot;spark.ui.port&quot;, &quot;8084&quot;)
      .getOrCreate()

    //引入隐式转换，保证String可以序列化
    import spark.implicits._

    //读取kafka数据
    val kafkaValue = spark.readStream
      .format(&quot;kafka&quot;)
      //.option(&quot;kafka.bootstrap.servers&quot;, &quot;10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092&quot;)
     // .option(&quot;subscribe&quot;, &quot;topic0619&quot;)
      .option(&quot;kafka.bootstrap.servers&quot;,&quot;localhost:9092&quot;)
      .option(&quot;subscribe&quot;,&quot;test06&quot;)
      .option(&quot;startingOffsets&quot;, &quot;latest&quot;)
      .load()
      .selectExpr(&quot;CAST(value AS STRING)&quot;)

    //将kafka数据转化为BaseTable结构
    val baseData = kafkaValue.as[String].map(values =&gt; {
//具体的解析数据逻辑
。。。
。。。
。。。
        BaseTable.apply(frameHeader, equipment, frameLength, orderName, baseID, labelID, signalPower, electricity, datetime, checkName, recDatetime)
      }
      else {
        BaseTable.apply(frameHeader, equipment, frameLength, orderName, baseID77, labelID, signalPower, electricity, datetime, checkName, recDatetime)
      }
    }).as[BaseTable]

    //将数据sink到mysql数据库中
    val query =  baseData.writeStream
      .foreach(new MysqlSink)
      .outputMode(&quot;append&quot;)
      .start()

    query.awaitTermination()
  }


  //创建BaseTable结构
  case class BaseTable(frameHeader: String, equipment: String, frameLength: String, orderName: String, baseId: String,
                       labelId: String, signalPower: Integer, electricity: Integer, datetime: String, checkName: String, recDatetime: String)

}


</code></pre><p>接着sink到mysql 数据库中</p>
<pre><code>package com.gree.cn.sink

import java.sql.{Connection, PreparedStatement, SQLException}

import com.gree.cn.utils.{DateUtils, MysqlPoolUtils}
import com.gree.cn.write2mysql.Write2mysql.BaseTable
import org.apache.spark.sql.ForeachWriter

class MysqlSink() extends ForeachWriter[BaseTable](){

  var conn:Connection = _
  var statement : PreparedStatement = _
  override def open(partitionId: Long, epochId: Long): Boolean = {
     conn = MysqlPoolUtils.getConnection.get
     conn.setAutoCommit(false)
     true
  }

  override def process(baseTable: BaseTable): Unit = {
    val insertSql = &quot; &quot;

    statement = conn.prepareStatement(insertSql)
    statement.setString(1, baseTable.frameHeader)
 。。。
。。。
    statement.setString(11, baseTable.recDatetime)
    statement.executeUpdate()
  }

  override def close(errorOrNull: Throwable): Unit = {
    try{
      if ( statement != null &amp;&amp; conn != null) {
        statement.close()
        conn.close()
      }
    } catch {
      case e: SQLException =&gt;
        e.printStackTrace()
    }
  }
}



</code></pre><p>最后配上数据库连接池</p>
<pre><code>package com.gree.cn.utils

import java.sql.Connection
import java.util.Properties

import com.alibaba.druid.pool.DruidDataSourceFactory
import javax.sql.DataSource
import org.apache.log4j.Logger

/**
  * @author sfz
  * MySql 数据库连接池
  */
object MysqlPoolUtils {

private val log = Logger.getLogger(MysqlPoolUtils.getClass.getName)

val dataSource:Option[DataSource] = {
  try {
    val druidProps = new Properties()
    //获取Druid连接池配置文件
    val druidConfig = getClass.getResourceAsStream(&quot;/application.properties&quot;)
    //加载配置文件
    druidProps.load(druidConfig)
    Some(DruidDataSourceFactory.createDataSource(druidProps))
  }catch {
    case error :Exception =&gt;
      log.error(&quot;Error Create Mysql Connection &quot;,error)
      None
  }
}
  //获取连接
  def getConnection:Option[Connection] = {
    dataSource match {
      case Some(ds) =&gt; Some(ds.getConnection)
      case None =&gt; None
    }
  }
}


</code></pre><p>application.properties</p>
<blockquote>
<p>driver-class-name = com.mysql.cj.jdbc.Driver<br>url=jdbc:mysql://localhost:3306/base_data?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false<br>username=root<br>password=lyxbdw<br>filters=stat,config<br>initialSize=2<br>maxActive=20<br>minIdle=2<br>maxWait=60000<br>timeBetweenEvictionRunsMillis=60000<br>minEvictableIdleTimeMillis=300000<br>validationQuery=SELECT 1<br>testWhileIdle=true<br>testOnBorrow=false<br>testOnReturn=false<br>removeAbandoned=true<br>logAbandoned=true<br>poolPreparedStatements=false<br>removeAbandonedTimeout=1800<br>maxOpenPreparedStatements=100</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/27/spark-structed-streaming-读取kafka数据/" data-id="ck3o5xx7x002x8wujzxrr18dj"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>

  </div>

  

  

</article>



      
        

<article id="post-蓝牙信标定位项目总结" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 itemprop="name">
      <a class="article-title" href="/2019/08/26/蓝牙信标定位项目总结/">蓝牙信标定位项目总结</a>
    </h2>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/08/26/蓝牙信标定位项目总结/" class="article-date">
  <time datetime="2019-08-26T07:54:34.000Z" itemprop="datePublished">2019-08-26</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/蓝牙定位/">蓝牙定位</a>
  </div>

      </div>
    

    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <h2 id="一、项目简介："><a href="#一、项目简介：" class="headerlink" title="一、项目简介："></a>一、项目简介：</h2><blockquote>
<p>  蓝牙信标定位项目又称物流容器信息化管理系统。主要是为了解决缩短物理配送周期，提高物流配送效率问题，<br>采用蓝牙定位技术,实时定位小车的位置以及监控小车的状态信息，达到及时优化和调整配送流程的效果。<br>以下是蓝牙基站数据处理时序图：</p>
</blockquote>
<p><img src="/images/2019/08/26/d389bdf0-c7d2-11e9-82cd-b1912fd86647.png" alt="image.png"></p>
<p>数据处理流程：</p>
<ol>
<li>作为服务端，需要接受来自客户端tcp/ip数据。(netty)</li>
<li>数据接收并校验成功后作出相应的动作，一部分数据需要返回时间戳，所有校验成功的数据都写入kafka。(kafka producer)</li>
<li>消费kafka中的数据，主要有三个部分，一是信标电量信息采集，二是基站状态信息的采集，三是小车位置的计算和物料信息的绑定（structured streaming/flink）</li>
<li>读取redis小车位置以及状态信息，通过websocket每隔五秒准实时推送一次最新数据到前端。(websocket)</li>
</ol>
<p><strong>说明</strong>：<br>1.使用netty作为数据接入使用可以参考：<a href="https://sfzsjx.github.io/2019/05/22/netty-%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/" target="_blank" rel="noopener">netty整合spring boot</a><br>2.数据接收后需要写入kafka集群中，关于kafka集群部署可以参考：<a href="https://sfzsjx.github.io/2019/04/30/kafka-%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85-%E9%83%A8%E7%BD%B2/" target="_blank" rel="noopener">kafka集群安装部署</a><br>3.kafka安装后，数据写入kafka,需要创建kafka producer，可以参考：<a href="https://sfzsjx.github.io/2019/04/30/kafka%E7%9B%B8%E5%85%B3%E7%9A%84%E5%B8%B8%E8%A7%84%E6%93%8D%E4%BD%9C/" target="_blank" rel="noopener">kafka相关操作</a><br>4.读取kafka数据，并做相应的计算，需要用到structed streaming/flink技术，相应的需要部署spark 集群或flink集群，spark集群部署可以参考：<a href="https://sfzsjx.github.io/2019/08/12/Spark-Windows10-%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/" target="_blank" rel="noopener">spark 集群安装部署</a><br>5.structed streaming 读取kafka 数据并做实时计算可以参考 ：<a href="https://sfzsjx.github.io/2019/08/27/spark-structed-streaming-%E8%AF%BB%E5%8F%96kafka%E6%95%B0%E6%8D%AE/" target="_blank" rel="noopener">structed streaming 读取kafka 数据</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/26/蓝牙信标定位项目总结/" data-id="ck3o5xxap007r8wujmbbtyrm0"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/蓝牙定位/">蓝牙定位</a></li></ul>

    </footer>

  </div>

  

  

</article>



      
  </article>
  

  
    <nav class="page-nav">
      
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">Next page</a>
    </nav>
  
</section>
</div>

  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2019 上善若水博客</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>

  <aside class="sidebar">
    
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/hexo.svg" alt="上善若水博客"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">Archives</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">Gallery</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">About</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/lazyload.min.js"></script>
<script src="/js/busuanzi-2.3.pure.min.js"></script>

  <script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/ocean.js"></script>

</body>
</html>