{"pages":[{"title":"关于","text":"","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"02 数据分析实战笔记--学习数据挖掘的最佳路径","text":"数据挖掘的基本流程（六大步骤）：1、商业理解：先从商业的角度理解项目的需求，然后再对数据挖掘的目标进行定义。2、数据理解：尝试收集部分数据，然后对数据进行探索，包括数据描述、数据质量验证3、数据准备：收集数据，并对数据进行清洗、数据集成等操作4、模型建立：选择和应用各种数据挖掘模型，并进行优化5、模型评估：对模型进行评价，并检查构建模型的每个步骤，确认模型是否实现预定的商业目标6、上线发布：项目只有落地实施才能体现价值。当然后序还需要一定的日常运维。 数据挖掘的十大算法按照不同的目的，算法主要分为四类：|分类算法：C4.5,朴素贝叶斯（Naive Bayes）,SVM,KNN,Adaboost,CART|聚类算法：K-Means,EM|关联分析：Apriori|连接分析：PageRank 数据挖掘的数学原理 1、概率论与数理统计2、线性代数3、图论4、最优化方法","link":"/2019/02/18/02-数据分析实战笔记-学习数据挖掘的最佳路径/"},{"title":"03 数据分析实战45 ： Python 基础语法","text":"Python 基础语法Python 语言特点 Python 语言最大的优点就是简洁，同时有大量的第三方库，功能强大，能够解决数据分析的大部分问题。比如科学计算工具NumPy 和 Pandas库，深度学习工具 Keras 和 TensorFlow,以及机器学习工具 Scikit-learn。 安装及 IDE 环境安装Python 3.x 官网下载安装。Python IDE 推荐使用 PythonCharm Python 基础语法输入输出123sum = 100+100print (&apos;hello,%s&apos; %name) # 打印输出print (&apos;sum = %d&apos; %sum) 判断语句 if … else …score>123456 print (&apos;Excellent&apos;)else: if score &lt; 60: print( &apos;Fail&apos;) else: print (&apos;Good Job&apos;) 循环语句 ： for … in123for number in range(11): sum = sum + numberprint (sum) 循环语句：while12345number = 1while number &lt; 11: sum = sum + number number = number + 1print （sum） 数据类型：列表、元组、字典、集合列表：[]123456lists.append(&apos;d&apos;)print (lists)print (len(lists))lists.insert(0,&apos;mm&apos;)lists.pop()print (lists) 元组（tuple）1print (tuples[0]) 字典（dictionary）-*- coding: utf-8 -*123456789101112# 定义一个 dictionaryscore = {&apos;guanyu&apos;:95,&apos;zhangfei&apos;:96}# 添加一个元素score[&apos;zhaoyun&apos;] = 98print (score)# 删除一个元素score.pop(&apos;zhangfei&apos;)# 查看 key 是否存在print (&apos;guanyu&apos; in score)# 查看一个 key 对应的值print (score.get(&apos;guanyu&apos;))print (score.get(&apos;yase&apos;,99)) 集合1234s.add(&apos;d&apos;)s.remove(&apos;b&apos;)print （s）print （&apos;c&apos; in s） 极客时间版权所有: https://time.geekbang.org/column/article/73574 极客时间版权所有: https://time.geekbang.org/column/article/73574","link":"/2019/03/01/03-数据分析实战45-：-Python-基础语法/"},{"title":"17-数据分析实战笔记： 决策树","text":"决策树决策树的工作原理 在现实生活，我们做的各种决策，都是基于以往的经验来判断的。如果将背后的逻辑整理成一个结构图，这实际上就是决策树。上图就是一个典型的决策树。而在实现决策树时会经历两个阶段：构造和剪枝。 决策树的构造优点：计算复杂度不高，输出结果易于理解，对中间值的确失不敏感，可以处理不相干特征数据缺点：易发生过拟合适用数据类型：数值型和标称型创建分支的伪代码函数createBranch()如下所示：检测数据集中的每个子项是否属于同一类： 12345678If so return 类标签：Else 寻找划分数据集的最好特征 划分数据集 创建分支节点 for 每个划分的子集 调用函数createBranch并增加返回结果到分支节点中 return 分支节点 构造 构造的过程就是选择什么样的属性作为节点的过程，一般在构造中存在三种类型的节点：1.根节点：树的顶端，最开始的那个节点2.内部节点：树的中间节点3.叶节点：树最底部的节点，也是决策树结果 构造过程中，需要解决的三个重要问题：1.选择哪个属性作为根节点；2.选择哪些属性作为子节点；3.什么时候停止并得到目标状态，即叶子节点。 决策树的一般流程 （1）收集数据：可以使用任何方法（2）准备数据：树构造算法只适应于标称型数据，因此数值型数据必须离散化（3）分析数据：可以适应任何方法，构造树完成之后，我们应该检查图形是否符合预期。（4）训练算法：构造树的数据结构（5）测试算法：使用经验树计算错误率。（6）使用算法：此步骤可以适应于任何监督学习算法，而使用决策树可以更好的理解数据的含义 信息增益划分数据集的最大原则：将无序的数据变得更加有序。 在划分数据集前后的信息变化称之为信息增益，而我们可以计算每个特征值划分收获的信息增益，获得信息增益最高 的特征就是最好的选择。 熵定义为信息的期望值。如果待分类的事务可能划分在多个分类之中，则xi的信息定义为： 为了计算熵，需要计算所有类别可能包含的信息期望值： 程序清单1 计算给定数据集的香农熵12345678910111213141516from math import logdef calcShannonEnt(dataSet): numEntries = len(dataSet) labelCounts = {} #为所有可能的分类创建字典 for featVec in dataSet: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 # 计算信息期望 for key in labelCounts: prob = float(labelCounts[key])/numEntries shannonEnt -= prob*log(prob,2) return shannonEnt 程序清单2 按照给定的特征划分数据集12345678def splitDataSet(dataSet,axis,value): retDataSet = [] for featVec in dataSet: if featVec[axis] == value： reduceFeatVec = featVec[:axis] reduceFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSet 剪枝 剪枝就是给决策树瘦身。也即不需要过多的判断也能得到不错的结果。主要是为了防止“过拟合（Overfitting）”现象的发生。过拟合现象会导致得到的模型虽然训练结果好，但是泛化能力差。剪枝一般分为两种：“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）","link":"/2019/02/23/17-数据分析实战笔记：-决策树/"},{"title":"Apache NiFi 简单介绍和使用","text":"一、什么是Apache NiFi简单来讲，NIFI就是为了构建系统之间的数据自动化传输的简易操作工具。提供了一些可靠的数据流的传输工具，解决了大部分现代企业中的数据ETL中所遇到的挑战。 二、NIFI 核心概念 三、NIFI 架构 四、 NIFI 使用界面介绍 主要包括工具栏、状态栏、总菜单栏，操作缩图和画布。平常用到的组件主要有Processor:The Processor is the NiFi component that is used to listen for incoming data; pull data from external sources; publish data to external sources; and route, transform, or extract information from FlowFiles. Processor Group:When a dataflow becomes complex, it often is beneficial to reason about the dataflow at a higher, more abstract level. NiFi allows multiple components, such as Processors, to be grouped together into a Process Group. The NiFi User Interface then makes it easy for a DFM to connect together multiple Process Groups into a logical dataflow, as well as allowing the DFM to enter a Process Group in order to see and manipulate the components within the Process Group. 我认为 Processor 就是一个数据处理器，它不仅可以从各种数据源中加载数据，转成Fiedflow基本nifi元素，还能够将数据处理成各种常见的格式以及将数据输出到各种数据库中而Processor Group 则是一个装载 Processor的容器。 五、案例（JSON_TO_MYSQL） 创建一个Processor Group ,将其拖到画布当中 然后修改名字 读取json文件，GetFile –&gt; 将JSON 转为sql语句，ConvertJSONToSQL –&gt; 运行sql语句，PutSQL GetFile 设置输入目录Input Directory，目标文件 File Filter ConvertJSONToSQL 设置数据库连接池JDBC Connection Pool ,SQL 类型 Statement Type ,表名 Table Name,其他默认 ，注意 ConvertJSONToSQL 只适用一个json值的文件 PutSQL 设置 数据库连接池，SQL Statement ，其他默认。一般来说只需要设置加黑的参数，但这里需要设置sql语句。","link":"/2019/09/11/Apache-NiFi-简单介绍和使用/"},{"title":"18| 决策树（中） : CART算法笔记","text":"决策树 基于信息度量的不同方式，我们把决策树分为ID3算法、C4.5算法和CART算法。CART算法（Classification And Regression Tree）,又称分类回归树。也就是说CART决策树既可以作为分类树，又可以作为回归树。而且CART只支持二叉树。分类树和回归树的区别分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本类别，而回归树可以对连续型的数值进行预测也就是数据在某个区间都有取值的可能，它输出的是一个数值。 CART分类树工作流程在属性选择上CART算法采用基尼系数作为衡量指标。 假设t为节点，那么该节点是GINI系数为：其中p(Ck|t)表示节点t属于类别Ck的概率，节点t的基尼系数为1减去个类别Ck概率平方和。在CART算法中，基于基尼系数对特征属性进行二元分裂，假设属性A将节点D划分为D1和D2，如下图所示：那么节点D的基尼系数等于子节点D1和D2的归一化基尼系数之和，用公式表示为：归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父节点D中的比例。节点D被属性A划分后基尼系数越大，样本集合的不确定性越大，也就是不纯度越高。 如何使用CART算法来创建分类树123456789101112131415161718192021# encoding=utf-8from sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.datasets import load_iris# 准备数据集iris=load_iris()# 获取特征集和分类标识features = iris.datalabels = iris.target# 随机抽取 33% 的数据作为测试集，其余为训练集train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)# 创建 CART 分类树clf = DecisionTreeClassifier(criterion=&apos;gini&apos;)# 拟合构造 CART 分类树clf = clf.fit(train_features, train_labels)# 用 CART 分类树做预测test_predict = clf.predict(test_features)# 预测结果与测试集结果作比对score = accuracy_score(test_labels, test_predict)print(&quot;CART 分类树准确率 %.4lf&quot; % score) CART回归树工作流程 在CART回归树中，通过样本的混乱程度，也就是样本的离散程度来评价“不纯度”。设x为样本的个体，均值为u。可以通过取差值的绝对值或者方差来评价。差值绝对值：方差：以上两种节点划分标准，分别对应着两种目标函数最优化的标准。最小绝对偏差（LAD）和最小二乘偏差（LSD）。 CART回归树预测例子：波士顿房价预测123456789101112131415161718192021222324# encoding=utf-8from sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_bostonfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_errorfrom sklearn.tree import DecisionTreeRegressor# 准备数据集boston=load_boston()# 探索数据print(boston.feature_names)# 获取特征集和房价features = boston.dataprices = boston.target# 随机抽取 33% 的数据作为测试集，其余为训练集train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)# 创建 CART 回归树dtr=DecisionTreeRegressor()# 拟合构造 CART 回归树dtr.fit(train_features, train_price)# 预测测试集中的房价predict_price = dtr.predict(test_features)# 测试集的结果评价print(&apos;回归树二乘偏差均值:&apos;, mean_squared_error(test_price, predict_price))print(&apos;回归树绝对值偏差均值:&apos;, mean_absolute_error(test_price, predict_price)) CART决策树的剪枝CART决策树的剪枝主要采用的是CCP（cost-complexity prune）方法,又称代价复杂度。这种剪枝方式采用了节点的表面误差率增益值作为评估：其中Tt代表以t为根节点的子树，C(Tt)表示节点t的子树没被裁剪时子树Tt的误差，C(t)表示节点t的子树被裁剪时节点t的误差，|Tt|代表子树Tt的叶子树，剪枝后，T的叶子数减少了|Tt|-1。所以节点的表面误差率增益值等于节点t的子树被剪枝后的误差变化除以减掉的叶子数量。","link":"/2019/03/07/18-决策树（中）-CART算法笔记/"},{"title":"Dataframe 常见问题记录","text":"问题1、不能够解析某个字段 解决办法：查看表名是否正确，数据库是否正确，是否存在数据库切换的情况。","link":"/2019/06/17/Dataframe-常见问题记录/"},{"title":"CDH 5.15 简易版离线安装完整版","text":"CDH 简易版离线安装 一、虚拟机搭建 准备一台32G内存的电脑，安装虚拟机VMware-workstation。虚拟机下载地址：http://download3.vmware.com/software/wkst/file/VMware-Workstation-Full-14.1.2-8497320.x86_64.bundle。根据自己的电脑系统下载不同的版本，我下载的是VMware-Workstation-Full-14.1.2-8497320.x86_64.bundle。安装完虚拟机后，下载操作系统镜像CentOS-7-x86_64-DVD-1804.iso（这是我选择的版本，你们可以选择不同的版本），创建一个新的虚拟机，至于虚拟机如何创建请自行解决。 经过上面的一系列的操作，目前拥有三台虚拟机 master 内存 16G 磁盘 150G slave1 内存 6G 磁盘 150G slave2 内存 6G 磁盘 150G 二、虚拟机配置 1.修改所有的主机名，这样便于管理。 hostnamectl set-hostname masterhostnamectl set-hostname slave1hostnamectl set-hostname slave22.配置静态IP 首先，选择NAT网络连接模式 然后，点击Edit编辑虚拟机网络设置，进入VMware network edit ,选中vmnet8 ,将Use local DHCP service to distribute IP addresses to VMs 前面的勾去掉。 接着，进入 /etc/sysconfig/network-scripts中查看现有的配置文件然后修改其中的配置文件，其中有个类似ifcfg-enth0的文件是你的网络名字 TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=$’\\751\\605\\615\\747\\675\\656 1’UUID=2bfdf6df-9fd6-44e3-ade7-5a397cf8d2e4ONBOOT=yesIPADDR=172.16.247.135GATEWAY=172.16.247.2NETMASK=255.255.255.0PREFIX=24 上面主要修改红色字体部分，其中BOOTPROTO=static 表示静态，IPADDR=172.16.247.135 表示静态IP地址 最后，保存退出，执行 重启网络service network restart查看IPifconfigping网络ping www.baidu.com3.编辑hosts 文件 添加ip地址 vi /etc/hosts 添加以下配置，你对应的三台机器的IP地址和对应的主机名 172.16.247.135 master172.16.247.132 slave1172.16.247.136 slave2 然后将这个文件分别拷贝到各个节点上 scp /etc/hosts root@slave1:/etc/hostsscp /etc/hosts root@slave2:/etc/hosts 4.配置SSH免密登陆 主要分为两个步骤：首先在所有的节点生成公钥 ssh-keygen -t rsa然后将所有的节点执行拷贝公钥 ssh-copy-id root@masterssh-copy-id root@slave1ssh-copy-id root@slave2当然也可以公钥添加到认证文件中，并设置authorized_keys的访问权限：https://blog.csdn.net/johnzhc/article/details/81119030 5.关闭selinux和防火墙 vi /etc/selinux/config SELINUX=disabled [hadoop@master network-scripts]$ cat /etc/selinux/config This file controls the state of SELinux on the system.SELINUX= can take one of these three values:enforcing - SELinux security policy is enforced.permissive - SELinux prints warnings instead of enforcing.disabled - No SELinux policy is loaded.SELINUX=disabled SELINUXTYPE= can take one of three two values:targeted - Targeted processes are protected,minimum - Modification of targeted policy. Only selected processes are protected.mls - Multi Level Security protection.SELINUXTYPE=targeted 关闭防火墙和查看防火墙状态： systemctl stop firewalldsystemctl disable firewalldsystemctl status firewalld6.安装NTP时间同步 yum install -y ntp #安装ntp服务（所有节点） vi /etc/ntp.conf #编辑ntp服务的配置文件（所有节点） 主节点master的ntp.conf修改红色部分，蓝色要注释掉 Note: Monitoring will not be disabled with the limited restriction flag.#disable monitorrestrict default nomodifyrestrict default nomodify notrapserver 127.127.1.0fudge 127.127.1.0 stratum 10includefile /etc/ntp/crypto/pwkeys /etc/ntp/keys 中国这边最活跃的时间服务器 : http://www.pool.ntp.org/zone/cnserver 0.cn.pool.ntp.orgserver 0.asia.pool.ntp.orgserver 3.asia.pool.ntp.org allow update time by the upper server允许上层时间服务器主动修改本机时间restrict 0.cn.pool.ntp.org nomodify notrap noqueryrestrict 0.asia.pool.ntp.org nomodify notrap noqueryrestrict 3.asia.pool.ntp.org nomodify notrap noquery Undisciplined Local Clock. This is a fake driver intended for backupand when no outside source of synchronized time is available.外部时间服务器不可用时，以本地时间作为时间服务 从节点slave的ntp.conf修改红色部分，紫色要注释掉 with symmetric key cryptography.keys /etc/ntp/keys Specify the key identifiers which are trusted.#trustedkey 4 8 42 Specify the key identifier to use with the ntpdc utility.#requestkey 8 Specify the key identifier to use with the ntpq utility.#controlkey 8 Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats Disable the monitoring facility to prevent amplification attacks using ntpdcmonlist command when default restrict does not include the noquery flag. SeeCVE-2013-5211 for more details.Note: Monitoring will not be disabled with the limited restriction flag.#disable monitorserver master prefer #master 是指你的主机名restrict default nomodify notrap nopeer noqueryrestrict -6 default kod nomodify notrap nopeer noquery 设置开机启动ntp服务 #关闭chronyd服务systemctl disable chronyd.service #开机自启动 systemctl enable ntpd.service 以上都配置完成后，执行 systemctl start ntpd #开启ntp服务 ntpstat #查看ntp运行状态 synchronised to NTP server (172.16.247.135) at stratum 3 time correct to within 350 ms polling server every 1024 s #出现这个表示成功同步 7.卸载Centos 系统自带的JDK rpm -qa | grep jdk #查看系统自带的jdk yum -y remove xxjdk #删除所有的jdk 8.CM 和CDH下载以及JDK和java驱动 Cloudera Manager下载地址：http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.15.0_x86_64.tar.gz CDH安装包地址：http://archive.cloudera.com/cdh5/parcels/latest/，由于我们的操作系统为CentOS7.2，需要下载以下文件： CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha1 manifest.json JDK 可以去官网下载 下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 下载版本为： jdk-8u172-linux-x64.rpm mysql的java驱动 下载地址：https://dev.mysql.com/downloads/connector/j/5.1.html 下载版本为：mysql-connector-java-5.1.46.tar.gz 9.安装CM和jdk以及mysql驱动 将下载好的安装包分发到各个节点上，并解压缩。 1.将和cloudera-manager-daemons-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm以及cloudera-manager-server-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm三个安装包传入管理节点（master节点）/tmp 目录下，当然其他目录也可以。但是tmp目录可以使得解压后的rpm包重启后删除不占内存。 2.将cloudera-manager-agent-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm和cloudera-manager-daemons-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm两个安装包传入所有从节点上（slave1和slave2节点）的/tmp目录下 3.将jdk-8u172-linux-x64.rpm安装包传入所有节点上/tmp目录,复制语句类似下面： scp cloudera-manager-agent-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm root@master:/tmp4.然后解压所有对应的安装包（所有节点） yum localinstall *.rpm5.配置JAVA_HOME变量（所有节点） echo “JAVA_HOME=/usr/java/latest/“ &gt;&gt; /etc/environment6.安装mysql驱动程序 将mysql-connector-java-5.1.46.tar.gz解压mysql-connector-java-5.1.46后将解压后包中的mysql-connector-java-5.1.46-bin.jar重命名为mysql-connector-java.jar传入 /usr/share/java目录里面。 tar mysql-connector-java-5.1.46.tar.gzsudo mkdir -p /usr/share/javacd mysql-connector-java-5.1.46sudo cp mysql-connector-java-5.1.46-bin.jar /usr/share/java/mysql-connector-java.jar 10.数据库安装 1.在master节点安装MariaDB(Mysql)这里安装MariDB,若要安装mysql，可参考https://blog.csdn.net/johnzhc/article/details/81119030。 sudo yum install mariadb-server #安装maridbsudo systemctl enable mariadb #设置开机启动sudo systemctl start mariadb #启动mariadbsudo /usr/bin/mysql_secure_installation #配置mariadb 可参考文档https://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/CDH5-Installation-Guide.html 2.为CDH创建数据库和用户 mysql -u root -p输入密码登陆mysql ，然后创建多个数据库，并完成授权。 CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON scm.* TO ‘scm‘@’%’ IDENTIFIED BY ‘scm’; CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON amon.* TO ‘amon‘@’%’ IDENTIFIED BY ‘amon’; CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON rman.* TO ‘rman‘@’%’ IDENTIFIED BY ‘rman’; CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON hue.* TO ‘hue‘@’%’ IDENTIFIED BY ‘hue’; CREATE DATABASE hive DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON hive.* TO ‘hive‘@’%’ IDENTIFIED BY ‘hive’; CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON sentry.* TO ‘sentry‘@’%’ IDENTIFIED BY ‘sentry’; CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON oozie.* TO ‘oozie‘@’%’ IDENTIFIED BY ‘oozie’;最后，退出数据库进行数据库的初始化，执行语句类似下面。 exit #退出数据库/user/share/cmf/schema/scm_prepare_database.sh (databaseType) (databaseName) (databaseuser) (databasepassword)例如scm数据库： /usr/share/cmf/schema/scm_prepare_database.sh mysql scm scm scm 11.安装CDH 1.在master节点创建parcel-repo仓库 mkdir -p /opt/cloudera/parcel-repochown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo2.将CDH安装包复制到/opt/cloudera/parcel-repo 目录下。 CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha1 manifest.json 然后将CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha1 重命名CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha 3.修改slave1和slave2的/etc/cloudera-scm-agent/config.ini 将server_host改为管理节点的网络名本例为master 4.分别启动cloudera-scm-server和cloudera-scm-agent 在主节点启动agent和server执行以下命令 systemctl start cloudera-scm-agent systemctl start cloudera-scm-server在slave1和slave2执行以下代码 systemctl start cloudera-scm-agent5.进入http://master:7180,默认的用户名和密码均为admin开始添加集群，下面是一种添加服务的顺序。 hdfs-&gt; yarn-&gt; hive-&gt; impala-&gt; zookeeper-&gt; hbase-&gt; oozie-&gt; hue-&gt;sqoop-&gt;kafka-&gt;spark 12.总结 安装过程主要遇到的坑： 1.静态IP的配置 2.时间同步ntp服务 3.安装服务的对应文件夹的权限问题。主要就是查看日志更改对应文件的权限： chmod 777 xxxchown root XXX4.kafka安装 在安装界面点击主机 点击parcel 点击KAFKA分配，并激活 然后添加kafka服务，并再配置的设置如下的参数： kafak mirrormaker： Destination Broker list slave1:9092 source list slave1:9092 topical whitelist slave1:9092 kafak Broker Advertiesd Host slave1 java heap size of broker 256","link":"/2019/01/14/CDH-5-15-简易版离线安装完整版/"},{"title":"Docker 三剑客之 Docker Compose","text":"概述12345678Docker Compose 定位是 定义和运行多个docker容器应用（Defining and running multi-container Docker applications）它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）Compose 中有两个重要的概念：服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 安装与卸载1234561.安装$ sudo curl -L https://github.com/docker/compose/releases/download/1.17.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose2.卸载sudo rm /usr/local/bin/docker-compose","link":"/2019/10/25/Docker-三剑客之-Docker-Compose/"},{"title":"Docker 学习笔记","text":"docker 安装docker 安装地址 常见问题12341、docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.See &apos;docker run --help&apos;.可能docker上次为正常关闭导致： systemctl start docker 验证docker 安装成功1docker run hello-world 常见镜像操作1234567891011121314151617181920212223242526272829303132333435361.获取镜像docker pull ubuntu:16.042.运行镜像,进入容器docker run -it --rm \\ubuntu:16.04 \\bash3. 退出容器exit4.列出镜像docker image ls5.查看镜像、容器、数据卷所占用的空间docker system df6.虚悬镜像docker image ls -f dangling=true7.删除虚悬镜像docker image prune8.中间层镜像docker image ls -a9.部分镜像列出docker image ls ubuntudocker image ls ubuntu:16.04docker image ls -f since=mongo:3.2docker image ls -f label=com.example.version=0.110. 特定格式显示docker image ls -qdocker image ls --format &quot;{{.ID}}: {{.Repository}}&quot;docker image ls --format &quot;table {{.ID}}\\t{{.Repository}}\\t{{.Tag}}&quot; 镜像构建1234567891011$ mkdir mynginx$ cd mynginx$ touch DockerfileDockerfile内容:FROM nginxRUN echo &apos;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&apos; &gt; /usr/share/nginx/html/index.html在Dockerfile所在的目录执行docker build -t nginx:v3 . 常见容器操作123456789101112131415161718192021222324252627282930311.新建并启动容器docker run ubuntu:16.04 /bin/echo &apos;Hello World!&apos;2.在终端打开容器docker run -t -i ubuntu:16.04 /bin/bash （其中，-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开）3. 启动已经终止的容器docker container start --help4.后台运行容器docker run -d ubuntu:16.04 /bin/sh -c &quot;while true ;do echo hello world;sleep 1;done&quot;5.查看运行容器docker container ls6.查看容器的运行日志docker logs container iddocker logs 583d0660cf1364e7e52a996e0ccab94c2c9e1b88a831233e5777adf6d46846017.终止一个容器docker container stop container iddocker container stop 583d0660cf138.查看所有的容器docker container ls -a9.启动已经停止的容器docker container start container iddocker container start 583d0660cf1310.重启容器docker container restart 进入容器两种方式123456789101. attach 命令docker run -dit ubuntudocker container lsdocker attach 14bf可以看出在退出容器是这个容器也会被停止。2. exec命令 docker exec -it f2a8 bash这个命令退出时不会停止容器。所有推荐使用这个命令进入容器中进行操作。 数据卷123456789101112131415161718192021222324251.创建数据卷docker volume create my-vol2.查看所有数据卷docker volume ls3.查看某个数据卷的具体信息docker volume inspect my-vol4.启动一个挂载数据卷的容器docker run -d -P \\&gt; --name web \\&gt; --mount source=my-vol,target=/webapp \\&gt; training/webapp \\&gt; python app.py5.删除数据卷docker volume rm my-vol如果报错：Error response from daemon: remove my-vol: volume is in use - [e6520e12082e077ff427d2ec87dc32c61bf30414570009a82363c706d5e2072e]则需要先停止正在使用数据卷的容器，并删除它。docker stop e6520e12082edocker rm e6520e12082e6.清除无主的数据卷docker volume prune docker 构建 Tomcat123456789101.搜索Tomcat镜像docker search tomcat2.拉取Tomcat镜像docker pull tomcat3.运行容器docker run --name tomcat -p 8080:8080 -v $PWD/test:/usr/local/tomcat/webapps/test -d tomcat命令说明：-p 8080:8080：将容器的8080端口映射到主机的8080端口-v $PWD/test:/usr/local/tomcat/webapps/test：将主机中当前目录下的test挂载到容器的/test docker 构建 mysql123456789101112131415161718191.搜索mysql镜像docker search mysql2.拉取mysql镜像docker pull mysql3.运行mysql镜像，构建mysql容器docker run -p 3306:3306 --name mysql \\-v /usr/local/docker/mysql/conf:/etc/mysql \\-v /usr/local/docker/mysql/logs:/var/log/mysql \\-v /usr/local/docker/mysql/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=123456 \\-d mysql命令参数：-p 3306:3306：将容器的3306端口映射到主机的3306端口-v /usr/local/docker/mysql/conf:/etc/mysql：将主机当前目录下的 conf 挂载到容器的 /etc/mysql-v /usr/local/docker/mysql/logs:/var/log/mysql：将主机当前目录下的 logs 目录挂载到容器的 /var/log/mysql-v /usr/local/docker/mysql/data:/var/lib/mysql：将主机当前目录下的 data 目录挂载到容器的 /var/lib/mysql-e MYSQL\\_ROOT\\_PASSWORD=123456：初始化root用户的密码","link":"/2019/10/23/Docker-学习笔记/"},{"title":"HBase 常用工具类","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165object HBaseUtils { def logger: Logger = LoggerFactory.getLogger(getClass) /** * 获取配置参数信息 * @return */ def getHBaseConf : Configuration = { val conf : Configuration = HBaseConfiguration.create conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;cdh-master01,cdh-master02,cdh-master03&quot;) conf.set(&quot;hadoop.security.authentication&quot;, &quot;Kerberos&quot;) conf } /** * 获取连接 * @param conf 配置信息 * @return */ def getConnection(conf:Configuration): Connection ={ ConnectionFactory.createConnection(conf) } /** * 获取管理员权限 * @param conn 连接信息 * @return HBaseAdmin */ def getAdmin(conn:Connection) : HBaseAdmin = { conn.getAdmin.asInstanceOf[HBaseAdmin] } /** * 创建表 * @param admin 管理员 * @param tableName 表名 * @param columnFamily 列族 * @throws org.apache.hadoop.hbase.MasterNotRunningException 异常 * @throws org.apache.hadoop.hbase.ZooKeeperConnectionException 异常 * @throws java.io.IOException 异常 */ @throws(classOf[MasterNotRunningException]) @throws(classOf[ZooKeeperConnectionException]) @throws(classOf[IOException]) def createTable(admin: HBaseAdmin,tableName: String,columnFamily:Array[String]):Unit = { val createTableName = TableName.valueOf(tableName) if (admin.tableExists(createTableName)){ logger.info(tableName + &quot;table exists!&quot;) }else{ val tableDesc = new HTableDescriptor(createTableName) tableDesc.addCoprocessor(&quot;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&quot;) for (singleColumnFamily &lt;- columnFamily){ val columnDesc = new HColumnDescriptor(singleColumnFamily) tableDesc.addFamily(columnDesc) } admin.createTable(tableDesc) logger.info(tableName + &quot; create table success!&quot;) }admin.close() } /** * 载入数据 * @param table HBase表 * @param rowKey 行键 * @param columnFamily 列族 * @param quorum 分布式信息 * @param value 数值 */ def addRow(table: Table,rowKey:String,columnFamily:String ,quorum:String,value:String): Unit ={ val rowPut:Put = new Put(Bytes.toBytes(rowKey)) if (value == null){ rowPut.addColumn(columnFamily.getBytes,quorum.getBytes,&quot;&quot;.getBytes) }else{ rowPut.addColumn(columnFamily.getBytes,quorum.getBytes,value.getBytes) } table.put(rowPut) } /** * 获取表数据 * @param table HBase表 * @param rowKey 行键 * @return HBase result */ def getRow(table: Table,rowKey: String ):Result = { val get:Get = new Get(Bytes.toBytes(rowKey)) val result:Result = table.get(get) for (rowKv &lt;- result.rawCells()){ println(&quot;Family:&quot; + new String(rowKv.getFamilyArray,rowKv.getFamilyOffset,rowKv.getFamilyLength,&quot;UT-8&quot;)) println(&quot;Qualifier:&quot; + new String(rowKv.getQualifierArray,rowKv.getQualifierOffset,rowKv.getQualifierLength,&quot;UT-8&quot;)) println(&quot;TimeStamp:&quot; + rowKv.getTimestamp) println(&quot;rowKey:&quot; + new String(rowKv.getRowArray,rowKv.getRowOffset,rowKv.getRowLength,&quot;UT-8&quot;)) println(&quot;Value:&quot; + new String(rowKv.getValueArray,rowKv.getValueOffset,rowKv.getValueLength,&quot;UT-8&quot;)) } result } /** * 批量添加数据 * @param table HBase表 * @param list 数据列表 */ def addDataBatch(table: Table,list: java.util.List[Put]): Unit = { try{ table.put(list) }catch{ logger.error(e.getMessage) case e: IOException =&gt; logger.error(e.getMessage) } } /** * 查询所有记录 * @param table HBase表 * @return resultScanner */ def queryAll(table: Table):ResultScanner = { val scan: Scan = new Scan try { val result: ResultScanner = table.getScanner(scan) result }catch { case e: IOException =&gt; logger.error(e.getMessage) null } } /** * 单条记录查询 * @param table HBase表 * @param queryColumn 查询列 * @param value 数值 * @param columns 列集合 * @return ResultScanner */ def queryBySingleColumn(table: Table, queryColumn: String, value: String, columns: Array[String]): ResultScanner = { if (columns == null || queryColumn == null || value == null ){ null }else{ try { val filter: SingleColumnValueFilter = new SingleColumnValueFilter(Bytes.toBytes(queryColumn),Bytes.toBytes(queryColumn),CompareOperator.EQUAL,new SubstringComparator(value)) val scan: Scan = new Scan for (column logger.error(e.getMessage) null } } } /** * 删除表 * @param hBaseConnection 链接 * @param tableName HBase表 */ def dropTable(hBaseConnection: Connection,tableName: String): Unit = { try { val admin: HBaseAdmin = hBaseConnection.getAdmin.asInstanceOf[HBaseAdmin] admin.disableTable(TableName.valueOf(tableName)) admin.deleteTable(TableName.valueOf(tableName)) }catch { case e: MasterNotRunningException =&gt; logger.error(e.getMessage) case e: ZooKeeperConnectionException =&gt; logger.error(e.getMessage) case e: IOException =&gt; logger.error(e.getMessage) } }}org.apache.hbase hbase-client${hbase.version}org.apache.hadoop hadoop-annotations","link":"/2019/10/17/HBase-常用工具类/"},{"title":"Intellij IDEA 问题记录","text":"Intellij IDEA 问题记录 问题一、123456789101112131415161718Intellij idea Language level和Java Compiler版本自动变化问题该问题主要是由于刷新pom.xml文件时，IDEA的这两个参数就会恢复成默认值解决办法：在pom.xml中加入以下配置&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;","link":"/2019/08/15/Intellij-IDEA-问题记录/"},{"title":"Linux 磁盘挂载","text":"在实际的生产应用中，经常需要对服务器的磁盘进行挂载。故总结下相关的操作，以备不时之需。1、fdisk -l 查看磁盘信息 2、fdisk /dev/vda 分区初始化 各个参数解析： m 显示所有命令列表 p 显示硬盘分割情形，打印分区表 a 设定硬盘启动区 n 设定新的硬盘分割区4.1 e 硬盘为延伸分割区4.2 p 硬盘为主要分割区5.d 删除硬盘分割区属性6.q 结束不存在硬盘分割区属性7.w 结束并写入硬盘分割区属性umount /opt 卸载挂载mke2fs -t ext4 /dev/vda1 # ext4创建文件系统df -h :查看当前硬盘使用情况","link":"/2019/05/07/Linux-磁盘挂载/"},{"title":"Linux 常用命令","text":"linux系统中清空文件内容的三种方法1.使用vi/vim命令打开文件后，输入”%d”清空，后保存即可。但当文件内容较大时，处理较慢，命令如下： vim file_name:%d:wq 2.使用cat命令情况，命令如下： cat /dev/null &gt; file_name 3.使用echo命令清空，此时会在文件中写入一个空行“\\n”，命令如下： echo “”&gt;file_name 推荐使用cat命令 vim 跳转最后一行和跳到行首第一种方式： :$ 跳到最后一行:1 跳到第一行 第二种方式： shift+g 跳到最后一行gg 跳转到第一行 查找文件关键字内容 grep -r “test” /data/reports 搜索命令 命令 作用 /string 向前搜索指定字符串；搜索时忽略大小写 :set ic n 搜索指定字符串的下一个出现位置 :%s/old/new/g 全文替换指定字符串 :n1,n2s/old/new/g 在一定范围内替换指定字符串 端口占用查询：123netstat -anlp | grep 80lsof -i:80 查看内存使用情况1free -h 查看磁盘使用情况 1df -h 查看文件创建数量 1df -i 删除某个文件的小文件12find . -type f -deletefind . -type d -print -delete 查看占用线程最大的程序1ps -eLf | wc -l 修改服务最大线程数12echo 1000000 &gt; /proc/sys/kernel/pid_max：修改pid_max值为1000000echo &quot;kernel.pid_max=1000000 &quot; &gt;&gt; /etc/sysctl.confsysctl -p：设置永久生效 监控java线程1ps -eLf | grep java | wc -l 监控网络客户连接数：1netstat -n | grep tcp | grep 侦听端口 | wc -l linux 将本地文件传给其他服务器123scp -r -P 2122 /home/lyxbdw/basedata/spark-2.4.3-bin-hadoop2.7.tgz root@lyxbdw-01:/usr/local/spark注意第二个路径与冒号之间不能有空格 查看某个目录下大文件1find / -size +50M | xargs du -h 查看CPU情况：1cat /proc/cpuinfo |grep &quot;model name&quot; &amp;&amp; cat /proc/cpuinfo |grep &quot;physical id&quot; 查看内存大小1cat /proc/meminfo | grep MemTotal","link":"/2019/05/27/Linux-常用命令/"},{"title":"MySQL 语句记录","text":"sqlserver 统计具有重复字段的记录：1SELECT product_id, COUNT(*) AS sumCount FROM dat_bill_201811 GROUP BY product_id HAVING sumCount &gt; 1; 统计出a表有的b表没有的数据：12select a.* FROM A a left outer join B b on a.qq = b.qqWHERE b.qq is null; mysql 统计具有重复字段的记录(hive也可以)：1select username,count(*) as count from hk_test group by username having count&gt;1; MySQL8.x 连接 jdbc.driverClass=com.mysql.cj.jdbc.Driverjdbc.connectionURL=jdbc:mysql://127.0.0.1:3306/spring-cloud-itoken-service-admin?serverTimezone=GMT&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falsejdbc.username=rootjdbc.password=123456 主要关注的驱动不同，新加了cj这个东东，然后可能时区问题新加serverTimezone=GMT","link":"/2019/05/16/MySQL-语句记录/"},{"title":"Mysql 优化原理 笔记","text":"Mysql 逻辑框架 MYSQL 逻辑框架整体分为三层，最上层为客户端层，并非Mysql独有，诸如：连接处理，授权认证，安全等。 MYSQL 大多数核心服务均在这中间这一层，包括查询解析、分析、优化、缓存、内置函数。所有的跨存储引擎也在这层实现，如：存储过程，触发器和视图等 最下层为存储引擎。负责Mysql中数据存储和提取。 Mysql查询过程 Mysql查询优化，首先得了解Mysql是如何优化和执行查询的，然后在实际的工作中就是遵循一些原则让Mysql的优化器能够按照预想的合理的方式运行而已。 参考：https://mp.weixin.qq.com/s/OeKXHpnk72kp37E6z97xMA","link":"/2019/03/16/Mysql-优化原理-笔记/"},{"title":"NEO4J 图数据库使用APOC数据导入","text":"Neo4j 数据导入 一、安装与部署 直接在官网下载安装包安装，解压即可。 二、下载相应的jar包 1.sqlserver 数据导入neo4j的jar包 apoc-3.4.0.1-all.jar mssql-jdbc-6.2.2.jre8.jar sqljdbc4-4.0.jar 2.mysql 数据导入neo4j的jar包 apoc-3.3.0.1-all.jar mysql-connector-java-8.0.8-dmr.jar 3.将对应jar包放在安装目录plugins文件目录里，然后conf目录里的neo4j.conf的后面加上 1234dbms.security.procedures.unrestricted=apoc.*apoc.import.file.enabled=trueapoc.export.file.enabled=true 4.restart neo4j,运行return apoc.version(),若有版本号，则成功。 三、导数据 12345678910111213import org.neo4j.driver.v1.*;public class Connect{public static void main(String[] args){ Driver driver = GraphDatabase.driver(&quot;bolt://localhost:7687&quot;,AuthTokens.basic(&quot;neo4j&quot;,&quot;neo4j&quot;)); Session session = driver.session(); String cypher=&quot;create constraint on (n:ITEM) ASSERT n.itemid is unique&quot;; //创建唯一索引，这样可以更快的导入数据 Session.run(cypher); cypher=&quot;CALL apoc.periodic.iterate(\\&quot;CALL apoc.load.jdbc(&apos;jdbc:sqlserver://localhost;username=name;password=word;database=db;characterEncoding=utf-8&apos;,\\\\\\&quot;SELECT * FROM TABLE1\\\\\\&quot;)\\&quot;,\\&quot;MERGE(n:ITEM{itemid:row.mitemid}) with * MERGE(m:ITEM{itemid:row.itemid}) with * create p=(n)-[r:rel{rels:row.rels}]-&gt;(m)\\&quot;,{batchSize:10000,iterateList:true})&quot;; //连接sqlserve数据库和设计创建neo4j图数据库数据模型 Session.run(cypher); session.close(); driver.close(); }} mysql数据库类似,不再赘述。 补充：1.使用neo4j-import导入数据的命令 1neo4j-admin import --nodes:item &quot;nodes.csv&quot; --relationships:rel &quot;rel_header.csv,rel.csv&quot; --ignore-missing-nodes 2.apoc 导出命令 1234call apoc.export.cypher.query(&quot;MATCH (p1:Person)-[r:KNOWS]-&gt;(p2:Person) RETURN p1,r,p2&quot;,&quot;/tmp/friendships.cypher&quot;,{format:&apos;plain&apos;,cypherFormat:&apos;updateStructure&apos;})` 参考： http://neo4j-contrib.github.io/neo4j-apoc-procedures/#_export_import12call apoc.export.cypher.query(&quot;match (n:lable) where not (n)--() and n.properties = &apos;400&apos; return distinct(n)&quot;,&quot;C://User/Desktop/test&quot;,{format:&apos;plain&apos;,cypherFormat:&apos;create&apos;}) 3.不用解压也能导数据load csv 123load csv from &quot;file:/twitter-2010.txt.gz&quot; as line fieldterminator &apos; &apos; with toInt(line[0]) as id,toInt(line[1]) as id1 return id,id1 limit 10using periodic commit 1000load csv from &quot;file:/twitter-2010.txt.gz&quot; as line fieldterminator &apos; &apos; create (item:ITEM{id:line[0],item:line[1]}) 常见问题总结： 问题一、apoc 导数据 时会漏掉一些数据，总是导不全？有一些批次失败。报错信息：{ “Cannot merge node using null property value for dsca”: 8} { “total”: 52562339, “committed”: 52554339, “failed”: 8000, “errors”: { “Cannot merge node using null property value for dsca”: 8 12具体原因：是dsca字段有null值造成的，可以看出导数据时需要先对数据清洗，清除脏数据。操作：将为null的数据转为&apos;null&apos;字符串。 问题二：Caused by: org.neo4j.driver.v1.exceptions.ClientException: Failed to invoke procedure apoc.periodic.iterate: Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: Connection reset数据库连接被重置可能原因有很多：可能数据库关闭了连接，也可能是防火墙和网络的原因，还有一些其他的异常可以通过try{}cathch来捕捉异常来解决。 问题三：neo4j由于查询数据量过多导致数据库挂掉。neo4j可能会因一次性查询数据太多，而挂掉。进入neo4j目录bin下。cd到bin目录下，执行启动命令：./neo4j stop另外neo4j还有其他命令，执行方式相同：neo4j { console | start | stop | restart | status }如果，./neo4j stop不能停止neo4j，用kill -s 9 强制杀掉进程。 然后，调用./neo4j start 启动neo4j 问题四：neo4j 导数据时程序未出现报错信息，但是一直卡着，数据导入失败。可以查看进程：jps、ps -p pid,直接将所有的关于neo4j的进程kill ,再重新执行数据导入程序在查看进行可以看到有两个进程CommunityEntryPoint和jar.可能造成的原因是由于上次导完数据后的链接数据库为关闭。 hive数据迁移到neo4j操作 下载与hive版本一致的jar包，如（hadoop-common-2.7.3.jar，hive-exec-1.2.1.jar，hive-jdbc-1.2.1.jar，hive-metastore-1.2.1.jar，hive-service-1.2.1.jar，httpclient-4.4.jar，httpcore-4.4.jar，libfb303-0.9.2.jar，libthrift-0.9.3.jar） 将jar包放入到plugin 目录中 重启neo4j,./neo4j restart 加载驱动，call apoc.load.driver(‘org.apache.hive.jdbc.HiveDriver’) 执行查询，call apoc.load.jdbc(‘jdbc:hive2://ip:1000/database;user=hdfs;password=hdfs’,’select name,age from user’) YIELD rowRETURN row.name, row.age; 注意： 1.hive 查询的时候需要将字段查出来，例如：select name，age from user ,如果使用 select * from user 则返回的数据为null2.hive 表中数据若是太大可能需要查询很久才能，会将数据全部查询出来才才开始进行图的构建。 问题五：报错：Node(934582) already exists with label ITEM and property itemid = ‘980_A700100171’原因：由于节点重复导致，由于itemid相同，但是节点其他属性不同，无法完成节点合并解决办法：去掉重复的节点，或者不创建唯一的itemID，但是这样数据导入速度会变慢，故建议采用去掉重复的节点id。","link":"/2019/01/14/NEO4J-图数据库使用APOC数据导入/"},{"title":"NIFI 安装部署","text":"::: hljs-center NIFI 安装部署 ::: 一、NIFI 下载 NIFI 下载链接：NIFI Downloads 选择与自己环境匹配的安装包 若是下载速度过慢，可以选择另一个下载地址：NIFI 下载 二、NIFI 安装 将下载的安装包放到 E:\\software\\ ，然后解压安装，非常简单 修改配置文件E:\\software\\nifi-1.9.2-bin\\nifi-1.9.2\\conf\\nifi.properties，更改端口号 三 、NIFI 运行 进入E:\\software\\nifi-1.9.2-bin\\nifi-1.9.2\\bin 目录，双击run-nifi.bat，若是linux则sh nifi.sh 打开浏览器，进入http://localhost:8081/nifi/，可能需要等会页面加载慢","link":"/2019/09/09/NIFI-安装部署/"},{"title":"Scala 安装问题","text":"1、此时不应有 \\scala\\bin。由于Scala 安装路径中包含空格，需要重新安装。","link":"/2019/07/10/Scala-安装问题/"},{"title":"RDD常用算子","text":"1.mapPartitionsWithIndex:独立运行在每个分片上，并带有分区的编号。 12345val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9),2) rdd1.saveAsTextFile(&quot;hdfs://master:8020/sfz&quot;) def func1(index:Int,iter: Iterator[(Int)]) : Iterator[String] = {iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;,val: &quot; + x +&quot;]&quot;).iterator} rdd1.mapPartitionsWithIndex(func1).collect 2.aggregate:先局部操作，再整体操作。 12345678910val rdd2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;),2)def func2(index: Int,iter: Iterator[(String)]) : Iterator[String] = {iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;,val: &quot; + x +&quot;]&quot;).iterator}rdd2.mapPartitionsWithIndex(func2).collectrdd2.aggregate(&quot;&quot;)(_+_,_+_)val rdd3 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;4567&quot;),2)rdd3.aggregate(&quot;&quot;)((x,y) =&gt; math.max(x.length,y.length).toString,(x,y) =&gt; x+y)val rdd5 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;&quot;,&quot;345&quot;),2)rdd5.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length,y.length).toString,(x,y) =&gt; x+y) 3.aggregateByKey：将key值相同的，先局部操作，再整体操作。和reduceByKey内部实现差不多 12val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;, 4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 2) pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect","link":"/2018/08/15/RDD常用算子/"},{"title":"Spark Windows10 安装部署","text":"Spark centos 安装部署 一、Scala 安装 jdk安装不用多说，一般没什么问题，下载Scala安装包，Scala安装包下载链接，选择自己安装的版本。123cd /usr/localmkdir scalatar -zxvf scala-2.11.8.tgz 解压安装后配置环境变量123456789vim /etc/profile export JAVA_HOME=/usr/local/java/jdk1.8.0_11 export SCALA_HOME=/usr/local/scala/scala-2.11.8 export SPARK_HOME=/usr/local/spark/spark-2.4.3-bin-hadoop2.7 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:${SCALA_HOME}/lib:${SPARK_HOME}/lib export PATH=${JAVA_HOME}/bin:$PATH:${SCALA_HOME}/bin:$PATH:${SPARK_HOME}/bin export SPARK_SSH_OPTS=&quot;-p 2122&quot; 主要是配置SCALA_HOME参数，然后运行source /etc/profile,并执行scala 二、Spark安装 Spark安装也没什么技术含量，直接下载spark安装包，spark下载链接，选择自己安装版本。1234cd /usr/localmkdir sparkcd sparktar -zxvf spark-2.4.3-bin-hadoop2.7.tgz 同样解压后配置环境变量，主要是SPARK_HOME配置，再次执行source /etc/profile,并执行spark-shell. 三、spark standalone模式部署 进入到配置目录12345678910111213141516cd /usr/local/spark/spark-2.4.3-bin-hadoop2.7/confmv slaves.template slavesmv spark-env.sh.template spark-env.shvim spark-env.sh添加以下配置# spark-env.shexport SCALA_HOME=/usr/local/scala/scala-2.11.8export JAVA_HOME=/usr/local/java/jdk1.8.0_11# 本地安装绑定export SPARK_MASTER_IP=127.0.0.1#export SPARK_LOCAL_IP=127.0.0.1export SPARK_MASTER_PORT=7077export SPARK_WORKER_MEMORY=8Gexport SPARK_EXECUTOR_CORES=4export SPARK_LOG_DIR=/lvm/data1/spark/log#export master=spark://10.7.20.191:7077 然后进入sbin目录,启动集群12cd /usr/local/spark/spark-2.4.3-bin-hadoop2.7/sbin./start-all.sh 注意若是服务器之间的免密登录端口不是22端口，则需要在 /etc/profile文件添加配置，改到对应端口1export SPARK_SSH_OPTS=&quot;-p 2122&quot; 至此spark安装完成。 Spark命令提交程序1nohup spark-submit --class com.gree.cn.location.LocationMap --master spark://lyxbdw-02:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar &gt; /lvm/data1/spark/log/location.log &amp; 执行脚本1234#!/bin/shresult_time=$(date +%Y%m%d)echo &quot;小车定位程序&quot;nohup spark-submit --class com.gree.cn.location.LocationMap --master spark://lyxbdw-02:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar &gt; /lvm/data1/spark/log/location/${result_time}_result.log &amp;","link":"/2019/08/12/Spark-Windows10-安装部署/"},{"title":"flink 基础","text":"一、Apache Flink定义：Apache Flink 是一个分布式大数据处理引擎，可以对有限流和无线流进行有状态和无状态进行计算，能够在各种集群环境上部署，对各种大小的数据集进行快速计算。 flink 启动命令1234linux./bin/start-cluster.shwindows.\\start-cluster.bat flink maven 项目依赖java版本：123456789101112&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; Scala版本：123456789101112&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; flink 关联 kafka 依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt;&lt;/dependency&gt; flink 推荐使用shade插件打包maven项目12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;artifactSet&gt; &lt;excludes&gt; &lt;exclude&gt;com.google.code.findbugs:jsr305&lt;/exclude&gt; &lt;exclude&gt;org.slf4j:*&lt;/exclude&gt; &lt;exclude&gt;log4j:*&lt;/exclude&gt; &lt;/excludes&gt; &lt;/artifactSet&gt; &lt;filters&gt; &lt;filter&gt; &lt;!-- Do not copy the signatures in the META-INF folder. Otherwise, this might cause SecurityExceptions when using the JAR. --&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;mainClass&gt;my.programs.main.clazz&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; flink 基础 API 概念 DataSet and DataStream: Flink具有特殊类DataSet并DataStream在程序中表示数据。您可以将它们视为可以包含重复项的不可变数据集合。在DataSet数据有限的情况下，对于一个DataStream元素的数量可以是无界的。 flink 一般处理步骤 获得一个execution environment， 加载/创建初始数据， 指定此数据的转换， 指定放置计算结果的位置， 触发程序执行","link":"/2019/08/30/flink-基础/"},{"title":"hive 笔记","text":"hive设置队列三种方式： set mapred.job.queue.name=queue3;SET mapreduce.job.queuename=queue3;set mapred.queue.names=queue3;","link":"/2019/04/04/hive-笔记/"},{"title":"jupyter 安装","text":"选择管理员运行cmd,执行以下命令： pip install jupyter 进入目录 C:\\Program Files\\Python37\\Scripts&gt;，运行命令： cd C:\\Program Files\\Python37\\Scripts jupyter notebook 注意管理员权限问题。 http://localhost:8888/tree 第三方库numpy安装 pip install numpy pip install -U scikit-learn pip install -U pandasql pip install graphviz pip install pydotplus","link":"/2019/01/21/jupyter-安装/"},{"title":"kafka 集群安装 部署","text":"kafka 集群安装部署 安装kafka首先需要安装zookeeper集群，这里使用的是kafka自带的zookeeper,不推荐使用（我也不知道为啥，别人都这么说）。 jdk 安装jdk 自己下载吧，后解压12cd /usr/local/javatar -zxvf jdk-8u11-linux-x64.tar.gz 配置环境变量123456vi /etc/profile#jdk环境变量配置 export JAVA_HOME=/usr/local/java/jdk1.8.0_11 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib export PATH=${JAVA_HOME}/bin:$PATH echo “JAVA_HOME=/usr/local/java/jdk1.8.0_11/“ &gt;&gt; /etc/environment 然后配置其他机器scp -r -P 2122 root@lyxbdw-02:/usr/local/java /usr/local zookeepeer集群安装123cd /usr/local/kafkatar -xzf kafka_2.12-2.2.0.tgzcd kafka_2.12-2.2.0 配置zookeeper配置文件 zookeeper.properties12345678910vim config/zookeeper.properties====================================================dataDir=/usr/local/kafka/kafka_2.11-2.2.0/zookeeperTime=2000 initLimit=10 syncLimit=5server.1=lyxbdw-01:2888:3888 server.2=lyxbdw-02:2888:3888 server.3=lyxbdw-03:2888:3888 然后在/usr/local/kafka/kafka_2.11-2.2.0/zookeeper 创建myid文件编辑123vi myid==================1 其他机器复制 zookeeper.properties到相应的目录，并设置不同的myid值每台机器都启动1bin/zookeeper-server-start.sh -daemon config/zookeeper.properties kafka集群安装这里主要就是配置相关的配置信息，server.properties123456vim config/server.properties=====================================broker.id=1host.name=lyxbdw-02log.dirs=/tmp/kafka-logszookeeper.connect=lyxbdw-01:2181,lyxbdw-02:2181,lyxbdw-03:2181 然后进入到log日志目录下，修改meta.properties文件123vim /tmp/kafka-logs/meta.properties======================================broker.id=1 其他机器类似，最后启动kafak 服务1bin/kafka-server-start.sh -daemon config/server.properties 到此，使用kafka自带的zookeeper安装就完成了。 当然，在实际的生成应用中，需要注意以下事项：1、数据目录需要放在磁盘的目录下2、进程启动最好是在后台启动 -daemon 命令可以实现。","link":"/2019/04/30/kafka-集群安装-部署/"},{"title":"jar 包相关知识","text":"1、查看jar包 1ps -aux | grep java 2、jar 包后台运行 12345678nohup java -jar xxx.jar &gt; xxx.log &amp;nohup java -jar analysisbasedata-0.0.1-SNAPSHOT.jar &gt; analysisbasedata-0.0.1-SNAPSHOT.lognohup java -jar /home/lyxbdw/basedata/comsumer/analysisbasedata-0.0.1-SNAPSHOT.jar &gt; /home/lyxbdw/basedata/comsumer/analysisbasedata.log &amp;nohup java -jar /home/lyxbdw/basedata/server/basedata-0.0.1-SNAPSHOT.jar &gt; /home/lyxbdw/basedata/server/basedata.log &amp; 3、杀死进程 1kill -9 pid 4、查找文件内容1grep &quot;17:46:49&quot; nohup.out","link":"/2019/06/14/jar-包相关知识/"},{"title":"kylin jdbc 连接","text":"kylin jdbc 连接 一 、kylin 简介 Kylin是ebay开发的一套OLAP系统，与Mondrian不同的是，它是一个MOLAP系统，主要用于支持大数据生态圈的数据分析业务，它主要是通过预计算的方式将用户设定的多维立方体缓存到HBase中。 二、引入pom文件 &lt;dependency&gt; &lt;groupId&gt;org.apache.kylin&lt;/groupId&gt; &lt;artifactId&gt;kylin-jdbc&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; 三 、例子 import org.apache.kylin.jdbc.Driver; import org.datanucleus.state.LifeCycleState; import java.sql.Connection; import java.sql.ResultSet; import java.sql.Statement; import java.util.ArrayList; import java.util.List; import java.util.Properties; public class KylinJdbc { public static void main(String[] args) throws Exception { Driver driver = (Driver) Class.forName(&quot;org.apache.kylin.jdbc.Driver&quot;).newInstance(); Properties info = new Properties();info.put(&quot;user&quot;, &quot;BI&quot;);info.put(&quot;password&quot;, &quot;16de#+ui9&quot;); Connection conn = driver.connect(&quot;jdbc:kylin://10.214.234.111:7070/Test_kylin&quot;, info); Statement state = conn.createStatement(); String sqlStr = &quot;&quot;; ResultSet resultSet = state.executeQuery(sqlStr); List list = new ArrayList(); while (resultSet.next()){ list.add(resultSet.getString(&quot;1&quot;)); } list.forEach(citg -&gt; System.out.println(citg)); } }","link":"/2019/01/18/kylin-jdbc-连接/"},{"title":"kafka相关的常规操作","text":"1、启动服务器12345cd /usr/local/kafka/kafka_2.11-2.2.0bin/zookeeper-server-start.sh -daemon config/zookeeper.propertiesbin/kafka-server-start.sh -daemon config/server.properties 2、创建主题创建一个名为“test”的主题，它只包含一个分区，只有一个副本：1bin/kafka-topics.sh --create --bootstrap-server lyxbdw-01:9092 --replication-factor 1 --partitions 1 --topic test 查看主题：1bin/kafka-topics.sh --list --bootstrap-server lyxbdw-01:9092 test 3、生产者发送消息1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 4.kafka生产者创建消息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103package com.gree.cn.basedata.kafka_netty;import java.io.IOException;import java.io.InputStream;import java.util.Properties;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;public final class KafkaProducerSingleton { private static KafkaProducer&lt;Integer, Object&gt; kafkaProducer; private String topic; private KafkaProducerSingleton() { } /** * 静态内部类 * * @author tanjie * */ private static class LazyHandler { private static final KafkaProducerSingleton instance = new KafkaProducerSingleton(); } /** * 单例模式,kafkaProducer是线程安全的,可以多线程共享一个实例 * * @return */ public static final KafkaProducerSingleton getInstance() { return LazyHandler.instance; } /** * kafka生产者进行初始化 * * @return KafkaProducer */ public void init(String topic) { this.topic = topic; if (null == kafkaProducer) { Properties props = new Properties(); InputStream inStream = null; try { inStream = this.getClass().getClassLoader() .getResourceAsStream(&quot;producer.properties&quot;); props.load(inStream); kafkaProducer = new KafkaProducer&lt;Integer, Object&gt;(props); } catch (IOException e) { e.printStackTrace(); } finally { if (null != inStream) { try { inStream.close(); } catch (IOException e) { e.printStackTrace(); } } } } } /** * 通过kafkaProducer发送消息 * 具体消息值 */ public void sendKafkaMessage( String message) { /** * 1、如果指定了某个分区,会只讲消息发到这个分区上 2、如果同时指定了某个分区和key,则也会将消息发送到指定分区上,key不起作用 * 3、如果没有指定分区和key,那么将会随机发送到topic的分区中 4、如果指定了key,那么将会以hash&lt;key&gt;的方式发送到分区中 */ ProducerRecord&lt;Integer, Object&gt; record = new ProducerRecord&lt;Integer, Object&gt;( topic, message); // send方法是异步的,添加消息到缓存区等待发送,并立即返回，这使生产者通过批量发送消息来提高效率 // kafka生产者是线程安全的,可以单实例发送消息 kafkaProducer.send(record); } /** * kafka实例销毁 */ public void close() { if (null != kafkaProducer) { kafkaProducer.close(); } } public String getTopic() { return topic; } public void setTopic(String topic) { this.topic = topic; }} 然后调用用producer实例1234KafkaProducerSingleton kafkaProducerSingleton = KafkaProducerSingleton .getInstance(); kafkaProducerSingleton.init(&quot;topic0619&quot;); kafkaProducerSingleton.sendKafkaMessage(msg+DateUtils.getFormatTimeNow()); 注意配置producer.properties文件 #指定节点列表bootstrap.servers=10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092 #指定序列化处理类key.serializer=org.apache.kafka.common.serialization.IntegerSerializervalue.serializer=org.apache.kafka.common.serialization.StringSerializeracks=0 #buffer.memory=33554432 #是否压缩compression.type=snappy #是否重试retries=0batch.size=500request.timeout.ms=5000","link":"/2019/04/30/kafka相关的常规操作/"},{"title":"java 基本知识","text":"1.泛型 通过泛型可以定义类型安全的数据结构（类型安全），而无须使用实际的数据类型（可扩展）。这能够显著提高性能并得到更高质量的代码（高性能），因为您可以重用数据处理算法，而无须复制类型特定的代码（可重用）。 2.java.sql.SQLException: Unknown system variable ‘query_cache_size’12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; 3.java.sql.SQLException: The server time zone value ‘ÖÐ¹ú±ê×¼Ê±¼ä’ is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the serverTimezone configuration property) to use a more specifc time zone value if you want to utilize time zone support. 1jdbc:mysql://localhost:3306/springboot_jpa?charset=utf8mb4&amp;useSSL=false&amp;serverTimezone=GMT 4.java.lang.Exception: No runnable methods 测试时 未在test方法上加上 @test 注解，同时保证注解依赖org.junit.test 5.STRING… 类型后面三个点(String…)，是从Java 5开始，Java语言对方法参数支持一种新写法，叫可变长度参数列表，其语法就是类型后跟…，表示此处接受的参数为0到多个Object类型的对象，或者是一个Object[]。","link":"/2019/11/29/java-基本知识/"},{"title":"maven 打包相关记录","text":"Maven项目打包这里主要记录常用的打包方式（将依赖打包进jar包）。 一、spring boot 项目对于spring boot项目只需要添加spring boot 和maven整合的插件就可以 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;!-- spring boot 依赖 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- spring boot end --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 二、普通的java项目对于普通的java项目需要添加额外的依赖才能够将依赖打包进jar中12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.gree.cn.location.LocationMap&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 三、scala 项目打包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;scala-compile-first&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;**/*.scala&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;scala-test-compile&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.gree.cn.write2mysql.Write2mysql&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt;","link":"/2019/09/05/maven-打包相关记录/"},{"title":"mybatis","text":"1、什么是mybatis MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生类型、接口和 Java 的 POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。 2、Exception in thread “main” tk.mybatis.mapper.MapperException: 无法获取实体类com.gree.cn.sparkstreamingkafka.entity.BasestationInfo对应的表名导包选择：import tk.mybatis.spring.annotation.MapperScan; 3.tk.mybatis与springboot结合使用案例（1）引入依赖1234567891011121314151617&lt;!-- 数据库连接--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 数据库连接 end --&gt; 可以看出tk.mybatis常常结合druid德鲁伊使用 （2）引入插件12345678910111213141516171819202122&lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;configuration&gt; &lt;configurationFile&gt;${basedir}/src/main/resources/generator/generatorConfig.xml&lt;/configurationFile&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;3.4.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; 可以看见接下来需要配置${basedir}/src/main/resources/generator/generatorConfig.xml配置文件 （3）在下图目录下创建generatorConfig.xml配置文件，并设置相应的参数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;&gt;&lt;generatorConfiguration&gt; &lt;!-- 引入数据库连接配置 --&gt; &lt;properties resource=&quot;jdbc.properties&quot;/&gt; &lt;context id=&quot;Mysql&quot; targetRuntime=&quot;MyBatis3Simple&quot; defaultModelType=&quot;flat&quot;&gt; &lt;property name=&quot;beginningDelimiter&quot; value=&quot;`&quot;/&gt; &lt;property name=&quot;endingDelimiter&quot; value=&quot;`&quot;/&gt; &lt;!-- 配置 tk.mybatis 插件 --&gt; &lt;plugin type=&quot;tk.mybatis.mapper.generator.MapperPlugin&quot;&gt; &lt;property name=&quot;mappers&quot; value=&quot;com.gree.cn.mybatistest.mapper.MyMapper&quot;/&gt; &lt;/plugin&gt; &lt;!-- 配置数据库连接 --&gt; &lt;jdbcConnection driverClass=&quot;${jdbc.driverClass}&quot; connectionURL=&quot;${jdbc.connectionURL}&quot; userId=&quot;${jdbc.username}&quot; password=&quot;${jdbc.password}&quot;&gt; &lt;/jdbcConnection&gt; &lt;!-- 配置实体类存放路径 --&gt; &lt;javaModelGenerator targetPackage=&quot;com.gree.cn.mybatistest.entity&quot; targetProject=&quot;src/main/java&quot;/&gt; &lt;!-- 配置 XML 存放路径 --&gt; &lt;sqlMapGenerator targetPackage=&quot;mapper&quot; targetProject=&quot;src/main/resources&quot;/&gt; &lt;!-- 配置 DAO 存放路径 --&gt; &lt;javaClientGenerator targetPackage=&quot;com.gree.cn.mybatistest.mapper&quot; targetProject=&quot;src/main/java&quot; type=&quot;XMLMAPPER&quot;/&gt; &lt;!-- 配置需要指定生成的数据库和表，% 代表所有表 --&gt; &lt;table catalog=&quot;bluetooth_location&quot; tableName=&quot;basestation_info&quot;&gt; &lt;!-- mysql 配置 --&gt; &lt;!-- &lt;generatedKey column=&quot;id&quot; sqlStatement=&quot;Mysql&quot; identity=&quot;true&quot;/&gt;--&gt; &lt;/table&gt; &lt;table catalog=&quot;bluetooth_location&quot; tableName=&quot;buffer_area_info&quot;&gt; &lt;!-- mysql 配置 --&gt; &lt;!-- &lt;generatedKey column=&quot;id&quot; sqlStatement=&quot;Mysql&quot; identity=&quot;true&quot;/&gt;--&gt; &lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 可以在上面的配置文件中需要创建数据库连接jdbc.properties配置文件,如下： 12345# MySQL 8.x: com.mysql.cj.jdbc.Driverjdbc.driverClass=com.mysql.cj.jdbc.Driverjdbc.connectionURL=jdbc:mysql://localhost:3306/bluetooth_location?serverTimezone=GMT&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falsejdbc.username=rootjdbc.password=lyxbdw 然后配置实体类存放路径 配置 XML 存放路径 配置 DAO 存放路径 配置需要指定生成的数据库和表 （4）这些配置完成后，将创建通用的父级接口MyMapper需要注意这个文件的创建目录，注释中有解释12345678910111213package com.gree.cn.mybatistest.tk.mybatis.mapper;import tk.mybatis.mapper.common.Mapper;import tk.mybatis.mapper.common.MySqlMapper;/** * * 自己的 Mapper * 特别注意，该接口不能被扫描到，否则会出错 * @param &lt;T&gt; */public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; {} （5）最后再在application.properties配置实体类路径和xml配置文件路径12mybatis.mapper-locations=classpath:mapper/*.xmlmybatis.type-aliases-package=com.gree.cn.mybatistest.entity 并在主程序入口添加注解@MapperScan(basePackages = “com.gree.cn.mybatistest.mapper”)扫描mapper包需要注意的是引入的依赖为 import tk.mybatis.spring.annotation.MapperScan; 再点击下图中的mybatis-generator:generate 前期的准备工作就完成。当然在使用前有两点需要注意下1、自动生成的mapper文件可能不会集成通用的父类mapper接口，手动引入jar包2、生成的实体类中表名可能会有两个点，需要手动删除一个 4、tk.mybatis查询数据库12345678910111213141516171819202122232425262728293031323334package com.gree.cn.mybatistest.service.impl;import com.gree.cn.mybatistest.entity.BasestationInfo;import com.gree.cn.mybatistest.entity.BufferAreaInfo;import com.gree.cn.mybatistest.mapper.BasestationInfoMapper;import com.gree.cn.mybatistest.mapper.BufferAreaInfoMapper;import com.gree.cn.mybatistest.service.MybatisTest;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import tk.mybatis.mapper.entity.Example;import java.util.List;@Servicepublic class MybatisTestimpl implements MybatisTest { @Autowired private BufferAreaInfoMapper bufferAreaInfoMapper; @Autowired private BasestationInfoMapper basestationInfoMapper; @Override public void mybatistest() { Example example = new Example(BufferAreaInfo.class); example.createCriteria().andEqualTo(&quot;stationxcoordinate&quot;, 811) .andEqualTo(&quot;stationycoordinate&quot;, 130); List&lt;BufferAreaInfo&gt; bufferAreaInfos = bufferAreaInfoMapper.selectByExample(example); BasestationInfo uu = basestationInfoMapper.selectByPrimaryKey(&quot;E07DEACF031C&quot;); System.out.println(bufferAreaInfos.size()); System.out.println(uu.getStationid()); }} 大概步骤就是注入mapper,然后使用mapper调用sql算子进行查询，其他的都类似。","link":"/2019/06/17/mybatis/"},{"title":"mysql 8.0以后授权","text":"mysql 授权操作 12345678910111213首先创建一个用户create user &apos;root&apos;@&apos;172.16.247.129&apos; identified by &apos;lyxbdw&apos;;然后进行授权 grant all privileges on *.* to &apos;root&apos;@&apos;172.16.247.129&apos; -&gt; ;刷新权限flush privileges;开启管理员权限，重启mysql![image.png](/images/2019/08/14/6a9ca990-be6f-11e9-8d50-b3f800759cdf.png) 查看某个用户权限 1show grants for &apos;root&apos;@&apos;172.16.247.129&apos;;","link":"/2019/08/14/mysql-8-0以后授权/"},{"title":"scala 正则匹配","text":"::: hljs-center scala正则匹配 ::: 中文字符匹配 1234//匹配中文字符,正则表达式val regrex =&quot;[\\u4e00-\\u9fa5]&quot;.r//匹配字符串中第一个中文字符val matches = regrex.findFirstMatchIn(dsca_part1).mkString(&quot;&quot;) 常规括号加数字匹配 1234567891011121314def get_dsca2(dsac_part2:String): String ={ var dsca_use = dsac_part2 //匹配左括号加5个数字，其中括号需要转义 val partment = &quot;\\\\([0-9]{5}&quot;.r //匹配多个括号，？代表多次匹配 val partment3 = &quot;\\\\((.*?)\\\\)&quot;.r val result = partment3.findAllMatchIn(dsca_use) result.foreach(x =&gt;{ if (partment.findAllMatchIn(x.toString()).isEmpty){ dsca_use = dsca_use.split(&quot;\\\\(&quot; +x.toString()+&quot;\\\\)&quot;)(0) } }) return dsca_use } 常用的正则表达式Scala 的正则表达式继承了 Java 的语法规则，Java 则大部分使用了 Perl 语言的规则。 下表我们给出了常用的一些正则表达式规则： 表达式 匹配规则^ 匹配输入字符串开始的位置。$ 匹配输入字符串结尾的位置。. 匹配除”\\r\\n”之外的任何单个字符。[…] 字符集。匹配包含的任一字符。例如，”[abc]”匹配”plain”中的”a”。[^…] 反向字符集。匹配未包含的任何字符。例如，”[^abc]”匹配”plain”中”p”，”l”，”i”，”n”。\\A 匹配输入字符串开始的位置（无多行支持）\\z 字符串结尾(类似$，但不受处理多行选项的影响)\\Z 字符串结尾或行尾(不受处理多行选项的影响)re 重复零次或更多次re+ 重复一次或更多次re? 重复零次或一次re{ n} 重复n次re{ n,}re{ n, m} 重复n到m次a|b 匹配 a 或者 b(re) 匹配 re,并捕获文本到自动命名的组里(?: re) 匹配 re,不捕获匹配的文本，也不给此分组分配组号(?&gt; re) 贪婪子表达式\\w 匹配字母或数字或下划线或汉字\\W 匹配任意不是字母，数字，下划线，汉字的字符\\s 匹配任意的空白符,相等于 [\\t\\n\\r\\f]\\S 匹配任意不是空白符的字符\\d 匹配数字，类似 [0-9]\\D 匹配任意非数字的字符\\G 当前搜索的开头\\n 换行符\\b 通常是单词分界位置，但如果在字符类里使用代表退格\\B 匹配不是单词开头或结束的位置\\t 制表符\\Q 开始引号：\\Q(a+b)3\\E 可匹配文本 “(a+b)3”。\\E 结束引号：\\Q(a+b)3\\E 可匹配文本 “(a+b)*3”。 正则表达式实例实例 描述. 匹配除”\\r\\n”之外的任何单个字符。[Rr]uby 匹配 “Ruby” 或 “ruby”rub[ye] 匹配 “ruby” 或 “rube”[aeiou] 匹配小写字母 ：aeiou[0-9] 匹配任何数字，类似 [0123456789][a-z] 匹配任何 ASCII 小写字母[A-Z] 匹配任何 ASCII 大写字母[a-zA-Z0-9] 匹配数字，大小写字母[^aeiou] 匹配除了 aeiou 其他字符[^0-9] 匹配除了数字的其他字符\\d 匹配数字，类似: [0-9]\\D 匹配非数字，类似: [^0-9]\\s 匹配空格，类似: [ \\t\\r\\n\\f]\\S 匹配非空格，类似: [^ \\t\\r\\n\\f]\\w 匹配字母，数字，下划线，类似: [A-Za-z0-9_]\\W 匹配非字母，数字，下划线，类似: [^A-Za-z0-9_]ruby? 匹配 “rub” 或 “ruby”: y 是可选的ruby* 匹配 “rub” 加上 0 个或多个的 y。ruby+ 匹配 “rub” 加上 1 个或多个的 y。\\d{3} 刚好匹配 3 个数字。\\d{3,} 匹配 3 个或多个数字。\\d{3,5} 匹配 3 个、4 个或 5 个数字。\\D\\d+ 无分组： + 重复 \\d(\\D\\d)+/ 分组： + 重复 \\D\\d 对([Rr]uby(, )?)+ 匹配 “Ruby”、”Ruby, ruby, ruby”，等等注意上表中的每个字符使用了两个反斜线。这是因为在 Java 和 Scala 中字符串中的反斜线是转义字符。所以如果你要输出 ..，你需要在字符串中写成 .\\. 来获取一个反斜线","link":"/2019/05/30/scala-正则匹配/"},{"title":"netty 遇到的坑","text":"一、 进制转换1. 十进制转为十六进制12int valueTen = 328;String strHex = Integer.toHexString(valueTen) 2. 十六进制转十进制12String strHex = &quot;0214010977&quot;int valueTen = Integer.pareseInt(strHex,16) 二、数据类型转换1. String字符串转Int整型123String str = &quot;2019&quot;;int i = Integer.pareseInt(str);int j = Integer.valuesOf(str).intValue(); 2.Int整型转String字符串类型123int i = 2019;String str1 = i+&quot;&quot;;String str2 = String.valueOf(i); 3.String转 byte[]1234567public static byte[] strToByteArray(String str) { if (str == null) { return null; } byte[] byteArray = str.getBytes(); return byteArray;} 4.byte[]转String1234567public static String byteArrayToStr(byte[] byteArray) { if (byteArray == null) { return null; } String str = new String(byteArray); return str;} 5. byte[]转十六进制String12345678910111213public static String byteArrayToHexStr(byte[] byteArray) { if (byteArray == null){ return null; } char[] hexArray = &quot;0123456789ABCDEF&quot;.toCharArray(); char[] hexChars = new char[byteArray.length * 2]; for (int j = 0; j &lt; byteArray.length; j++) { int v = byteArray[j] &amp; 0xFF; hexChars[j * 2] = hexArray[v &gt;&gt;&gt; 4]; hexChars[j * 2 + 1] = hexArray[v &amp; 0x0F]; } return new String(hexChars);} 6. 十六进制String转byte[]123456789101112131415public static byte[] hexStrToByteArray(String str){ if (str == null) { return null; } if (str.length() == 0) { return new byte[0]; } byte[] byteArray = new byte[str.length() / 2]; for (int i = 0; i &lt; byteArray.length; i++){ String subStr = str.substring(2 * i, 2 * i + 2); byteArray[i] = ((byte)Integer.parseInt(subStr, 16)); } return byteArray;} 三、字节数组操作1. 合并数组123456789101112 /*** 合并byte[]数组 （不改变原数组）* @param byte_1* @param byte_2* @return 合并后的数组*/ public byte[] byteMerger(byte[] byte_1, byte[] byte_2){ byte[] byte_3 = new byte[byte_1.length+byte_2.length]; System.arraycopy(byte_1, 0, byte_3, 0, byte_1.length); System.arraycopy(byte_2, 0, byte_3, byte_1.length, byte_2.length); return byte_3; } 2. 截取数组123456789101112 /** * 截取byte数组 不改变原数组 * @param b 原数组 * @param off 偏差值（索引） * @param length 长度 * @return 截取后的数组 */public byte[] subByte(byte[] b,int off,int length){ byte[] b1 = new byte[length]; System.arraycopy(b, off, b1, 0, length); return b1;} 四、时间字符串转时间戳格式12345678910111213141516public class TimeFormatTest { public static void main(String[] args) throws ParseException { String time = &quot;2019-5-23 9:24:1&quot;; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); java.util.Date date_util = sdf.parse(time); //转换为util.date java.sql.Date date_sql = new java.sql.Date(date_util.getTime());//转换为sql.date System.out.println(date_util); System.out.println(date_sql); String date = sdf.format(date_sql); System.out.println(date); date = sdf.format(date_util); System.out.println(date); } } 五、 netty和springboot整合引入pom.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.gree.cn&lt;/groupId&gt; &lt;artifactId&gt;basedata&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;basedata&lt;/name&gt; &lt;description&gt;get base data&lt;/description&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- spring boot begin--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- springboot 和 netty 整合 socket--&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.31.Final&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring boot 和 netty整合 socket 结束--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.20&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt; &lt;version&gt;5.2.0&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;!--kafka 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.44&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log4j--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt; &lt;version&gt;1.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 其中主要引入包为：12345&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.31.Final&lt;/version&gt; &lt;/dependency&gt; 第一步创建NettyTcoServer服务端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.gree.cn.basedata.server;import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelOption;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;import io.netty.util.concurrent.Future;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;import javax.annotation.PreDestroy;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;@Componentpublic class NettyTcpServer { private static final Logger log = LoggerFactory.getLogger(NettyTcpServer.class); //boss事件轮询线程组 //处理Accept连接事件的线程，这里线程数设置为1即可，netty处理链接事件默认为单线程，过度设置反而浪费cpu资源 private EventLoopGroup boss = new NioEventLoopGroup(1); //worker事件轮询线程组 //处理hadnler的工作线程，其实也就是处理IO读写 。线程数据默认为 CPU 核心数乘以2 private EventLoopGroup worker = new NioEventLoopGroup(); @Autowired ServerChannelInitializer serverChannelInitializer; @Value(&quot;${netty.tcp.client.port}&quot;) private Integer port; //与客户端建立连接后得到的通道对象 private Channel channel; /** * 存储client的channel * key:ip，value:Channel */ public static Map&lt;String, Channel&gt; map = new ConcurrentHashMap&lt;String, Channel&gt;(); /** * 开启Netty tcp server服务 * * @return */ public ChannelFuture start() { //启动类 ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(boss, worker)//组配置，初始化ServerBootstrap的线程组 .channel(NioServerSocketChannel.class)///构造channel通道工厂//bossGroup的通道，只是负责连接 .childHandler(serverChannelInitializer)//设置通道处理者ChannelHandler////workerGroup的处理器 .option(ChannelOption.SO_BACKLOG, 1024)//socket参数，当服务器请求处理程全满时，用于临时存放已完成三次握手请求的队列的最大长度。如果未设置或所设置的值小于1，Java将使用默认值50。 .childOption(ChannelOption.SO_KEEPALIVE, true);//启用心跳保活机制，tcp，默认2小时发一次心跳 //Future：异步任务的生命周期，可用来获取任务结果 ChannelFuture channelFuture1 = serverBootstrap.bind(port).syncUninterruptibly();//绑定端口，开启监听，同步等待 if (channelFuture1 != null &amp;&amp; channelFuture1.isSuccess()) { channel = channelFuture1.channel();//获取通道 log.info(&quot;Netty tcp server start success, port = {}&quot;, port); } else { log.error(&quot;Netty tcp server start fail&quot;); } return channelFuture1; } /** * 停止Netty tcp server服务 */ @PreDestroy public void destroy() { if (channel != null) { channel.close(); } try { Future&lt;?&gt; future = worker.shutdownGracefully().await(); if (!future.isSuccess()) { log.error(&quot;netty tcp workerGroup shutdown fail, {}&quot;, future.cause()); } Future&lt;?&gt; future1 = boss.shutdownGracefully().await(); if (!future1.isSuccess()) { log.error(&quot;netty tcp bossGroup shutdown fail, {}&quot;, future1.cause()); } } catch (InterruptedException e) { e.printStackTrace(); } log.info(&quot;Netty tcp server shutdown success&quot;); }} 第二步编写通道初始化流程1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.gree.cn.basedata.server;import com.gree.cn.basedata.utils.decodeutil.MyDecoder;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.codec.DelimiterBasedFrameDecoder;import io.netty.handler.codec.string.StringDecoder;import io.netty.handler.codec.string.StringEncoder;import io.netty.handler.timeout.IdleStateHandler;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import java.util.concurrent.TimeUnit;@Componentpublic class ServerChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; { @Autowired ServerChannelHandler serverChannelHandler; @Override protected void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); //IdleStateHandler心跳机制,如果超时触发Handle中userEventTrigger()方法 pipeline.addLast(&quot;idleStateHandler&quot;, new IdleStateHandler(10, 0, 0, TimeUnit.MINUTES)); //自定义编码器 byte[] delimiterByte = new byte[2]; delimiterByte[0] = 0x02; delimiterByte[1] = 0x14; ByteBuf delimiterCode = Unpooled.copiedBuffer(delimiterByte); pipeline.addLast(new DelimiterBasedFrameDecoder(64,delimiterCode)); pipeline.addLast(&quot;decoder&quot;,new MyDecoder()); //字符串编解码器 pipeline.addLast( new StringDecoder(), new StringEncoder()); //自定义Handler pipeline.addLast(&quot;serverChannelHandler&quot;, serverChannelHandler); }} 第三步自定义处理数据，并存入kafka123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182package com.gree.cn.basedata.server;import com.gree.cn.basedata.kafka_netty.KafkaProducerSingleton;import com.gree.cn.basedata.utils.DateUtils;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandler;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import io.netty.handler.timeout.IdleState;import io.netty.handler.timeout.IdleStateEvent;import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Component;import java.io.InputStream;import java.util.Properties;@Component@ChannelHandler.Sharable@Slf4jpublic class ServerChannelHandler extends SimpleChannelInboundHandler&lt;Object&gt; { /** * @param ctx 通道 * @param msg 消息 * @throws Exception 异常消息 */ @Override protected void channelRead0(ChannelHandlerContext ctx, Object msg) throws Exception { //kafka 生产者配置 Properties properties = new Properties(); //加载配置文件 InputStream is = ServerChannelHandler.class.getResourceAsStream(&quot;/producer.properties&quot;); properties.load(is); String substring = msg.toString().substring(8, 10); // 返回时间戳 if (substring.equals(&quot;77&quot;)) { short yeardate = Short.parseShort(DateUtils.getTimeNow().substring(0, 4)); int monthdate = Integer.parseInt(DateUtils.getTimeNow().substring(4, 6)); int daydate = Integer.parseInt(DateUtils.getTimeNow().substring(6, 8)); int hoursdate = Integer.parseInt(DateUtils.getTimeNow().substring(8, 10)); int mindate = Integer.parseInt(DateUtils.getTimeNow().substring(10, 12)); int seconddate = Integer.parseInt(DateUtils.getTimeNow().substring(12, 14)); byte[] test = new byte[13]; test[0] = 0x02; test[1] = 0x14; test[2] = 0x01; test[3] = 0x09; test[4] = 0x77; test[5] = (byte) (0x00FF &amp; yeardate); test[6] = (byte) (0x00FF &amp; (yeardate &gt;&gt; 8)); test[7] = ((byte) monthdate); test[8] = ((byte) daydate); test[9] = ((byte) hoursdate); test[10] = ((byte) mindate); test[11] = ((byte) seconddate); //生成异或校验码 byte temp = test[3]; for (int i = 4; i &lt; test.length; i++) { temp ^= test[i]; } test[12] = temp; ByteBuf out = Unpooled.buffer(); // ByteOrder order; // order = ByteOrder.LITTLE_ENDIAN; //out.order(order); out.writeBytes(test); ctx.channel().writeAndFlush(out); } //然后将传入的数据全部写入到kafka内，并加上接收时间 // VoteProduceSendmsg voteProduceSendmsg = new VoteProduceSendmsg(); // voteProduceSendmsg.sendMsg(msg+DateUtils.getFormatTimeNow(),properties); // ExecutorService executorService = Executors.newFixedThreadPool(2);// executorService.submit(new HandlerProducer(msg)); KafkaProducerSingleton kafkaProducerSingleton = KafkaProducerSingleton .getInstance(); kafkaProducerSingleton.init(&quot;topic0619&quot;); kafkaProducerSingleton.sendKafkaMessage(msg+DateUtils.getFormatTimeNow()); //kafkaProducerSingleton.close(); ctx.channel().flush(); } /** * 活跃的、有效的通道 * 第一次连接成功后进入的方法 * * @param ctx 有效的通道 * @throws Exception 异常 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { super.channelActive(ctx); log.info(&quot;tcp client &quot; + getRemoteAddress(ctx) + &quot; connect success&quot;); //往channel map中添加channel信息 NettyTcpServer.map.put(getIPString(ctx), ctx.channel()); } /** * 不活动的通道 * 连接丢失后执行的方法（client端可据此实现断线重连） * * @param ctx 不活动的通道 * @throws Exception 异常 */ @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception { //删除Channel Map中的失效Client log.info(&quot;不活动通道关闭!&quot;); NettyTcpServer.map.remove(getIPString(ctx)); ctx.close(); } /** * 异常处理 * @param ctx 通道 * @throws Exception 异常 */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { super.exceptionCaught(ctx, cause); //发生异常，关闭连接 log.error(&quot;引擎 {} 的通道发生异常，即将断开连接&quot;, getRemoteAddress(ctx)); ctx.close();//再次建议close } /** * 心跳机制，超时处理 * @param ctx 通道 * @param evt 事件 * @throws Exception 异常 */ @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception { String socketString = ctx.channel().remoteAddress().toString(); if (evt instanceof IdleStateEvent) { IdleStateEvent event = (IdleStateEvent) evt; if (event.state() == IdleState.READER_IDLE) { log.info(&quot;Client: &quot; + socketString + &quot; READER_IDLE 读超时&quot;); ctx.disconnect();//断开 } else if (event.state() == IdleState.WRITER_IDLE) { log.info(&quot;Client: &quot; + socketString + &quot; WRITER_IDLE 写超时&quot;); ctx.disconnect(); } else if (event.state() == IdleState.ALL_IDLE) { log.info(&quot;Client: &quot; + socketString + &quot; ALL_IDLE 总超时&quot;); ctx.disconnect(); } } } /** * 获取client对象：ip+port * * @param ctx 通道 * @return 返回值 */ private String getRemoteAddress(ChannelHandlerContext ctx) { String socketString = &quot;&quot;; socketString = ctx.channel().remoteAddress().toString(); return socketString; } /** * 获取client的ip * @param ctx 通道 * @return 返回值 */ private String getIPString(ChannelHandlerContext ctx) { String ipString = &quot;&quot;; String socketString = ctx.channel().remoteAddress().toString(); int colonAt = socketString.indexOf(&quot;:&quot;); ipString = socketString.substring(1, colonAt); return ipString; }} 12创建necat .\\nc.exe -l -p 9000","link":"/2019/05/22/netty-遇到的坑/"},{"title":"redis 数据库","text":"Redis 数据库 Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。 Redis 安装参考redis基础教程 一、客户端模式：123cd /usr/local/redis/redis-5.0.5/src./redis-cli --rawhvals materialinfo 二、springboot 与 redis 整合1.引入依赖123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; 2.配置依赖文件12345678# redisspring.redis.host = 172.80.122.33spring.redis.port = 6379spring.redis.database = 0spring.redis.timeout = 100msspring.redis.lettuce.pool.max-active = 8spring.redis.lettuce.pool.max-idle = 8spring.redis.lettuce.pool.min-idle = 0spring.redis.lettuce.pool.max-wait = 100ms 3.使用redisTemplate1234@Autowired private RedisTemplate&lt;String,String&gt; redisTemplate;redisTemplate.opsForHash().entries(&quot;scada:dashboard:enamelling_line_status&quot;); redis-client操作redis数据1.引入依赖123456&lt;!-- Redis客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; 2.配置数据库连接池1234567891011121314private static JedisPool jedisPool; private Jedis jedis; static { JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(20); config.setMaxIdle(5); config.setMaxWaitMillis(1000); config.setMinIdle(2); config.setTestOnBorrow(false); jedisPool = new JedisPool(config, &quot;127.0.0.1&quot;, 6379); } private static synchronized Jedis getJedis(){ return jedisPool.getResource(); } 3.操作数据库12jedis = getJedis();jedis.hvals(&quot;materialinfo&quot;); redis client 命令操作对String操作命令 set(Key,value) :给数据库中名称为Key的String赋值value get(Key):返回数据库中名称为Key的String的value getset(key, value)：给名称为 key 的 string 赋予上一次的 value mget(key1, key2,…, key N)：返回库中多个 string 的 value setnx(key, value)：添加 string，名称为 key，值为 value setex(key, time, value)：向库中添加 string，设定过期时间 time mset(key N, value N)：批量设置多个 string 的值 msetnx(key N, value N)：如果所有名称为 key i 的 string 都不存在 incr(key)：名称为 key 的 string 增 1 操作 incrby(key, integer)：名称为 key 的 string 增加 integer decr(key)：名称为 key 的 string 减 1 操作 decrby(key, integer)：名称为 key 的 string 减少 integer append(key, value)：名称为 key 的 string 的值附加 value substr(key, start, end)：返回名称为 key 的 string 的 value 的子串 对List操作命令 rpush(key, value)：在名称为 key 的 list 尾添加一个值为 value 的元素 lpush(key, value)：在名称为 key 的 list 头添加一个值为 value 的元素 llen(key)：返回名称为 key 的 list 的长度 lrange(key, start, end)：返回名称为 key 的 list 中 start 至 end 之间的元素 ltrim(key, start, end)：截取名称为 key 的 list lindex(key, index)：返回名称为 key 的 list 中 index 位置的元素 lset(key, index, value)：给名称为 key 的 list 中 index 位置的元素赋值 lrem(key, count, value)：删除 count 个 key 的 list 中值为 value 的元素 lpop(key)：返回并删除名称为 key 的 list 中的首元素 rpop(key)：返回并删除名称为 key 的 list 中的尾元素 blpop(key1, key2,… key N, timeout)：lpop 命令的 block 版本。 brpop(key1, key2,… key N, timeout)：rpop 的 block 版本。 rpoplpush(srckey, dstkey)：返回并删除名称为 srckey 的 list 的尾元素，并将该元素添加到名称为 dstkey 的 list 的头部 对Set操作命令 sadd(key, member)：向名称为 key 的 set 中添加元素 member srem(key, member) ：删除名称为 key 的 set 中的元素 member spop(key) ：随机返回并删除名称为 key 的 set 中一个元素 smove(srckey, dstkey, member) ：移到集合元素 scard(key) ：返回名称为 key 的 set 的基数 sismember(key, member) ：member 是否是名称为 key 的 set 的元素 sinter(key1, key2,…key N) ：求交集 sinterstore(dstkey, (keys)) ：求交集并将交集保存到 dstkey 的集合 sunion(key1, (keys)) ：求并集 sunionstore(dstkey, (keys)) ：求并集并将并集保存到 dstkey 的集合 sdiff(key1, (keys)) ：求差集 sdiffstore(dstkey, (keys)) ：求差集并将差集保存到 dstkey 的集合 smembers(key) ：返回名称为 key 的 set 的所有元素 srandmember(key) ：随机返回名称为 key 的 set 的一个元素 对 Hash 操作的命令 hset(key, field, value)：向名称为 key 的 hash 中添加元素 field hget(key, field)：返回名称为 key 的 hash 中 field 对应的 value hmget(key, (fields))：返回名称为 key 的 hash 中 field i 对应的 value hmset(key, (fields))：向名称为 key 的 hash 中添加元素 field hincrby(key, field, integer)：将名称为 key 的 hash 中 field 的 value 增加 integer hexists(key, field)：名称为 key 的 hash 中是否存在键为 field 的域 hdel(key, field)：删除名称为 key 的 hash 中键为 field 的域 hlen(key)：返回名称为 key 的 hash 中元素个数 hkeys(key)：返回名称为 key 的 hash 中所有键 hvals(key)：返回名称为 key 的 hash 中所有键对应的 value hgetall(key)：返回名称为 key 的 hash 中所有的键（field）及其对应的 value","link":"/2019/08/22/redis-数据库/"},{"title":"shell 编写 脚本常用命令","text":"统计test.sh 文件命令的行数sed -n ‘$=’ /home/hadoop/test/test.sh shell 脚本日志写入追加时间文件夹date_time=date +'%Y-%m-%d' &amp;&amp; sh /home/greesj2b/jiaoben/bomrank.sh &gt;&gt; /home/greesj2b/jiaoben/bomrank/log/bomrank_$date_time.log 2&gt;&amp;1 hive 查看表锁情况 show locks; 对表解锁unlock table tablename; 关闭锁机制set hive.support.concurrency=false;默认为true 问题一 syntax error: unexpected end of file原因：最后发现我的脚本是在window环境下编写的，然后传到linux服务器上的，这时候问题来了，doc下的文本内容格式和unix下的格式有所不同，比如dos文件传输到unix系统时,会在每行的结尾多一个^M结束符。（我的就是这个原因）解决办法： 123vim serverDeploy.sh:set fileformat=unix:wq","link":"/2019/03/15/shell-编写-脚本常用命令/"},{"title":"spark structed streaming 读取kafka数据","text":"structed streaming : Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. Structed streaming 官方文档 2.4.3官方文档 一、部分关于如何连接kafka数据，并做计算。首先引入pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.gree.cn&lt;/groupId&gt; &lt;artifactId&gt;structstreaminglocation&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;StructStreamingLocation&lt;/name&gt; &lt;description&gt;Demo project for StructStreamingLocation&lt;/description&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- spark 相关依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- sparkStreaming kafka 整合--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- sparkStreaming kafka end --&gt; &lt;!-- struct streaming kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- struct streaming kafka 结束--&gt; &lt;!-- spark 相关依赖 --&gt; &lt;!-- 数据库连接--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 数据库连接 end --&gt; &lt;!-- Redis客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 常用工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.2&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;!-- 常用工具 end --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.gree.cn.location.LocationMap&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 然后编写相应的代码逻辑：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package com.gree.cn.location;import com.gree.cn.entity.*;import com.gree.cn.sink.BaseHeartSink;import com.gree.cn.sink.LabelSink;import com.gree.cn.sink.MaterialInfoSink;import com.gree.cn.sink.MaterialRedisSink;import com.gree.cn.utils.DruidPoolUtils;import org.apache.spark.api.java.function.MapFunction;import org.apache.spark.sql.*;import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;import org.apache.spark.sql.streaming.StreamingQuery;import org.apache.spark.sql.streaming.StreamingQueryException;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.util.ArrayList;import java.util.Collections;import java.util.Properties;import static org.apache.spark.sql.functions.*;public class LocationMap { public static void main(String[] args) { //创建SparkSession SparkSession spark = SparkSession .builder() .appName(&quot;小车定位程序&quot;) //.master(&quot;local[2]&quot;) .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.local.dir&quot;, &quot;/lvm/data1/bluetooth_location/spark/location&quot;) .config(&quot;spark.ui.port&quot;, &quot;8083&quot;) .config(&quot;spark.sql.shuffle.partitions&quot;, &quot;50&quot;) .config(&quot;spark.default.parallelism&quot;, &quot;12&quot;) .getOrCreate(); //读取kafka数据 Dataset&lt;Row&gt; kafkaValue = spark.readStream() .format(&quot;kafka&quot;) .option(&quot;kafka.bootstrap.servers&quot;, &quot;10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092&quot;) .option(&quot;subscribe&quot;, &quot;topic0619&quot;) //.option(&quot;kafka.bootstrap.servers&quot;,&quot;172.16.247.129:9092&quot;) //.option(&quot;subscribe&quot;,&quot;test06&quot;) .option(&quot;startingOffsets&quot;, &quot;latest&quot;) .load() .selectExpr(&quot;CAST(value AS STRING)&quot;); //基站数据包括 接收时间，基站ID，信标ID，信号强度 Dataset&lt;BaseTable&gt; baseTableDs = kafkaValue.as(Encoders.STRING()).map( (MapFunction&lt;String, BaseTable&gt;) values -&gt; { BaseTable baseTable = new BaseTable(); //具体逻辑 ... ... ... return baseTable; } , ExpressionEncoder.javaBean(BaseTable.class)); //获取信标电量表数据 Dataset&lt;LabelData&gt; labelData = baseTableDs .filter(col(&quot;orderName&quot;).equalTo(&quot;66&quot;)) .select(col(&quot;labelID&quot;), col(&quot;electricity&quot;), col(&quot;baseID&quot;),col(&quot;recDatetime&quot;)) .as(ExpressionEncoder.javaBean(LabelData.class)); /*StreamingQuery labelDataStart =*/ labelData.repartition(2, col(&quot;labelID&quot;)) .writeStream() .outputMode(&quot;append&quot;) .foreach(new LabelSink()) .start(); //labelData.writeStream().format(&quot;console&quot;).start();} 二、如何将数据sink到mysql12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.gree.cn.sink;import com.gree.cn.entity.LabelData;import com.gree.cn.utils.DruidPoolUtils;import org.apache.spark.sql.ForeachWriter;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.SQLException;public class LabelSink extends ForeachWriter&lt;LabelData&gt; { /** * sink 信标数据到MySQL中 * */ private PreparedStatement statement; private Connection connection; public boolean open(long arg0, long arg1) { return true; } public void process(LabelData labelData) { try { connection = DruidPoolUtils.getConnection(); String insertSql = &quot; REPLACE INTO bluetooth_location.beacon_info (beaconId,electricity,stationId,updateTime) VALUES(?,?,?,?) &quot;; statement = connection.prepareStatement(insertSql); statement.setString(1,labelData.getLabelID()); statement.setInt(2,labelData.getElectricity()); statement.setString(3,labelData.getBaseID()); statement.setString(4,labelData.getRecDatetime()); statement.executeUpdate(); } catch (SQLException e) { e.printStackTrace(); }finally { DruidPoolUtils.close(connection,statement); } } @Override public void close(Throwable arg0) { }} 三、数据库连接池配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package com.gree.cn.utils;import com.alibaba.druid.pool.DruidDataSourceFactory;import javax.sql.DataSource;import java.sql.Connection;import java.sql.ResultSet;import java.sql.SQLException;import java.sql.Statement;import java.util.Properties;/** * 数据库连接池工具类 */public class DruidPoolUtils { //创建成员变量 private static DataSource dataSource; //加载配置文件 static { try { Properties properties = new Properties(); //加载类路径 properties.load(DruidPoolUtils.class.getResourceAsStream(&quot;/druid.properties&quot;)); //读取属性文件，创建连接池 dataSource = DruidDataSourceFactory.createDataSource(properties); }catch (Exception e){ e.printStackTrace(); } } //获取数据源 public static DataSource getDataSource(){ return dataSource; } //获取连接对象 public static Connection getConnection(){ try { return dataSource.getConnection(); }catch (SQLException e){ throw new RuntimeException(e); } } //释放资源 public static void close(Connection connection, Statement statement, ResultSet resultSet){ if (resultSet != null ){ try { resultSet.close(); }catch (SQLException e){ e.printStackTrace(); } } if (statement != null){ try { statement.close(); }catch (SQLException e){ e.printStackTrace(); } } if (connection != null){ try { connection.close(); }catch (SQLException e){ e.printStackTrace(); } } } public static void close(Connection connection,Statement statement){ close(connection,statement,null); }} druid.properties driverClassName=com.mysql.cj.jdbc.Driverurl=jdbc:mysql://localhost:3306?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falseusername=rootpassword=lyxbdwinitialSize=5maxActive=20maxWait=3000minIdle=3 Scala编写structed stremaing引入pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.gree.cn&lt;/groupId&gt; &lt;artifactId&gt;write2mysql&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;write2mysql&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- 常用插件 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.6&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 常用插件 结束 --&gt; &lt;!-- spark 相关依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spark streaming kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spark streaming kafka 结束--&gt; &lt;!-- struct streaming kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- struct streaming kafka 结束--&gt; &lt;!-- spark 相关依赖 结束--&gt; &lt;!-- 数据库连接--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.15&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 数据库连接 end --&gt; &lt;!-- log 日志--&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log 日志结束--&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;scala-compile-first&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;**/*.scala&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;scala-test-compile&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.gree.cn.write2mysql.Write2mysql&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 与java的差别主要是打包插件不同，编写相应逻辑123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.gree.cn.write2mysqlimport com.gree.cn.sink.MysqlSinkimport org.apache.spark.sql.SparkSession/** * 原始数据保存 * @author sfz * Time 2019-08-08 */object Write2mysql { def main(args: Array[String]): Unit = { //创建SparkSession val spark = SparkSession .builder() .appName(&quot;write2mysql&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.local.dir&quot;, &quot;/lvm/data1/bluetooth_location/spark/write2mysql&quot;) .config(&quot;spark.ui.port&quot;, &quot;8084&quot;) .getOrCreate() //引入隐式转换，保证String可以序列化 import spark.implicits._ //读取kafka数据 val kafkaValue = spark.readStream .format(&quot;kafka&quot;) //.option(&quot;kafka.bootstrap.servers&quot;, &quot;10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092&quot;) // .option(&quot;subscribe&quot;, &quot;topic0619&quot;) .option(&quot;kafka.bootstrap.servers&quot;,&quot;localhost:9092&quot;) .option(&quot;subscribe&quot;,&quot;test06&quot;) .option(&quot;startingOffsets&quot;, &quot;latest&quot;) .load() .selectExpr(&quot;CAST(value AS STRING)&quot;) //将kafka数据转化为BaseTable结构 val baseData = kafkaValue.as[String].map(values =&gt; {//具体的解析数据逻辑。。。。。。。。。 BaseTable.apply(frameHeader, equipment, frameLength, orderName, baseID, labelID, signalPower, electricity, datetime, checkName, recDatetime) } else { BaseTable.apply(frameHeader, equipment, frameLength, orderName, baseID77, labelID, signalPower, electricity, datetime, checkName, recDatetime) } }).as[BaseTable] //将数据sink到mysql数据库中 val query = baseData.writeStream .foreach(new MysqlSink) .outputMode(&quot;append&quot;) .start() query.awaitTermination() } //创建BaseTable结构 case class BaseTable(frameHeader: String, equipment: String, frameLength: String, orderName: String, baseId: String, labelId: String, signalPower: Integer, electricity: Integer, datetime: String, checkName: String, recDatetime: String)} 接着sink到mysql 数据库中1234567891011121314151617181920212223242526272829303132333435363738394041package com.gree.cn.sinkimport java.sql.{Connection, PreparedStatement, SQLException}import com.gree.cn.utils.{DateUtils, MysqlPoolUtils}import com.gree.cn.write2mysql.Write2mysql.BaseTableimport org.apache.spark.sql.ForeachWriterclass MysqlSink() extends ForeachWriter[BaseTable](){ var conn:Connection = _ var statement : PreparedStatement = _ override def open(partitionId: Long, epochId: Long): Boolean = { conn = MysqlPoolUtils.getConnection.get conn.setAutoCommit(false) true } override def process(baseTable: BaseTable): Unit = { val insertSql = &quot; &quot; statement = conn.prepareStatement(insertSql) statement.setString(1, baseTable.frameHeader) 。。。。。。 statement.setString(11, baseTable.recDatetime) statement.executeUpdate() } override def close(errorOrNull: Throwable): Unit = { try{ if ( statement != null &amp;&amp; conn != null) { statement.close() conn.close() } } catch { case e: SQLException =&gt; e.printStackTrace() } }} 最后配上数据库连接池123456789101112131415161718192021222324252627282930313233343536373839package com.gree.cn.utilsimport java.sql.Connectionimport java.util.Propertiesimport com.alibaba.druid.pool.DruidDataSourceFactoryimport javax.sql.DataSourceimport org.apache.log4j.Logger/** * @author sfz * MySql 数据库连接池 */object MysqlPoolUtils {private val log = Logger.getLogger(MysqlPoolUtils.getClass.getName)val dataSource:Option[DataSource] = { try { val druidProps = new Properties() //获取Druid连接池配置文件 val druidConfig = getClass.getResourceAsStream(&quot;/application.properties&quot;) //加载配置文件 druidProps.load(druidConfig) Some(DruidDataSourceFactory.createDataSource(druidProps)) }catch { case error :Exception =&gt; log.error(&quot;Error Create Mysql Connection &quot;,error) None }} //获取连接 def getConnection:Option[Connection] = { dataSource match { case Some(ds) =&gt; Some(ds.getConnection) case None =&gt; None } }} application.properties driver-class-name = com.mysql.cj.jdbc.Driverurl=jdbc:mysql://localhost:3306/base_data?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falseusername=rootpassword=lyxbdwfilters=stat,configinitialSize=2maxActive=20minIdle=2maxWait=60000timeBetweenEvictionRunsMillis=60000minEvictableIdleTimeMillis=300000validationQuery=SELECT 1testWhileIdle=truetestOnBorrow=falsetestOnReturn=falseremoveAbandoned=truelogAbandoned=truepoolPreparedStatements=falseremoveAbandonedTimeout=1800maxOpenPreparedStatements=100","link":"/2019/08/27/spark-structed-streaming-读取kafka数据/"},{"title":"spark sql  java 自定义函数","text":"::: hljs-center java自定义函数 ::: //自定义函数将信号强度转距离 //其中udf+数字中数字代表入参个数，最后一个参数代表出参的数据类型 //出现序列化问题 123456spark.udf().register(&quot;transsinglepower&quot;, new UDF1&lt;Integer,Double&gt;() { @Override public Double call(Integer single_poer) throws Exception { Double mesure_distinct = Math.pow(10, (Math.abs(single_poer) - 59) / (10 * 2.0)); return mesure_distinct; }}, DataTypes.DoubleType);","link":"/2019/07/18/spark-sql-java-自定义函数/"},{"title":"neo4j 连接 spark","text":"今天使用neo4j连接sparkneo4j版本3.4spark版本1.6.0 (1) 首先，需要添加jar包 neo4j-spark-connector_2.10-1.0.0-RC1.jar或者添加maven依赖123org.neo4j.spark neo4j-spark-connector_2.10 1.0.0-RC1 (2) 设置spark连接信息val conf : SparkConf = new SparkConf().setAppName(“InitSpark”).setMaster(“local[*]”) conf.set(“spark.neo4j.bolt.url”,”bolt://localhost:7687”)conf.set(“spark.neo4j.bolt.user”,”neo4j”)conf.set(“spark.neo4j.bolt.password”,”123456”) 简单例子 123456object Neo4jDataFrameTest { def main(args: Array[String]): Unit = { val sQLContext = InitSpark.getSqlContext val query =&quot; MATCH p=(m:ITEM)&lt;-[r:rel*]-(n:ITEM) where m.item = &apos;84010420&apos; and ALL(c in r where c.exdt&gt;&apos;2018-12-15&apos;)&quot; + &quot; with m.item as mark,length(r) as len, last(r).number as num,reduce(s=1.0 ,x in rels(p)| s*tofloat(x.number)) as nums, last(r).mitem as mitem , last(r) as bom &quot; + &quot; return mark,bom.erpid as erpid, bom.virtual as virtual,mitem,bom.item as item,bom.pono as pono,&quot; + &quot; bom.comment as comment, bom.warehouse as warehouse, bom.exdt as exdt, bom.indt as indt,tofloat(num) as num,nums,len &quot; val df = Neo4jDataFrame.withDataType(sQLContext,query,Seq.empty,&quot;mark&quot; -&gt; StringType,&quot;erpid&quot; -&gt;StringType,&quot;virtual&quot;-&gt;LongType, &quot;mitem&quot;-&gt;StringType,&quot;item&quot;-&gt;StringType,&quot;pono&quot;-&gt;LongType,&quot;comment&quot;-&gt;StringType,&quot;warehouse&quot;-&gt;StringType,&quot;exdt&quot;-&gt;StringType, &quot;indt&quot; -&gt;StringType,&quot;num&quot;-&gt;DoubleType,&quot;nums&quot;-&gt; DoubleType ,&quot;len&quot;-&gt;LongType) df.show(1000) }} 12345678object Neo4jConnectSparkGraph { def main(args: Array[String]): Unit = { val sc = InitSpark.getSC val sQLContext = InitSpark.getSqlContext val cypher = &quot;match (n:ITEM) return n.item limit 10&quot; val neo = Neo4jRowRDD(sc,cypher) } }","link":"/2019/03/11/neo4j-连接-spark/"},{"title":"spark standalone  运行原理","text":"一 、spark standalone 运行模式 standalone 运行模式 是由客户端，主节点和多个worker节点组成。Worker节点可以通过ExecutorRunner运行在当前节点上的CoarseGrainedExecutorBackend进程，每个Worker节点上存在一个或多个CoarseGrainedExecutorBackend进程，每个进程包含一个Executor对象。 该对象持有一个线程池,每个线程可以执行一个task。12345671.启动应用程序，在SparkContext启动过程中，先初始化DAGScheduler 和 TaskSchedulerImpl两个调度器， 同时初始化SparkDeploySchedulerBackend，并在其内部启动DriverEndpoint 和 ClientEndpoint2.ClientEndpoint向Master注册应用程序。Master收到注册消息后把应用放到待运行应用列表，使用自己的资源调度算法分配Worker资源给应用程序。3.应用程序获得Worker时，Master会通知Worker中的WorkerEndpoint创建CoarseGrainedExecutorBackend进程，在该进程中创建执行容器Executor。4.Executor创建完毕后发送消息到Master 和 DriverEndpoint。在SparkContext创建成功后， 等待Driver端发过来的任务。5.SparkContext分配任务给CoarseGrainedExecutorBackend执行，在Executor上按照一定调度执行任务(这些任务就是自己写的代码)6.CoarseGrainedExecutorBackend在处理任务的过程中把任务状态发送给SparkContext，SparkContext根据任务不同的结果进行处理。如果任务集处理完毕后，则继续发送其他任务集。7.应用程序运行完成后，SparkContext会进行资源回收。 二、spark standalone两种提交模式1. standalone-client 任务提交模式提交命令：12345678# Run on a Spark standalone cluster in client deploy mode./bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master spark://207.184.161.138:7077 \\ --executor-memory 20G \\ --total-executor-cores 100 \\ /path/to/examples.jar \\ 1000 例子：12345#!/bin/shresult_time=$(date +%Y%m%d)echo &quot;小车定位程序&quot;nohup spark-submit --class com.gree.cn.location.LocationMap --master spark://lyxbdw-02:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 --executor-memory 4G --total-executor-cores 12 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar &gt; /lvm/data1/spark/log/location/${result_time}_result.log &amp; standalone client 模式任务流程 执行流程 client 模式提交任务后，会在客户端启动Driver进程。 Driver 会向Master申请启动Application启动资源。 资源申请成功后，Driver端会将task发送到worker端执行。 worker端执行成功后将执行结果返回给Driver端 1. standalone-cluster 任务提交模式提交命令：12345678910# Run on a Spark standalone cluster in cluster deploy mode with supervise./bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master spark://207.184.161.138:7077 \\ --deploy-mode cluster \\ --supervise \\ --executor-memory 20G \\ --total-executor-cores 100 \\ /path/to/examples.jar \\ 1000 例子：12345#!/bin/shresult_time=$(date +%Y%m%d)echo &quot;小车定位程序&quot;spark-submit --class com.gree.cn.location.LocationMap --master spark://lyxbdw-02:7077 --deploy-mode cluster --supervise --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 --executor-memory 4G --total-executor-cores 12 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar 注意：Standalone-cluster提交方式，应用程序使用的所有jar包和文件，必须保证所有的worker节点都要有，因为此种方式，spark不会自动上传包。 standalone-cluster模式提交任务 执行流程： 客户端使用命令spark-submit –deploy-mode cluster 后会启动spark-submit进程 此进程为Driver向Master 申请资源。 Master会随机在一台Worker节点来启动Driver进程。 Driver启动成功后，spark-submit关闭，然后Driver向Master申请资源。 Master接收到请求后，会在资源充足的Worker节点上启动Executor进程。 Driver分发Task到Executor中执行。 建议：在平时调试时使用client模式，在正式生产环境时建议使用cluster模式 参考文章：1、大话Spark(5)-三图详述Spark Standalone/Client/Cluster运行模式2、Spark中Standalone的两种提交模式（Standalone-client模式与Standalone-cluster模式）","link":"/2019/08/26/spark-standalone-运行原理/"},{"title":"spark 常用 api","text":"1.spark 中采用部分关联字段 12//可以看出Temp_khcp 字段包含 first_chart 字段 ，通过部分关联来实现表连接 val alphabet_Beign = number_Beign.join(custom_file_regular, col(&quot;Temp_khcp&quot;).startsWith(col(&quot;first_chart&quot;)), &quot;left&quot;).drop(&quot;first_chart&quot;).na.fill(&quot;&quot;, Seq(&quot;before_file&quot;))","link":"/2019/11/26/spark-常用-api/"},{"title":"spark 常用参数设置","text":"1234// task数量设置spark.sql.shuffle.partitions 50//并行度设置spark.default.parallelism 10 spark提交部署程序脚本12345#!/bin/shresult_time=$(date +%Y%m%d)echo &quot;小车定位程序&quot;spark-submit --class com.gree.cn.location.LocationMap --master spark://lyxbdw-02:7077 --deploy-mode cluster --supervise --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 --executor-memory 4G --total-executor-cores 12 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar","link":"/2019/08/09/spark-常用参数设置/"},{"title":"spring boot 注解","text":"问题一、 Could not autowire. No beans of ‘RestTemplate’ type found 在mapper 类添加注解@Repository 即可解决，简单有效 方法2：在mapper文件上加@Component注解，把普通pojo实例化到spring容器中","link":"/2019/07/16/spring-boot-注解/"},{"title":"spring cloud","text":"::: hljs-center spring cloud 常见错误以及解决办法::: 问题 1：找不到主类或加载不到主类解决办法：尝试 mvn clean ，mvn install 问题2：Module “spring-cloud-itoken”must not contain source root “C:\\Users\\hadoop\\IdeaProjects\\spring-cloud-itoken\\spring-cloud-itoken-admin\\src\\main\\java”.The root already belongs to module “spring-cloud-itoken-admin”解决办法：这类问题通常是由于多模块项目，删除了root 模块的src 目录造成的 所有找到root模块，然后将其右侧的sources 下的多余文件找到并删除即可。 问题3 ：java程序找不到包解决办法： 重新设置module sdk 的路径一般能够解决。 问题4 ：org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘com.sfz.spring.cloud.itoken.service.admin.test.admin.AdminServiceTest’: Unsatisfied dependency expressed through field ‘adminService’; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type ‘com.sfz.spring.cloud.itoken.service.admin.service.AdminService’ available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)} 解决办法啊： 在 serviceImpl 加上注解 @Service 问题5 ： java.lang.IllegalStateException: Failed to load ApplicationContext 解决办法 ：未启动ConfigApplication,验证方法：打开http://127.0.0.1:8888/spring-cloud-itoken-service-admin/prod 问题6 ：Caused by: java.net.ConnectException: Connection refused: connect解决办法： 未加载注解 @ActiveProfiles(value = “prod”) 问题7 ：Failed to execute goal on project spring-cloud-itoken-web-admin: Could not resolve dependencies for project com.sfz:spring-cloud-itoken-web-admin:jar:1.0.0-SNAPSHOT: Failure to find com.sfz:spring-cloud-itoken-common-web:jar:1.0.0-SNAPSHOT in https://oss.sonatype.org/content/repositories/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of sonatype-repos-s has elapsed or updates are forced -&gt; [Help 1]解决办法： 在父项目下有的子项目在首次运行clean 和install前应该先运行父项目的clean和install 问题8：Caused by: java.lang.ClassNotFoundException: com.netflix.hystrix.contrib.javanica.aop.aspectj.HystrixCommandAspect解决办法：新增依赖1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;","link":"/2019/05/12/spring-cloud/"},{"title":"spring boot jpa","text":"spring boot jpa jpa 常用于数据库访问 1.依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.gree.bdc&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-jpa&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;jpa-test&lt;/name&gt; &lt;description&gt;jpa-test&lt;/description&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- spring boot begin--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--mysql 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.11&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log4j--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt; &lt;version&gt;1.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring boot jpa --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2.application.properties 配置，创建入口类。12345678910111213#通用数据源配置spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driverspring.datasource.url=jdbc:mysql://localhost:3306/springboot_jpa?charset=utf8mb4&amp;useSSL=false&amp;serverTimezone=GMTspring.datasource.username=rootspring.datasource.password=123456# Hikari 数据源专用配置spring.datasource.hikari.maximum-pool-size=20spring.datasource.hikari.minimum-idle=5# JPA 相关配置spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialectspring.jpa.show-sql=truespring.jpa.hibernate.ddl-auto=create 1234567891011package com.gree.bdc.jpa;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class JpaApplication { public static void main(String[] args) { SpringApplication.run(JpaApplication.class,args); }} 3.创建实体123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.gree.bdc.jpa.entity;import javax.persistence.Column;import javax.persistence.Entity;import javax.persistence.Id;import javax.persistence.Table;@Entity@Table(name = &quot;AUTH_USER&quot;)public class UserDO { @Id private Long id; @Column(length = 32) private String name; @Column(length = 32) private String account; @Column(length = 64) private String pwd; public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getAccount() { return account; } public void setAccount(String account) { this.account = account; } public String getPwd() { return pwd; } public void setPwd(String pwd) { this.pwd = pwd; }} 4.写一个DAO继承Repository或他的子类，执行入口类创建数据库表。后注释掉配置文件 spring.jpa.hibernate.ddl-auto=create 123456789package com.gree.bdc.jpa.repository;import com.gree.bdc.jpa.entity.UserDO;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.stereotype.Repository;@Repositorypublic interface UserDAO extends JpaRepository&lt;UserDO,Long&gt; {} 5.使用DAO调用其操作数据库方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.gree.bdc.jpa;import com.gree.bdc.jpa.entity.UserDO;import com.gree.bdc.jpa.repository.UserDAO;import org.junit.After;import org.junit.Before;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;import java.util.List;@RunWith(SpringRunner.class)@SpringBootTestpublic class UserDOTest { @Autowired private UserDAO userDAO; @Before public void before(){ UserDO userDO = new UserDO(); userDO.setId(1L); userDO.setName(&quot;风清扬&quot;); userDO.setAccount(&quot;fengqy&quot;); userDO.setPwd(&quot;123456&quot;); userDAO.save(userDO); userDO = new UserDO(); userDO.setId(3L); userDO.setName(&quot;东方不败&quot;); userDO.setAccount(&quot;bubai&quot;); userDO.setPwd(&quot;123456&quot;); userDAO.save(userDO); userDO.setId(5L); userDO.setName(&quot;向问天&quot;); userDO.setAccount(&quot;wentian&quot;); userDO.setPwd(&quot;123456&quot;); userDAO.save(userDO); } @Test public void testAdd(){ UserDO userDO = new UserDO(); userDO.setId(2L); userDO.setName(&quot;任我行&quot;); userDO.setAccount(&quot;renwox&quot;); userDO.setPwd(&quot;123456&quot;); userDAO.save(userDO); userDO = new UserDO(); userDO.setId(4L); userDO.setName(&quot;令狐冲&quot;); userDO.setAccount(&quot;linghuc&quot;); userDO.setPwd(&quot;123456&quot;); userDAO.save(userDO); } @After public void after(){ userDAO.deleteById(1L); List&lt;UserDO&gt; users = userDAO.findAll(); for(UserDO user: users){ System.out.println(user.getName()); } }} 参考文档：spring boot jpa","link":"/2019/11/30/spring-boot-jpa/"},{"title":"spring 注解 笔记","text":"@Controller和@RestController的区别知识点：@RestController注解相当于@ResponseBody ＋ @Controller合在一起的作用。 1) 如果只是使用@RestController注解Controller，则Controller中的方法无法返回jsp页面，或者html，配置的视图解析器 InternalResourceViewResolver不起作用，返回的内容就是Return 里的内容。 2) 如果需要返回到指定页面，则需要用 @Controller配合视图解析器InternalResourceViewResolver才行。 如果需要返回JSON，XML或自定义mediaType内容到页面，则需要在对应的方法上加上@ResponseBody注解。 例如： 1.使用@Controller 注解，在对应的方法上，视图解析器可以解析return 的jsp,html页面，并且跳转到相应页面 若返回json等内容到页面，则需要加@ResponseBody注解 @RequestMapping 和 @GetMapping @PostMapping 区别@GetMapping是一个组合注解，是@RequestMapping(method = RequestMethod.GET)的缩写。 @PostMapping是一个组合注解，是@RequestMapping(method = RequestMethod.POST)的缩写。","link":"/2019/04/04/spring-注解-笔记/"},{"title":"修炼指南","text":"MAS学习法 认知：我们只有把知识转化为自己的语言，它才真正编程我们自己的东西。这个转换的过程就叫做认知的过程。如何提高我们的学习吸收能力，就要做到知行合一。如果说认知是大脑，那么工具就是我们的双手。所以我们需要把握两点原则: 1、不要重复造轮子 也即尽量选择已有的第三方工具来完成我们的项目，因为大部分的业务场景都能找到相关的工具来解决，这样既省时又省力。2、工具决定效率 在工作中选择工具的原则是选择使用者最多的工具。因为：Bug少、文档全、案例多。同时找到适合的工具，这样就能大大提高我们的工作效率。 选择好工具后就需要大量的积累。一般而言，我们很难记住大段的知识和相关指令。但是我们能够记住自己曾经做过的相关的项目，题目和故事。这就需要多练，练熟练透它。正所谓熟能生巧。量变引起质变。 总结一下几点 1、记录下自己每天的认知2、这些认知对应工具的那些操作3、勤加练习，巩固知识。","link":"/2019/02/18/修炼指南/"},{"title":"图数据集群搭建","text":"** NEO4J高可用集群搭建 ** 高可用的neo4j集群主要采用了主从的结构，来保证集群的容错能力和应变能力，同时也保证了了集群在读取密集型的数据的场景下可横向的扩展能力。同时，它还支持缓存分区，使得NEO4J高可用性集群比neo4j单实例具有更大的负载能力。但HA集群很快要不支持了。 好了，话不多说，如果看过前一篇文章https://blog.csdn.net/fffsssfff6/article/details/81215416, 完成了前半部分的一些基本准备，那么就可以直接进行HA集群搭建。若没有准备，则需要完成至JDK安装的步骤。下面就开始了： 一、首先 下载neo4j企业版的安装包。可以参考https://blog.csdn.net/xubo245/article/details/50033003. 执行下面命令123456789101112 http://dist.neo4j.org/neo4j-enterprise-3.4.0-unix.tar.gz``` 或者前往neo4j官网下载 ：https://neo4j.com/download/ .二、然后将安装包解压后分别传入到 /opt/neo4j 目录下。``` bashtar -zxvf neo4j-enterprise-3.4.0-unix.tar.gzscp -r neo4j-enterprise-3.4.0 root@master: /opt/neo4jscp -r neo4j-enterprise-3.4.0 root@slave1: /opt/neo4jscp -r neo4j-enterprise-3.4.0 root@slave2: /opt/neo4j 三 、修改配置文件neo4j.conf(重要) master节点：12345dbms.mode=HAha.server_id=1ha.initial_hosts=172.16.247.135:5001,172.16.247.132:5001,172.16.247.136:5001dbms.connectors.default_listen_address=0.0.0.0 slave1节点： 12345dbms.mode=HAha.server_id=2ha.initial_hosts=172.16.247.135:5001,172.16.247.132:5001,172.16.247.136:5001dbms.connectors.default_listen_address=0.0.0.0 slave2节点： 1234dbms.mode=HAha.server_id=3ha.initial_hosts=172.16.247.135:5001,172.16.247.132:5001,172.16.247.136:5001dbms.connectors.default_listen_address=0.0.0.0 四、启动HA集群,分别进入neo4j 目录下执行 1234./bin/neo4j start./bin/neo4j start./bin/neo4j start 五、进入localhost：7474查看集群信息","link":"/2018/08/14/图数据集群搭建/"},{"title":"数据分析全景图笔记","text":"数据分析的三个重要组成部分： 1、数据采集。（源头） 2、数据挖掘。（核心，主要是挖掘数据的商业价值） 3、数据可视化。（直观感受数据分析结果） 数据采集 主要是各种各样的数据源打交道，然后使用不用的工具来进行采集。 数据挖掘 第二部分主要熟悉数据挖掘的基本流程、十大算法、以及背后的数学基础。 数据可视化 数据可视化可以帮我们很好的理解数据的结构，以及分析结果的呈现。主要有两种方法实现数据的可视化。 第一种就是使用Python。在Python对数据进行清洗、挖掘的过程中，可以使用Matplotlib,Seaborn等第三方库进行呈现。 第二种就是使用第三方工具。若已经生成了csv文件，想采用所见即所得的方式呈现，可以采用微图、DataV、Data GIF Maker等第三方工具。","link":"/2019/02/18/数据分析全景图笔记/"},{"title":"数据挖掘实战（2）： 信用卡诈骗分析","text":"信用卡诈骗分析目标：通过以往的交易数据分析出每笔交易是否正常，是否存在盗刷的风险。学习目标： 学习逻辑回归 信用卡欺诈属于二分类问题，欺诈交易在所有交易中比例很小，对于这种数据不平衡情况，到底采用什么样的模型评估标准更准确； 完成信用卡欺诈分析实战项目，并通过数据可视化对数据探索和模型评估进一步加强了解。 构建逻辑回归分类器 逻辑回归（logistic回归），主要解决二分类问题。在逻辑回归中使用Logistic函数也称Sigmoid函数。 函数形如S状： 算法实现理解：我们要实现一个二分类任务，0为不发生，1为发生。而通过对历史样本学习得到一个新的模型，当新的样本给出时，预测出结果。这里得到是y的一个预测概率。通常我们认为概率大于50%则发生，反之，则不发生。 在sklearn中，使用LogisticRegression()函数构建逻辑回归分类器，函数常用的构造参数： penalty ：惩罚项，取值为l1或l2，默认为l2。当模型参数满足高斯分布时用l2，当模型参数满足拉普拉斯分布时用l1; solver: 代表的是逻辑回归损失函数的优化方法。有5个参数可选，分别为liblinear、lbfgs、newton-cg、sag或saga.默认为liblinear,s适用于数据量较小的数据集，当数据量较大的时候可以选用sag或saga方法。 max_iter:算法收敛最大迭代次数，默认为10. n_jobs:拟合和预测的时候CPU的核数，默认为1.","link":"/2019/03/15/数据挖掘实战（2）：-信用卡诈骗分析/"},{"title":"学习笔记","text":"chcp 查看windows系统编码格式 chcp 65001 将Windows格式编码设为utf-8模式 设置 hue 指定队列进行查询操作 set mapred.job.queue.name=QueueA; 设置 hue 使用 spark 引擎查询 set spark.master=yarn-cluster; //默认即为yarn-cluster模式，该参数可以不配置set hive.execution.engine=spark;set spark.eventLog.enabled=true;set spark.eventLog.dir=hdfs://cdh5/tmp/sparkeventlog;set spark.executor.memory=1g;set spark.executor.instances=50; //executor数量，默认貌似只有2个set spark.driver.memory=1g;set spark.serializer=org.apache.spark.serializer.KryoSerializer; host文件配置： C:\\Windows\\System32\\drivers\\etc 后台运行Java程序nohup java -jar basedata-1.0-SNAPSHOT.jar &amp; 7z Linux 解压命令7za x ***.7z restful api linux 测试get方法 ： curl “http://sysapp.gree.com/GreeMesOpenApi/GreeMesApi/api/services/app/SupTimeZoneJXDetailService/GetSupTimeZoneJXDetail?mlotno=&quot;9840021904060061&quot;&quot;post方法 ： curl -d ‘info={“mlotno”:”9840021904060061”}’ http://sysapp.gree.com/GreeMesOpenApi/GreeMesApi/api/services/app/SupTimeZoneJXDetailService/GetSupTimeZoneJXDetail 注意空格。 在windows下执行java包命令1java -jar xx.jar 后台执行命令123将jdk 重新命名项目名（如bom.exe）bom.exe -jar xx.jar","link":"/2019/01/18/学习笔记/"},{"title":"朴素贝叶斯","text":"使用概率分布进行分类 学习朴素贝叶斯分类器 解析RSS源数据 使用朴素贝叶斯来分析不同地区的态度通用贝叶斯公式 朴素贝叶斯优点：在数据较少的情况下仍然有效，可以处理多类别问题。缺点：对于输入数据的准备方式较为敏感适应数据类型：标称型数据贝叶斯决策理论的核心思想：选择高概率所对应的类别朴素贝叶斯它是一个简单但是及其强大的预测算法，之所以称为朴素贝叶斯是因为他有个非常强硬的假设，即它假设输入的每个变量都是独立的 。条件概率： 朴素贝叶斯文档分类应用一般过程 收集数据 准备数据：需要数值型或者布尔型数据 分析数据：在有大量特征时，绘制特征作用不大，此时使用直方图效果更好 训练算法：计算不同的独立特征的条件概率 测试算法：计算错误率 使用算法：一个常见的朴素贝叶斯应用是文档分类。 程序清单 1 ： 词表到向量的转换函数 123456789101112131415161718192021def loadDataSet(): postingList = [[&apos;my&apos;,&apos;dog&apos;,&apos;has&apos;,&apos;flea&apos;,\\ &apos;problems&apos;,&apos;help&apos;,&apos;please&apos;], [&apos;maybe&apos;,&apos;not&apos;,&apos;take&apos;,&apos;him&apos;,\\ &apos;I&apos;,&apos;love&apos;,&apos;him&apos;]]classVec = [0,1] # 1代表有侮辱性文字，0代表正常言论return postingList,classVecdef createVocabList(dataSet): vocabSet = set([]) for document in dataSet: vocabSet = vocabSet | set(document) return list(vocabSet)def setOfWords2Vec(vocabList,inputSet):returnVec = [0]*len(vocabList)for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else:print &quot;the Word : %s is not in my Vocabulary!&quot; % wordreturn returnVec 训练算法函数伪代码：123456789计算每个类别中的文档数目对每篇训练文档： 对每个类别： 如果词条出现文档中 --&gt; 增加该词条的计数值 增加所有词条的计数值 对每个类别： 对每个词条： 将该词条的数目除以总词条数目得到条件概率 返回每个类别的条件概率 程序清单2 朴素贝叶斯分类器训练函数123456789101112131415161718def trainNB0(trainMatrix,trainCategory): # 初始化概率 numTrainDocs = len(trainMatrix) numWords = len(trainMatrix[0]) pAbusive = sum(trainCategory)/float(numTrainDocs) p0Num = zeros(numWords);plNum = zeros(numWords) p0Denom = 0.0;plDenom = 0.0 # 向量相加 for i in range(numTrainDocs): if trainCategory[i] ==1; p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = p1Num/p1Denom p0Vect = p0Num/p0Denom return p0Vect,p1Vect,pAbusive 简单案例离散数据案例（判断性别【高，中，中】） 可知 共有三个属性，用A代表属性，A1,A2,A3分别表示 身高 = 高、体重 = 中、鞋码 = 中。 共有两个类别，用C代表类别，C1,C2 分别表示为 男、女，未知情况用C~j~ 表示。补充： （1）条件概率： P(A|B)=P(AB)/P(B) （P(B)&gt;0） (2) 乘法公式： - P(AB)=P(A|B)P(B)=P(B|A)P(A) - P(A1A2...An-1An)=P(A1)P(A2|A1)P(A3|A1A2)...P(An|A1A2...An-1) （n≥2，P(A1A2...An-1) &gt; 0） （3）全概率公式: 如果事件组B1，B2，…. 满足 1.B1，B2….两两互斥，即 Bi ∩ Bj = ∅ ，i≠j ， i,j=1，2，….，且P(Bi)&gt;0,i=1,2,….; 2.B1∪B2∪….=Ω ，则称事件组 B1,B2,…是样本空间Ω的一个划分 设 B1,B2,…是样本空间Ω的一个划分，A为任一事件，则： 现在需要求得在 A1,A2,A3 下 C~j~ 的概率： 由于P(A1A2A3) 固定，故求P(C~j~|A1A2A3)最大等价于P(A1A2A3|C~j~)P(C~j~)最大值。","link":"/2019/03/13/朴素贝叶斯/"},{"title":"数据挖掘实战（1） ： 信用卡违约率分析","text":"数据挖掘常遇见的问题，也是核心问题： 如何选择各种分类器，到底选择哪个分类算法？ 如何优化分类器参数，以便得到更好的分类器？ 三个目标： 创建各种分类器，包括已经掌握的SVM、决策树、KNN分类器，以及随机森林分类器； 掌握GridSearchCV工具，优化算法模型的参数； 使用Pipeline 管道机制进行流水线作业。（数据规范或者数据降维） 构建随机森林分类器 随机森林（Random Forest,简称 RF），他实际是一个包含多个决策树分类器，每个子分类器都是一颗CART分类回归树。所以随机森林既可以做分类也可以做回归任务。做分类时，输出的结果是每个子分类器中分类结果最多的那个；做回归时，输出的结果是每个子分类的任务输出结果的平均值。在sklearn中，使用RandomForestClassifier()构造随机森林模型，常用的参数有： 使用 GridSearchCV 工具对模型参数进行调优 GridSearchCV 是Python 的参数自动搜索模块，只要告诉它想要的调优的参数有哪些以及参数的取值范围，它就会把所以的情况都跑一遍，然后告诉我们哪个参数最优，结果如何。GridSearchCV(estimator,param_grid,cv = None,scoring=None)构造参数的自动搜索模块，主要参数说明； 简单例子：12345678910from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_irisrf = RandomForestClassifier()parameters = {&quot;n_estimators&quot;:range(1,11)}iris = load_iris()clf = GridSearchCV(estimator=rf,param_grid=parameters,cv =5)clf.fit(iris.data,iris.target)print(&quot;最优分数：%.4lf&quot; %clf.best_score_)print(&quot;最优参数：&quot;,clf.best_params_) 使用 Pipeline 管道机制进行流水作业Python 有一种 Pipeline 管道机制。管道机制就是让我们把每一步都按照顺序下来，从而创建Pipeline流水作业。每一步都采用（’名称’，步骤）的方式来表示。 下面使用Pipeline管道机制，用随机森林对iris数据集做分类任务。先用StandardScaler方法对数据规范化，然后使用随机森林分类： 123456789101112131415161718192021from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_irisfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipelinerf = RandomForestClassifier()parameters = {&quot;randomforestclassifier__n_estimators&quot;: range(1,11)}iris = load_iris()pipeline = Pipeline([ (&apos;scaler&apos;,StandardScaler()), (&apos;randomforestclassifier&apos;,rf) ]) clf = GridSearchCV(estimator=pipeline,param_grid=parameters)clf.fit(iris.data,iris.target)print(&quot;最优分数：%.4lf&quot; %clf.best_score_)print(&quot;最优参数：&quot;,clf.best_params_) 对信用卡违约率进行分析 数据来源github上下载即可 ：信用卡违约数据这个数据集是台湾某银行05年4月到9月的信用卡数据，数据集一共25个字段，具体含义如下： 现在针对这个数据集构建一个信用卡违约率分类器。项目的流程如下： 加载数据 准备阶段：探索数据，采用数据可视化方式可以让我们对数据有更加直观的了解 分类阶段：之所以把数据规范化放到这个阶段，是因为我们采用了Pipeline 管道机制。而为了找到合适的分类算法以及相适应的分类器参数，需要多试几个分类器以及采用GridSearchCV工具找到最优参数具体代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# -*- coding: utf-8 -*-# 信用卡违约率分析import pandas as pdfrom sklearn.model_selection import learning_curve, train_test_split,GridSearchCVfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipelinefrom sklearn.metrics import accuracy_scorefrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom matplotlib import pyplot as pltimport seaborn as sns# 数据加载data = data = pd.read_csv(&apos;./UCI_Credit_Card.csv&apos;)# 数据探索print(data.shape) # 查看数据集大小print(data.describe()) # 数据集概览# 查看下一个月违约率的情况next_month = data[&apos;default.payment.next.month&apos;].value_counts()print(next_month)df = pd.DataFrame({&apos;default.payment.next.month&apos;: next_month.index,&apos;values&apos;: next_month.values})plt.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;] # 用来正常显示中文标签plt.figure(figsize = (6,6))plt.title(&apos;信用卡违约率客户\\n (违约：1，守约：0)&apos;)sns.set_color_codes(&quot;pastel&quot;)sns.barplot(x = &apos;default.payment.next.month&apos;, y=&quot;values&quot;, data=df)locs, labels = plt.xticks()plt.show()# 特征选择，去掉 ID 字段、最后一个结果字段即可data.drop([&apos;ID&apos;], inplace=True, axis =1) #ID 这个字段没有用target = data[&apos;default.payment.next.month&apos;].valuescolumns = data.columns.tolist()columns.remove(&apos;default.payment.next.month&apos;)features = data[columns].values# 30% 作为测试集，其余作为训练集train_x, test_x, train_y, test_y = train_test_split(features, target, test_size=0.30, stratify = target, random_state = 1) # 构造各种分类器classifiers = [ SVC(random_state = 1, kernel = &apos;rbf&apos;), DecisionTreeClassifier(random_state = 1, criterion = &apos;gini&apos;), RandomForestClassifier(random_state = 1, criterion = &apos;gini&apos;), KNeighborsClassifier(metric = &apos;minkowski&apos;),]# 分类器名称classifier_names = [ &apos;svc&apos;, &apos;decisiontreeclassifier&apos;, &apos;randomforestclassifier&apos;, &apos;kneighborsclassifier&apos;,]# 分类器参数classifier_param_grid = [ {&apos;svc__C&apos;:[1], &apos;svc__gamma&apos;:[0.01]}, {&apos;decisiontreeclassifier__max_depth&apos;:[6,9,11]}, {&apos;randomforestclassifier__n_estimators&apos;:[3,5,6]} , {&apos;kneighborsclassifier__n_neighbors&apos;:[4,6,8]},] # 对具体的分类器进行 GridSearchCV 参数调优def GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, param_grid, score = &apos;accuracy&apos;): response = {} gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = score) # 寻找最优的参数 和最优的准确率分数 search = gridsearch.fit(train_x, train_y) print(&quot;GridSearch 最优参数：&quot;, search.best_params_) print(&quot;GridSearch 最优分数： %0.4lf&quot; %search.best_score_) predict_y = gridsearch.predict(test_x) print(&quot; 准确率 %0.4lf&quot; %accuracy_score(test_y, predict_y)) response[&apos;predict_y&apos;] = predict_y response[&apos;accuracy_score&apos;] = accuracy_score(test_y,predict_y) return response for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid): pipeline = Pipeline([ (&apos;scaler&apos;, StandardScaler()), (model_name, model) ]) result = GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, model_param_grid , score = &apos;accuracy&apos;)","link":"/2019/03/14/数据挖掘实战（1）-：-信用卡违约率分析/"},{"title":"校验工具类","text":"1、异或校验1234567ByteBuf out = Unpooled.buffer(); byte[] b = new byte[out.readableBytes()];// 进行异或校验byte temp=b[3]; for (int i = 4; i &lt;b.length-1; i++) { temp ^=b[i]; }","link":"/2019/06/15/校验工具类/"},{"title":"日志切割脚本","text":"123456789101112131415161718192021222324252627282930313233#!/bin/shlog_dir=&apos;/home/lyxbdw/basedata/server/split&apos;monitor_file=$1echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;` &apos;初始化切割日志文件为：&apos; $monitor_file &gt;&gt; /home/lyxbdw/basedata/server/split/split.logif [ ! $1 ]then echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;输入参数为空，使用默认路径：/home/lyxbdw/basedata/server/basedata.log&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log monitor_file=/home/lyxbdw/basedata/server/basedata.log #文件的绝对路径filog_name=`basename $monitor_file`echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;切割文件名称为：&apos;$log_name &gt;&gt; /home/lyxbdw/basedata/server/split/split.logfile_size=`du $monitor_file | awk &apos;{print $1}&apos;`echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;切割文件大小为：&apos;$file_size &gt;&gt; /home/lyxbdw/basedata/server/split/split.logif [ $file_size -ge 1024000 ]then echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;文件大小大于 1024000 开始切割&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log if [ ! -d $log_dir ] then echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;日志切割路径不存在，新建....&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log mkdir /home/lyxbdw/basedata/server/split #创建保存切割文件目录,这个路径可以自行修改，保存到你想要的目录 fi echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;开始切割日志....&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;开始切割日志....&apos; cp $monitor_file /home/lyxbdw/basedata/server/$log_name-`date +%Y%m%d%H%M%S`.log #保存日志文件 echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;日志切割完成。&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;日志切割完成。&apos; echo `date &apos;+%Y-%m-%d-%H:%M:%S&apos;`&apos;文件切割&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log #记录切割日志 echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;开始清空源文件...&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;开始清空源文件...&apos; echo &quot;&quot; &gt; $monitor_file #清空tomcat的log/catalina.out文件内容 echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;清空源文件完成。&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;清空源文件完成。&apos;fi","link":"/2019/07/02/日志切割脚本/"},{"title":"时间工具类","text":"::: hljs-center 时间工具类 ::: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200package com.gree.cn.basedata.utils;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Calendar;import java.util.Date;/** * 日期时间工具类 * @author Administrator * */public class DateUtils { public static final SimpleDateFormat TIME_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); public static final SimpleDateFormat DATE_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); public static final SimpleDateFormat DATEKEY_FORMAT = new SimpleDateFormat(&quot;yyyyMMdd&quot;); public static final SimpleDateFormat DATE_NOW = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;); /** * 获取某个时刻的时间信息 * */ public static String getTimeNow(){ return DATE_NOW.format(new Date()); } /** * 获取格式化某个时间 */ public static String getFormatTimeNow(){ return TIME_FORMAT.format(new Date()); } /** * 判断一个时间是否在另一个时间之前 * @param time1 第一个时间 * @param time2 第二个时间 * @return 判断结果 */ public static boolean before(String time1, String time2) { try { Date dateTime1 = TIME_FORMAT.parse(time1); Date dateTime2 = TIME_FORMAT.parse(time2); if(dateTime1.before(dateTime2)) { return true; } } catch (Exception e) { e.printStackTrace(); } return false; } /** * 判断一个时间是否在另一个时间之后 * @param time1 第一个时间 * @param time2 第二个时间 * @return 判断结果 */ public static boolean after(String time1, String time2) { try { Date dateTime1 = TIME_FORMAT.parse(time1); Date dateTime2 = TIME_FORMAT.parse(time2); if(dateTime1.after(dateTime2)) { return true; } } catch (Exception e) { e.printStackTrace(); } return false; } /** * 计算时间差值（单位为秒） * @param time1 时间1 * @param time2 时间2 * @return 差值 */ public static int minus(String time1, String time2) { try { Date datetime1 = TIME_FORMAT.parse(time1); Date datetime2 = TIME_FORMAT.parse(time2); long millisecond = datetime1.getTime() - datetime2.getTime(); return Integer.valueOf(String.valueOf(millisecond / 1000)); } catch (Exception e) { e.printStackTrace(); } return 0; } /** * 获取年月日和小时 * @param datetime 时间（yyyy-MM-dd HH:mm:ss） * @return 结果（yyyy-MM-dd_HH） */ public static String getDateHour(String datetime) { String date = datetime.split(&quot; &quot;)[0]; String hourMinuteSecond = datetime.split(&quot; &quot;)[1]; String hour = hourMinuteSecond.split(&quot;:&quot;)[0]; return date + &quot;_&quot; + hour; } /** * 获取当天日期（yyyy-MM-dd） * @return 当天日期 */ public static String getTodayDate() { return DATE_FORMAT.format(new Date()); } /** * 获取昨天的日期（yyyy-MM-dd） * @return 昨天的日期 */ public static String getYesterdayDate() { Calendar cal = Calendar.getInstance(); cal.setTime(new Date()); cal.add(Calendar.DAY_OF_YEAR, -1); Date date = cal.getTime(); return DATE_FORMAT.format(date); } /** * 格式化日期（yyyy-MM-dd） * @param date Date对象 * @return 格式化后的日期 */ public static String formatDate(Date date) { return DATE_FORMAT.format(date); } /** * 格式化时间（yyyy-MM-dd HH:mm:ss） * @param date Date对象 * @return 格式化后的时间 */ public static String formatTime(Date date) { return TIME_FORMAT.format(date); } /** * 解析时间字符串 * @param time 时间字符串 * @return Date */ public static Date parseTime(String time) { try { return TIME_FORMAT.parse(time); } catch (ParseException e) { e.printStackTrace(); } return null; } /** * 格式化日期key * @param date * @return */ public static String formatDateKey(Date date) { return DATEKEY_FORMAT.format(date); } /** * 格式化日期key * @param datekey * @return */ public static Date parseDateKey(String datekey) { try { return DATEKEY_FORMAT.parse(datekey); } catch (ParseException e) { e.printStackTrace(); } return null; } /** * 格式化时间，保留到分钟级别 * yyyyMMddHHmm * @param date * @return */ public static String formatTimeMinute(Date date) { SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMddHHmm&quot;); return sdf.format(date); }}","link":"/2019/06/14/时间工具类/"},{"title":"蓝牙信标定位","text":"::: hljs-center 蓝牙信标定位::: 一、蓝牙信标信号强度转换 计算公式： d = 10^((abs(RSSI) - A) / (10 * n))其中：d - 计算所得距离RSSI - 接收信号强度（负值）A - 发射端和接收端相隔1米时的信号强度 n - 环境衰减因子 代码实现：123//信号强度转距离Integer signal_power = (Integer.valueOf(values.substring(34, 36), 16).shortValue()) -256 ;Double mesure_distinct = Math.pow(10, (Math.abs(signal_power) - 59) / (10 * 2.0)); 二 、base数据执行程序 1.基站数据分析 cd /home/lyxbdw/basedata/comsumer/target/java -jar analysisbasedata-1.0-SNAPSHOT.jar//后台运行程序nohup java -jar analysisbasedata-1.0-SNAPSHOT.jar &amp; 2.基站数据接收 cd /home/lyxbdw/basedata/server/target/java -jar basedata-1.0-SNAPSHOT.jar//后台运行nohup java -jar basedata-1.0-SNAPSHOT.jar &amp; 基站分析数据 nohup java -jar /home/lyxbdw/basedata/comsumer/analysisbasedata-0.0.1-SNAPSHOT.jar &gt; /home/lyxbdw/basedata/comsumer/analysisbasedata.log &amp; 基站接收数据 nohup java -jar /home/lyxbdw/basedata/server/basedata-0.0.1-SNAPSHOT.jar &gt; /home/lyxbdw/basedata/server/basedata.log &amp; 3.Linux jar 包后台运行12#!/bin/bashnohup java -jar rocketmq-console-ng-1.0.0.jar --server.port=12581 --rocketmq.config.namesrvAddr=127.0.0.1:9876 &gt; log.out &amp; 解释：1、目的是使rocketmq-console-ng-1.0.0.jar在后台运行；2、nohup 就是linux后台挂起的命令3、“&gt; log.out” 指定日志输出位置为log.out","link":"/2019/05/23/蓝牙信标定位/"},{"title":"蓝牙信标定位项目总结","text":"一、项目简介： 蓝牙信标定位项目又称物流容器信息化管理系统。主要是为了解决缩短物理配送周期，提高物流配送效率问题，采用蓝牙定位技术,实时定位小车的位置以及监控小车的状态信息，达到及时优化和调整配送流程的效果。以下是蓝牙基站数据处理时序图： 数据处理流程： 作为服务端，需要接受来自客户端tcp/ip数据。(netty) 数据接收并校验成功后作出相应的动作，一部分数据需要返回时间戳，所有校验成功的数据都写入kafka。(kafka producer) 消费kafka中的数据，主要有三个部分，一是信标电量信息采集，二是基站状态信息的采集，三是小车位置的计算和物料信息的绑定（structured streaming/flink） 读取redis小车位置以及状态信息，通过websocket每隔五秒准实时推送一次最新数据到前端。(websocket) 说明：1.使用netty作为数据接入使用可以参考：netty整合spring boot2.数据接收后需要写入kafka集群中，关于kafka集群部署可以参考：kafka集群安装部署3.kafka安装后，数据写入kafka,需要创建kafka producer，可以参考：kafka相关操作4.读取kafka数据，并做相应的计算，需要用到structed streaming/flink技术，相应的需要部署spark 集群或flink集群，spark集群部署可以参考：spark 集群安装部署5.structed streaming 读取kafka 数据并做实时计算可以参考 ：structed streaming 读取kafka 数据","link":"/2019/08/26/蓝牙信标定位项目总结/"},{"title":"数据挖掘实战（3） ： 如何对比特币走势进行预测","text":"时间序列分析： 时间序列分析模型建立了观察结果与时间变化的关系，能够帮助我们预测未来一段时间内的结果变化情况。 需要掌握的目标： 了解时间序列预测的概念，以及常用的算法，包括AR、MA、ARMA、ARIMA模型等； 掌握并使用ARMA模型工具，对一个时间序列数据进行建模和预测； 对比特币的历史数据进行时间序列建模，并预测未来6个月的走势。 时间序列预测 关于时间序列，可以简单理解为按照时间组成的数字序列。在时间序列预测模型中，经典模型有：AR、MA、ARIMA、ARMA。 AR(Auto Regressive) 又叫自回归模型。它认为过去若干个时刻的点通过线性组合，再加上白噪声就可以预测未来某个时刻的点。白噪声可以理解成一个期望为0，方差为常数的纯随机过程。AR模型还存在一个阶数，称为 AR（p）模型，也叫做 p 阶自回归模型。他指的是通过这个时刻的前p个点，通过线性组合再加上白噪声来预测当前时刻点的值。 MA（Moving Average）,又叫滑动平均模型。它与 AR模型大同小异，AR模型是历史时序值的线性组合，MA是通过历史白噪声点进行线性组合来 影响当前时刻点。同样MA模型也存在一个阶数，称为MA(q)阶模型，也叫做q阶移动平均模型。 ARMA（Auto Regressive Moving Average）,又叫自回归滑动平均模型，也是AR和MA两者的混合。相较于ARMA,ARIMA多了个差分的过程，作用是对不平稳数据进行差分平稳，在差分平稳后再进行建模。ARMA(p,q) 两个阶数，ARIMA（p,d,q）三个阶数，其中d 表示差分阶数。 ARMA 模型工具 引入相关工具包1from statsmodels.tsa.arima_model import ARMA 然后通过ARMA(endog,order,exog=None)创建ARMA类，参数说明： endog: endogenous variable,代表内生变量，又叫非政策性变量，它由模型决定，不被政策左右，可以说是我们想要分析的变量，或者说是我们这次项目需要用到的变量。 order ：代表是p和q的值，也就是ARMA中的阶数。 exog: exogenous variables,代表外生变量。外生变量和内生变量一样是经济模型中重要变量。相较于内生变量而言，外生变量有称为政策性变量，在经济机制内受外部的影响，不是我们模型要研究的变量。 代码例子：12345678910111213141516171819202122232425262728import pandas as pdimport matplotlib.pyplot as pltimport statsmodels.api as smfrom statsmodels.tsa.arima_model import ARMAfrom statsmodels.graphics.api import qqplot# 创建数据data = [5922, 5308, 5546, 5975, 2704, 1767, 4111, 5542, 4726, 5866, 6183, 3199, 1471, 1325, 6618, 6644, 5337, 7064, 2912, 1456, 4705, 4579, 4990, 4331, 4481, 1813, 1258, 4383, 5451, 5169, 5362, 6259, 3743, 2268, 5397, 5821, 6115, 6631, 6474, 4134, 2728, 5753, 7130, 7860, 6991, 7499, 5301, 2808, 6755, 6658, 7644, 6472, 8680, 6366, 5252, 8223, 8181, 10548, 11823, 14640, 9873, 6613, 14415, 13204, 14982, 9690, 10693, 8276, 4519, 7865, 8137, 10022, 7646, 8749, 5246, 4736, 9705, 7501, 9587, 10078, 9732, 6986, 4385, 8451, 9815, 10894, 10287, 9666, 6072, 5418]data = pd.Series(data)data_index = sm.tsa.datetools.dates_from_range(&apos;1901&apos;,&apos;1990&apos;)# 绘制数据图data.index = pd.Index(data_index)data.plot(figsize=(12,8))plt.show()# 创建 ARMA 模型arma = ARMA(data,(7,0)).fit()print(&apos;AIC: %0.4lf&apos; %arma.aic)# 模型预测predict_y = arma.predict(&apos;1990&apos;,&apos;2000&apos;)# 预测结果绘制fig,ax = plt.subplots(figsize=(12,8))ax = data.ix[&apos;1901&apos;:].plot(ax=ax)predict_y.plot(ax= ax)plt.show() 如何判断一个模型是否合适？ 需要引入AIC准则，也叫赤池准则，数值越小代表模型拟合的越好。 比特币预测123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# -*- coding: utf-8 -*-# 比特币走势预测，使用时间序列 ARMAimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom statsmodels.tsa.arima_model import ARMAimport warningsfrom itertools import productfrom datetime import datetimewarnings.filterwarnings(&apos;ignore&apos;)# 数据加载df = pd.read_csv(&apos;./bitcoin_2012-01-01_to_2018-10-31.csv&apos;)# 将时间作为 df 的索引df.Timestamp = pd.to_datetime(df.Timestamp)df.index = df.Timestamp# 数据探索print(df.head())# 按照月，季度，年来统计df_month = df.resample(&apos;M&apos;).mean()df_Q = df.resample(&apos;Q-DEC&apos;).mean()df_year = df.resample(&apos;A-DEC&apos;).mean()# 按照天，月，季度，年来显示比特币的走势fig = plt.figure(figsize=[15, 7])plt.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;] # 用来正常显示中文标签plt.suptitle(&apos;比特币金额（美金）&apos;, fontsize=20)plt.subplot(221)plt.plot(df.Weighted_Price, &apos;-&apos;, label=&apos;按天&apos;)plt.legend()plt.subplot(222)plt.plot(df_month.Weighted_Price, &apos;-&apos;, label=&apos;按月&apos;)plt.legend()plt.subplot(223)plt.plot(df_Q.Weighted_Price, &apos;-&apos;, label=&apos;按季度&apos;)plt.legend()plt.subplot(224)plt.plot(df_year.Weighted_Price, &apos;-&apos;, label=&apos;按年&apos;)plt.legend()plt.show()# 设置参数范围ps = range(0, 3)qs = range(0, 3)parameters = product(ps, qs)parameters_list = list(parameters)# 寻找最优 ARMA 模型参数，即 best_aic 最小results = []best_aic = float(&quot;inf&quot;) # 正无穷for param in parameters_list: try: model = ARMA(df_month.Weighted_Price,order=(param[0], param[1])).fit() except ValueError: print(&apos;参数错误:&apos;, param) continue aic = model.aic if aic &lt; best_aic: best_model = model best_aic = aic best_param = param results.append([param, model.aic])# 输出最优模型result_table = pd.DataFrame(results)result_table.columns = [&apos;parameters&apos;, &apos;aic&apos;]print(&apos;最优模型: &apos;, best_model.summary())# 比特币预测df_month2 = df_month[[&apos;Weighted_Price&apos;]]date_list = [datetime(2018, 11, 30), datetime(2018, 12, 31), datetime(2019, 1, 31), datetime(2019, 2, 28), datetime(2019, 3, 31), datetime(2019, 4, 30), datetime(2019, 5, 31), datetime(2019, 6, 30)]future = pd.DataFrame(index=date_list, columns= df_month.columns)df_month2 = pd.concat([df_month2, future])df_month2[&apos;forecast&apos;] = best_model.predict(start=0, end=91)# 比特币预测结果显示plt.figure(figsize=(20,7))df_month2.Weighted_Price.plot(label=&apos;实际金额&apos;)df_month2.forecast.plot(color=&apos;r&apos;, ls=&apos;--&apos;, label=&apos;预测金额&apos;)plt.legend()plt.title(&apos;比特币金额（月）&apos;)plt.xlabel(&apos;时间&apos;)plt.ylabel(&apos;美金&apos;)plt.show()","link":"/2019/03/18/数据挖掘实战（3）-：-如何对比特币走势进行预测/"}],"tags":[{"name":"数据分析","slug":"数据分析","link":"/tags/数据分析/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"决策树","slug":"决策树","link":"/tags/决策树/"},{"name":"ETL","slug":"ETL","link":"/tags/ETL/"},{"name":"NIFI","slug":"NIFI","link":"/tags/NIFI/"},{"name":"Dataframe","slug":"Dataframe","link":"/tags/Dataframe/"},{"name":"容器","slug":"容器","link":"/tags/容器/"},{"name":"IDEA","slug":"IDEA","link":"/tags/IDEA/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"数据库","slug":"数据库","link":"/tags/数据库/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"neo4j","slug":"neo4j","link":"/tags/neo4j/"},{"name":"Scala","slug":"Scala","link":"/tags/Scala/"},{"name":"Spark","slug":"Spark","link":"/tags/Spark/"},{"name":"流处理","slug":"流处理","link":"/tags/流处理/"},{"name":"flink","slug":"flink","link":"/tags/flink/"},{"name":"hive","slug":"hive","link":"/tags/hive/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"jar","slug":"jar","link":"/tags/jar/"},{"name":"kylin","slug":"kylin","link":"/tags/kylin/"},{"name":"消息队列","slug":"消息队列","link":"/tags/消息队列/"},{"name":"数据库 mybatis","slug":"数据库-mybatis","link":"/tags/数据库-mybatis/"},{"name":"正则匹配","slug":"正则匹配","link":"/tags/正则匹配/"},{"name":"netty","slug":"netty","link":"/tags/netty/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"spark","slug":"spark","link":"/tags/spark/"},{"name":"springboot","slug":"springboot","link":"/tags/springboot/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"spring cloud","slug":"spring-cloud","link":"/tags/spring-cloud/"},{"name":"微服务","slug":"微服务","link":"/tags/微服务/"},{"name":"学习","slug":"学习","link":"/tags/学习/"},{"name":"朴素贝叶斯","slug":"朴素贝叶斯","link":"/tags/朴素贝叶斯/"},{"name":"校验","slug":"校验","link":"/tags/校验/"},{"name":"工具类，时间工具类","slug":"工具类，时间工具类","link":"/tags/工具类，时间工具类/"},{"name":"蓝牙定位","slug":"蓝牙定位","link":"/tags/蓝牙定位/"}],"categories":[{"name":"数据分析","slug":"数据分析","link":"/categories/数据分析/"},{"name":"决策树","slug":"决策树","link":"/categories/决策树/"},{"name":"ETL","slug":"ETL","link":"/categories/ETL/"},{"name":"数据挖掘","slug":"数据分析/数据挖掘","link":"/categories/数据分析/数据挖掘/"},{"name":"DataFrame","slug":"DataFrame","link":"/categories/DataFrame/"},{"name":"python","slug":"数据分析/python","link":"/categories/数据分析/python/"},{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"NIFI","slug":"ETL/NIFI","link":"/categories/ETL/NIFI/"},{"name":"IDEA","slug":"IDEA","link":"/categories/IDEA/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"sql","slug":"sql","link":"/categories/sql/"},{"name":"mysql，数据库","slug":"mysql，数据库","link":"/categories/mysql，数据库/"},{"name":"数据挖掘","slug":"决策树/数据挖掘","link":"/categories/决策树/数据挖掘/"},{"name":"图数据库","slug":"图数据库","link":"/categories/图数据库/"},{"name":"Scala","slug":"Scala","link":"/categories/Scala/"},{"name":"spark","slug":"spark","link":"/categories/spark/"},{"name":"大数据","slug":"大数据","link":"/categories/大数据/"},{"name":"hive","slug":"hive","link":"/categories/hive/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"kafka","slug":"kafka","link":"/categories/kafka/"},{"name":"Java，jar","slug":"Java，jar","link":"/categories/Java，jar/"},{"name":"kylin","slug":"kylin","link":"/categories/kylin/"},{"name":"数据库 ","slug":"数据库","link":"/categories/数据库/"},{"name":"scala","slug":"scala","link":"/categories/scala/"},{"name":"网络编程","slug":"网络编程","link":"/categories/网络编程/"},{"name":"脚本","slug":"脚本","link":"/categories/脚本/"},{"name":"spring boot","slug":"spring-boot","link":"/categories/spring-boot/"},{"name":"spring","slug":"spring","link":"/categories/spring/"},{"name":"消息队列","slug":"kafka/消息队列","link":"/categories/kafka/消息队列/"},{"name":"mybatis","slug":"数据库/mybatis","link":"/categories/数据库/mybatis/"},{"name":"正则匹配","slug":"scala/正则匹配","link":"/categories/scala/正则匹配/"},{"name":"spring cloud ","slug":"spring/spring-cloud","link":"/categories/spring/spring-cloud/"},{"name":"微服务","slug":"spring/spring-cloud/微服务","link":"/categories/spring/spring-cloud/微服务/"},{"name":"学习","slug":"学习","link":"/categories/学习/"},{"name":"数据挖掘算法","slug":"数据挖掘算法","link":"/categories/数据挖掘算法/"},{"name":"校验","slug":"校验","link":"/categories/校验/"},{"name":"shell","slug":"shell","link":"/categories/shell/"},{"name":"工具类，Java","slug":"工具类，Java","link":"/categories/工具类，Java/"},{"name":"蓝牙定位","slug":"蓝牙定位","link":"/categories/蓝牙定位/"}]}