{"pages":[{"title":"关于","text":"","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"02 数据分析实战笔记--学习数据挖掘的最佳路径","text":"数据挖掘的基本流程（六大步骤）：1、商业理解：先从商业的角度理解项目的需求，然后再对数据挖掘的目标进行定义。2、数据理解：尝试收集部分数据，然后对数据进行探索，包括数据描述、数据质量验证3、数据准备：收集数据，并对数据进行清洗、数据集成等操作4、模型建立：选择和应用各种数据挖掘模型，并进行优化5、模型评估：对模型进行评价，并检查构建模型的每个步骤，确认模型是否实现预定的商业目标6、上线发布：项目只有落地实施才能体现价值。当然后序还需要一定的日常运维。 数据挖掘的十大算法按照不同的目的，算法主要分为四类：|分类算法：C4.5,朴素贝叶斯（Naive Bayes）,SVM,KNN,Adaboost,CART|聚类算法：K-Means,EM|关联分析：Apriori|连接分析：PageRank 数据挖掘的数学原理 1、概率论与数理统计2、线性代数3、图论4、最优化方法","link":"/2019/02/18/02-数据分析实战笔记-学习数据挖掘的最佳路径/"},{"title":"17-数据分析实战笔记： 决策树","text":"决策树决策树的工作原理 在现实生活，我们做的各种决策，都是基于以往的经验来判断的。如果将背后的逻辑整理成一个结构图，这实际上就是决策树。上图就是一个典型的决策树。而在实现决策树时会经历两个阶段：构造和剪枝。 决策树的构造优点：计算复杂度不高，输出结果易于理解，对中间值的确失不敏感，可以处理不相干特征数据缺点：易发生过拟合适用数据类型：数值型和标称型创建分支的伪代码函数createBranch()如下所示：检测数据集中的每个子项是否属于同一类： 12345678If so return 类标签：Else 寻找划分数据集的最好特征 划分数据集 创建分支节点 for 每个划分的子集 调用函数createBranch并增加返回结果到分支节点中 return 分支节点 构造 构造的过程就是选择什么样的属性作为节点的过程，一般在构造中存在三种类型的节点：1.根节点：树的顶端，最开始的那个节点2.内部节点：树的中间节点3.叶节点：树最底部的节点，也是决策树结果 构造过程中，需要解决的三个重要问题：1.选择哪个属性作为根节点；2.选择哪些属性作为子节点；3.什么时候停止并得到目标状态，即叶子节点。 决策树的一般流程 （1）收集数据：可以使用任何方法（2）准备数据：树构造算法只适应于标称型数据，因此数值型数据必须离散化（3）分析数据：可以适应任何方法，构造树完成之后，我们应该检查图形是否符合预期。（4）训练算法：构造树的数据结构（5）测试算法：使用经验树计算错误率。（6）使用算法：此步骤可以适应于任何监督学习算法，而使用决策树可以更好的理解数据的含义 信息增益划分数据集的最大原则：将无序的数据变得更加有序。 在划分数据集前后的信息变化称之为信息增益，而我们可以计算每个特征值划分收获的信息增益，获得信息增益最高 的特征就是最好的选择。 熵定义为信息的期望值。如果待分类的事务可能划分在多个分类之中，则xi的信息定义为： 为了计算熵，需要计算所有类别可能包含的信息期望值： 程序清单1 计算给定数据集的香农熵12345678910111213141516from math import logdef calcShannonEnt(dataSet): numEntries = len(dataSet) labelCounts = {} #为所有可能的分类创建字典 for featVec in dataSet: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 # 计算信息期望 for key in labelCounts: prob = float(labelCounts[key])/numEntries shannonEnt -= prob*log(prob,2) return shannonEnt 程序清单2 按照给定的特征划分数据集12345678def splitDataSet(dataSet,axis,value): retDataSet = [] for featVec in dataSet: if featVec[axis] == value： reduceFeatVec = featVec[:axis] reduceFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSet 剪枝 剪枝就是给决策树瘦身。也即不需要过多的判断也能得到不错的结果。主要是为了防止“过拟合（Overfitting）”现象的发生。过拟合现象会导致得到的模型虽然训练结果好，但是泛化能力差。剪枝一般分为两种：“预剪枝”（Pre-Pruning）和“后剪枝”（Post-Pruning）","link":"/2019/02/23/17-数据分析实战笔记：-决策树/"},{"title":"03 数据分析实战45 ： Python 基础语法","text":"Python 基础语法Python 语言特点 Python 语言最大的优点就是简洁，同时有大量的第三方库，功能强大，能够解决数据分析的大部分问题。比如科学计算工具NumPy 和 Pandas库，深度学习工具 Keras 和 TensorFlow,以及机器学习工具 Scikit-learn。 安装及 IDE 环境安装Python 3.x 官网下载安装。Python IDE 推荐使用 PythonCharm Python 基础语法输入输出123sum = 100+100print (&apos;hello,%s&apos; %name) # 打印输出print (&apos;sum = %d&apos; %sum) 判断语句 if … else …score>123456 print (&apos;Excellent&apos;)else: if score &lt; 60: print( &apos;Fail&apos;) else: print (&apos;Good Job&apos;) 循环语句 ： for … in123for number in range(11): sum = sum + numberprint (sum) 循环语句：while12345number = 1while number &lt; 11: sum = sum + number number = number + 1print （sum） 数据类型：列表、元组、字典、集合列表：[]123456lists.append(&apos;d&apos;)print (lists)print (len(lists))lists.insert(0,&apos;mm&apos;)lists.pop()print (lists) 元组（tuple）1print (tuples[0]) 字典（dictionary）-*- coding: utf-8 -*123456789101112# 定义一个 dictionaryscore = {&apos;guanyu&apos;:95,&apos;zhangfei&apos;:96}# 添加一个元素score[&apos;zhaoyun&apos;] = 98print (score)# 删除一个元素score.pop(&apos;zhangfei&apos;)# 查看 key 是否存在print (&apos;guanyu&apos; in score)# 查看一个 key 对应的值print (score.get(&apos;guanyu&apos;))print (score.get(&apos;yase&apos;,99)) 集合1234s.add(&apos;d&apos;)s.remove(&apos;b&apos;)print （s）print （&apos;c&apos; in s） 极客时间版权所有: https://time.geekbang.org/column/article/73574 极客时间版权所有: https://time.geekbang.org/column/article/73574","link":"/2019/03/01/03-数据分析实战45-：-Python-基础语法/"},{"title":"18| 决策树（中） : CART算法笔记","text":"决策树 基于信息度量的不同方式，我们把决策树分为ID3算法、C4.5算法和CART算法。CART算法（Classification And Regression Tree）,又称分类回归树。也就是说CART决策树既可以作为分类树，又可以作为回归树。而且CART只支持二叉树。分类树和回归树的区别分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本类别，而回归树可以对连续型的数值进行预测也就是数据在某个区间都有取值的可能，它输出的是一个数值。 CART分类树工作流程在属性选择上CART算法采用基尼系数作为衡量指标。 假设t为节点，那么该节点是GINI系数为：其中p(Ck|t)表示节点t属于类别Ck的概率，节点t的基尼系数为1减去个类别Ck概率平方和。在CART算法中，基于基尼系数对特征属性进行二元分裂，假设属性A将节点D划分为D1和D2，如下图所示：那么节点D的基尼系数等于子节点D1和D2的归一化基尼系数之和，用公式表示为：归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父节点D中的比例。节点D被属性A划分后基尼系数越大，样本集合的不确定性越大，也就是不纯度越高。 如何使用CART算法来创建分类树123456789101112131415161718192021# encoding=utf-8from sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.tree import DecisionTreeClassifierfrom sklearn.datasets import load_iris# 准备数据集iris=load_iris()# 获取特征集和分类标识features = iris.datalabels = iris.target# 随机抽取 33% 的数据作为测试集，其余为训练集train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=0)# 创建 CART 分类树clf = DecisionTreeClassifier(criterion=&apos;gini&apos;)# 拟合构造 CART 分类树clf = clf.fit(train_features, train_labels)# 用 CART 分类树做预测test_predict = clf.predict(test_features)# 预测结果与测试集结果作比对score = accuracy_score(test_labels, test_predict)print(&quot;CART 分类树准确率 %.4lf&quot; % score) CART回归树工作流程 在CART回归树中，通过样本的混乱程度，也就是样本的离散程度来评价“不纯度”。设x为样本的个体，均值为u。可以通过取差值的绝对值或者方差来评价。差值绝对值：方差：以上两种节点划分标准，分别对应着两种目标函数最优化的标准。最小绝对偏差（LAD）和最小二乘偏差（LSD）。 CART回归树预测例子：波士顿房价预测123456789101112131415161718192021222324# encoding=utf-8from sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import load_bostonfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_errorfrom sklearn.tree import DecisionTreeRegressor# 准备数据集boston=load_boston()# 探索数据print(boston.feature_names)# 获取特征集和房价features = boston.dataprices = boston.target# 随机抽取 33% 的数据作为测试集，其余为训练集train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=0.33)# 创建 CART 回归树dtr=DecisionTreeRegressor()# 拟合构造 CART 回归树dtr.fit(train_features, train_price)# 预测测试集中的房价predict_price = dtr.predict(test_features)# 测试集的结果评价print(&apos;回归树二乘偏差均值:&apos;, mean_squared_error(test_price, predict_price))print(&apos;回归树绝对值偏差均值:&apos;, mean_absolute_error(test_price, predict_price)) CART决策树的剪枝CART决策树的剪枝主要采用的是CCP（cost-complexity prune）方法,又称代价复杂度。这种剪枝方式采用了节点的表面误差率增益值作为评估：其中Tt代表以t为根节点的子树，C(Tt)表示节点t的子树没被裁剪时子树Tt的误差，C(t)表示节点t的子树被裁剪时节点t的误差，|Tt|代表子树Tt的叶子树，剪枝后，T的叶子数减少了|Tt|-1。所以节点的表面误差率增益值等于节点t的子树被剪枝后的误差变化除以减掉的叶子数量。","link":"/2019/03/07/18-决策树（中）-CART算法笔记/"},{"title":"Dataframe 常见问题记录","text":"问题1、不能够解析某个字段 解决办法：查看表名是否正确，数据库是否正确，是否存在数据库切换的情况。","link":"/2019/06/17/Dataframe-常见问题记录/"},{"title":"CDH 5.15 简易版离线安装完整版","text":"CDH 简易版离线安装 一、虚拟机搭建 准备一台32G内存的电脑，安装虚拟机VMware-workstation。虚拟机下载地址：http://download3.vmware.com/software/wkst/file/VMware-Workstation-Full-14.1.2-8497320.x86_64.bundle。根据自己的电脑系统下载不同的版本，我下载的是VMware-Workstation-Full-14.1.2-8497320.x86_64.bundle。安装完虚拟机后，下载操作系统镜像CentOS-7-x86_64-DVD-1804.iso（这是我选择的版本，你们可以选择不同的版本），创建一个新的虚拟机，至于虚拟机如何创建请自行解决。 经过上面的一系列的操作，目前拥有三台虚拟机 master 内存 16G 磁盘 150G slave1 内存 6G 磁盘 150G slave2 内存 6G 磁盘 150G 二、虚拟机配置 1.修改所有的主机名，这样便于管理。 hostnamectl set-hostname masterhostnamectl set-hostname slave1hostnamectl set-hostname slave22.配置静态IP 首先，选择NAT网络连接模式 然后，点击Edit编辑虚拟机网络设置，进入VMware network edit ,选中vmnet8 ,将Use local DHCP service to distribute IP addresses to VMs 前面的勾去掉。 接着，进入 /etc/sysconfig/network-scripts中查看现有的配置文件然后修改其中的配置文件，其中有个类似ifcfg-enth0的文件是你的网络名字 TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=$’\\751\\605\\615\\747\\675\\656 1’UUID=2bfdf6df-9fd6-44e3-ade7-5a397cf8d2e4ONBOOT=yesIPADDR=172.16.247.135GATEWAY=172.16.247.2NETMASK=255.255.255.0PREFIX=24 上面主要修改红色字体部分，其中BOOTPROTO=static 表示静态，IPADDR=172.16.247.135 表示静态IP地址 最后，保存退出，执行 重启网络service network restart查看IPifconfigping网络ping www.baidu.com3.编辑hosts 文件 添加ip地址 vi /etc/hosts 添加以下配置，你对应的三台机器的IP地址和对应的主机名 172.16.247.135 master172.16.247.132 slave1172.16.247.136 slave2 然后将这个文件分别拷贝到各个节点上 scp /etc/hosts root@slave1:/etc/hostsscp /etc/hosts root@slave2:/etc/hosts 4.配置SSH免密登陆 主要分为两个步骤：首先在所有的节点生成公钥 ssh-keygen -t rsa然后将所有的节点执行拷贝公钥 ssh-copy-id root@masterssh-copy-id root@slave1ssh-copy-id root@slave2当然也可以公钥添加到认证文件中，并设置authorized_keys的访问权限：https://blog.csdn.net/johnzhc/article/details/81119030 5.关闭selinux和防火墙 vi /etc/selinux/config SELINUX=disabled [hadoop@master network-scripts]$ cat /etc/selinux/config This file controls the state of SELinux on the system.SELINUX= can take one of these three values:enforcing - SELinux security policy is enforced.permissive - SELinux prints warnings instead of enforcing.disabled - No SELinux policy is loaded.SELINUX=disabled SELINUXTYPE= can take one of three two values:targeted - Targeted processes are protected,minimum - Modification of targeted policy. Only selected processes are protected.mls - Multi Level Security protection.SELINUXTYPE=targeted 关闭防火墙和查看防火墙状态： systemctl stop firewalldsystemctl disable firewalldsystemctl status firewalld6.安装NTP时间同步 yum install -y ntp #安装ntp服务（所有节点） vi /etc/ntp.conf #编辑ntp服务的配置文件（所有节点） 主节点master的ntp.conf修改红色部分，蓝色要注释掉 Note: Monitoring will not be disabled with the limited restriction flag.#disable monitorrestrict default nomodifyrestrict default nomodify notrapserver 127.127.1.0fudge 127.127.1.0 stratum 10includefile /etc/ntp/crypto/pwkeys /etc/ntp/keys 中国这边最活跃的时间服务器 : http://www.pool.ntp.org/zone/cnserver 0.cn.pool.ntp.orgserver 0.asia.pool.ntp.orgserver 3.asia.pool.ntp.org allow update time by the upper server允许上层时间服务器主动修改本机时间restrict 0.cn.pool.ntp.org nomodify notrap noqueryrestrict 0.asia.pool.ntp.org nomodify notrap noqueryrestrict 3.asia.pool.ntp.org nomodify notrap noquery Undisciplined Local Clock. This is a fake driver intended for backupand when no outside source of synchronized time is available.外部时间服务器不可用时，以本地时间作为时间服务 从节点slave的ntp.conf修改红色部分，紫色要注释掉 with symmetric key cryptography.keys /etc/ntp/keys Specify the key identifiers which are trusted.#trustedkey 4 8 42 Specify the key identifier to use with the ntpdc utility.#requestkey 8 Specify the key identifier to use with the ntpq utility.#controlkey 8 Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats Disable the monitoring facility to prevent amplification attacks using ntpdcmonlist command when default restrict does not include the noquery flag. SeeCVE-2013-5211 for more details.Note: Monitoring will not be disabled with the limited restriction flag.#disable monitorserver master prefer #master 是指你的主机名restrict default nomodify notrap nopeer noqueryrestrict -6 default kod nomodify notrap nopeer noquery 设置开机启动ntp服务 #关闭chronyd服务systemctl disable chronyd.service #开机自启动 systemctl enable ntpd.service 以上都配置完成后，执行 systemctl start ntpd #开启ntp服务 ntpstat #查看ntp运行状态 synchronised to NTP server (172.16.247.135) at stratum 3 time correct to within 350 ms polling server every 1024 s #出现这个表示成功同步 7.卸载Centos 系统自带的JDK rpm -qa | grep jdk #查看系统自带的jdk yum -y remove xxjdk #删除所有的jdk 8.CM 和CDH下载以及JDK和java驱动 Cloudera Manager下载地址：http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.15.0_x86_64.tar.gz CDH安装包地址：http://archive.cloudera.com/cdh5/parcels/latest/，由于我们的操作系统为CentOS7.2，需要下载以下文件： CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha1 manifest.json JDK 可以去官网下载 下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 下载版本为： jdk-8u172-linux-x64.rpm mysql的java驱动 下载地址：https://dev.mysql.com/downloads/connector/j/5.1.html 下载版本为：mysql-connector-java-5.1.46.tar.gz 9.安装CM和jdk以及mysql驱动 将下载好的安装包分发到各个节点上，并解压缩。 1.将和cloudera-manager-daemons-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm以及cloudera-manager-server-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm三个安装包传入管理节点（master节点）/tmp 目录下，当然其他目录也可以。但是tmp目录可以使得解压后的rpm包重启后删除不占内存。 2.将cloudera-manager-agent-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm和cloudera-manager-daemons-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm两个安装包传入所有从节点上（slave1和slave2节点）的/tmp目录下 3.将jdk-8u172-linux-x64.rpm安装包传入所有节点上/tmp目录,复制语句类似下面： scp cloudera-manager-agent-5.15.0-1.cm5150.p0.62.el7.x86_64.rpm root@master:/tmp4.然后解压所有对应的安装包（所有节点） yum localinstall *.rpm5.配置JAVA_HOME变量（所有节点） echo “JAVA_HOME=/usr/java/latest/“ &gt;&gt; /etc/environment6.安装mysql驱动程序 将mysql-connector-java-5.1.46.tar.gz解压mysql-connector-java-5.1.46后将解压后包中的mysql-connector-java-5.1.46-bin.jar重命名为mysql-connector-java.jar传入 /usr/share/java目录里面。 tar mysql-connector-java-5.1.46.tar.gzsudo mkdir -p /usr/share/javacd mysql-connector-java-5.1.46sudo cp mysql-connector-java-5.1.46-bin.jar /usr/share/java/mysql-connector-java.jar 10.数据库安装 1.在master节点安装MariaDB(Mysql)这里安装MariDB,若要安装mysql，可参考https://blog.csdn.net/johnzhc/article/details/81119030。 sudo yum install mariadb-server #安装maridbsudo systemctl enable mariadb #设置开机启动sudo systemctl start mariadb #启动mariadbsudo /usr/bin/mysql_secure_installation #配置mariadb 可参考文档https://www.cloudera.com/documentation/cdh/5-1-x/CDH5-Installation-Guide/CDH5-Installation-Guide.html 2.为CDH创建数据库和用户 mysql -u root -p输入密码登陆mysql ，然后创建多个数据库，并完成授权。 CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON scm.* TO ‘scm‘@’%’ IDENTIFIED BY ‘scm’; CREATE DATABASE amon DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON amon.* TO ‘amon‘@’%’ IDENTIFIED BY ‘amon’; CREATE DATABASE rman DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON rman.* TO ‘rman‘@’%’ IDENTIFIED BY ‘rman’; CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON hue.* TO ‘hue‘@’%’ IDENTIFIED BY ‘hue’; CREATE DATABASE hive DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON hive.* TO ‘hive‘@’%’ IDENTIFIED BY ‘hive’; CREATE DATABASE sentry DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON sentry.* TO ‘sentry‘@’%’ IDENTIFIED BY ‘sentry’; CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci; GRANT ALL ON oozie.* TO ‘oozie‘@’%’ IDENTIFIED BY ‘oozie’;最后，退出数据库进行数据库的初始化，执行语句类似下面。 exit #退出数据库/user/share/cmf/schema/scm_prepare_database.sh (databaseType) (databaseName) (databaseuser) (databasepassword)例如scm数据库： /usr/share/cmf/schema/scm_prepare_database.sh mysql scm scm scm 11.安装CDH 1.在master节点创建parcel-repo仓库 mkdir -p /opt/cloudera/parcel-repochown cloudera-scm:cloudera-scm /opt/cloudera/parcel-repo2.将CDH安装包复制到/opt/cloudera/parcel-repo 目录下。 CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha1 manifest.json 然后将CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha1 重命名CDH-5.15.0-1.cdh5.15.0.p0.21-el7.parcel.sha 3.修改slave1和slave2的/etc/cloudera-scm-agent/config.ini 将server_host改为管理节点的网络名本例为master 4.分别启动cloudera-scm-server和cloudera-scm-agent 在主节点启动agent和server执行以下命令 systemctl start cloudera-scm-agent systemctl start cloudera-scm-server在slave1和slave2执行以下代码 systemctl start cloudera-scm-agent5.进入http://master:7180,默认的用户名和密码均为admin开始添加集群，下面是一种添加服务的顺序。 hdfs-&gt; yarn-&gt; hive-&gt; impala-&gt; zookeeper-&gt; hbase-&gt; oozie-&gt; hue-&gt;sqoop-&gt;kafka-&gt;spark 12.总结 安装过程主要遇到的坑： 1.静态IP的配置 2.时间同步ntp服务 3.安装服务的对应文件夹的权限问题。主要就是查看日志更改对应文件的权限： chmod 777 xxxchown root XXX4.kafka安装 在安装界面点击主机 点击parcel 点击KAFKA分配，并激活 然后添加kafka服务，并再配置的设置如下的参数： kafak mirrormaker： Destination Broker list slave1:9092 source list slave1:9092 topical whitelist slave1:9092 kafak Broker Advertiesd Host slave1 java heap size of broker 256","link":"/2019/01/14/CDH-5-15-简易版离线安装完整版/"},{"title":"Apache NiFi 简单介绍和使用","text":"一、什么是Apache NiFi简单来讲，NIFI就是为了构建系统之间的数据自动化传输的简易操作工具。提供了一些可靠的数据流的传输工具，解决了大部分现代企业中的数据ETL中所遇到的挑战。 二、NIFI 核心概念 三、NIFI 架构 四、 NIFI 使用界面介绍 主要包括工具栏、状态栏、总菜单栏，操作缩图和画布。平常用到的组件主要有Processor:The Processor is the NiFi component that is used to listen for incoming data; pull data from external sources; publish data to external sources; and route, transform, or extract information from FlowFiles. Processor Group:When a dataflow becomes complex, it often is beneficial to reason about the dataflow at a higher, more abstract level. NiFi allows multiple components, such as Processors, to be grouped together into a Process Group. The NiFi User Interface then makes it easy for a DFM to connect together multiple Process Groups into a logical dataflow, as well as allowing the DFM to enter a Process Group in order to see and manipulate the components within the Process Group. 我认为 Processor 就是一个数据处理器，它不仅可以从各种数据源中加载数据，转成Fiedflow基本nifi元素，还能够将数据处理成各种常见的格式以及将数据输出到各种数据库中而Processor Group 则是一个装载 Processor的容器。 五、案例（JSON_TO_MYSQL） 创建一个Processor Group ,将其拖到画布当中 然后修改名字 读取json文件，GetFile –&gt; 将JSON 转为sql语句，ConvertJSONToSQL –&gt; 运行sql语句，PutSQL GetFile 设置输入目录Input Directory，目标文件 File Filter ConvertJSONToSQL 设置数据库连接池JDBC Connection Pool ,SQL 类型 Statement Type ,表名 Table Name,其他默认 ，注意 ConvertJSONToSQL 只适用一个json值的文件 PutSQL 设置 数据库连接池，SQL Statement ，其他默认。一般来说只需要设置加黑的参数，但这里需要设置sql语句。","link":"/2019/09/11/Apache-NiFi-简单介绍和使用/"},{"title":"Docker 三剑客之 Docker Compose","text":"概述12345678Docker Compose 定位是 定义和运行多个docker容器应用（Defining and running multi-container Docker applications）它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）Compose 中有两个重要的概念：服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 安装与卸载1234561.安装$ sudo curl -L https://github.com/docker/compose/releases/download/1.17.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose2.卸载sudo rm /usr/local/bin/docker-compose","link":"/2019/10/25/Docker-三剑客之-Docker-Compose/"},{"title":"HBase 常用工具类","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165object HBaseUtils { def logger: Logger = LoggerFactory.getLogger(getClass) /** * 获取配置参数信息 * @return */ def getHBaseConf : Configuration = { val conf : Configuration = HBaseConfiguration.create conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;cdh-master01,cdh-master02,cdh-master03&quot;) conf.set(&quot;hadoop.security.authentication&quot;, &quot;Kerberos&quot;) conf } /** * 获取连接 * @param conf 配置信息 * @return */ def getConnection(conf:Configuration): Connection ={ ConnectionFactory.createConnection(conf) } /** * 获取管理员权限 * @param conn 连接信息 * @return HBaseAdmin */ def getAdmin(conn:Connection) : HBaseAdmin = { conn.getAdmin.asInstanceOf[HBaseAdmin] } /** * 创建表 * @param admin 管理员 * @param tableName 表名 * @param columnFamily 列族 * @throws org.apache.hadoop.hbase.MasterNotRunningException 异常 * @throws org.apache.hadoop.hbase.ZooKeeperConnectionException 异常 * @throws java.io.IOException 异常 */ @throws(classOf[MasterNotRunningException]) @throws(classOf[ZooKeeperConnectionException]) @throws(classOf[IOException]) def createTable(admin: HBaseAdmin,tableName: String,columnFamily:Array[String]):Unit = { val createTableName = TableName.valueOf(tableName) if (admin.tableExists(createTableName)){ logger.info(tableName + &quot;table exists!&quot;) }else{ val tableDesc = new HTableDescriptor(createTableName) tableDesc.addCoprocessor(&quot;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&quot;) for (singleColumnFamily &lt;- columnFamily){ val columnDesc = new HColumnDescriptor(singleColumnFamily) tableDesc.addFamily(columnDesc) } admin.createTable(tableDesc) logger.info(tableName + &quot; create table success!&quot;) }admin.close() } /** * 载入数据 * @param table HBase表 * @param rowKey 行键 * @param columnFamily 列族 * @param quorum 分布式信息 * @param value 数值 */ def addRow(table: Table,rowKey:String,columnFamily:String ,quorum:String,value:String): Unit ={ val rowPut:Put = new Put(Bytes.toBytes(rowKey)) if (value == null){ rowPut.addColumn(columnFamily.getBytes,quorum.getBytes,&quot;&quot;.getBytes) }else{ rowPut.addColumn(columnFamily.getBytes,quorum.getBytes,value.getBytes) } table.put(rowPut) } /** * 获取表数据 * @param table HBase表 * @param rowKey 行键 * @return HBase result */ def getRow(table: Table,rowKey: String ):Result = { val get:Get = new Get(Bytes.toBytes(rowKey)) val result:Result = table.get(get) for (rowKv &lt;- result.rawCells()){ println(&quot;Family:&quot; + new String(rowKv.getFamilyArray,rowKv.getFamilyOffset,rowKv.getFamilyLength,&quot;UT-8&quot;)) println(&quot;Qualifier:&quot; + new String(rowKv.getQualifierArray,rowKv.getQualifierOffset,rowKv.getQualifierLength,&quot;UT-8&quot;)) println(&quot;TimeStamp:&quot; + rowKv.getTimestamp) println(&quot;rowKey:&quot; + new String(rowKv.getRowArray,rowKv.getRowOffset,rowKv.getRowLength,&quot;UT-8&quot;)) println(&quot;Value:&quot; + new String(rowKv.getValueArray,rowKv.getValueOffset,rowKv.getValueLength,&quot;UT-8&quot;)) } result } /** * 批量添加数据 * @param table HBase表 * @param list 数据列表 */ def addDataBatch(table: Table,list: java.util.List[Put]): Unit = { try{ table.put(list) }catch{ logger.error(e.getMessage) case e: IOException =&gt; logger.error(e.getMessage) } } /** * 查询所有记录 * @param table HBase表 * @return resultScanner */ def queryAll(table: Table):ResultScanner = { val scan: Scan = new Scan try { val result: ResultScanner = table.getScanner(scan) result }catch { case e: IOException =&gt; logger.error(e.getMessage) null } } /** * 单条记录查询 * @param table HBase表 * @param queryColumn 查询列 * @param value 数值 * @param columns 列集合 * @return ResultScanner */ def queryBySingleColumn(table: Table, queryColumn: String, value: String, columns: Array[String]): ResultScanner = { if (columns == null || queryColumn == null || value == null ){ null }else{ try { val filter: SingleColumnValueFilter = new SingleColumnValueFilter(Bytes.toBytes(queryColumn),Bytes.toBytes(queryColumn),CompareOperator.EQUAL,new SubstringComparator(value)) val scan: Scan = new Scan for (column logger.error(e.getMessage) null } } } /** * 删除表 * @param hBaseConnection 链接 * @param tableName HBase表 */ def dropTable(hBaseConnection: Connection,tableName: String): Unit = { try { val admin: HBaseAdmin = hBaseConnection.getAdmin.asInstanceOf[HBaseAdmin] admin.disableTable(TableName.valueOf(tableName)) admin.deleteTable(TableName.valueOf(tableName)) }catch { case e: MasterNotRunningException =&gt; logger.error(e.getMessage) case e: ZooKeeperConnectionException =&gt; logger.error(e.getMessage) case e: IOException =&gt; logger.error(e.getMessage) } }}org.apache.hbase hbase-client${hbase.version}org.apache.hadoop hadoop-annotations","link":"/2019/10/17/HBase-常用工具类/"},{"title":"Docker 学习笔记","text":"docker 安装docker 安装地址 常见问题12341、docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.See &apos;docker run --help&apos;.可能docker上次为正常关闭导致： systemctl start docker 验证docker 安装成功1docker run hello-world 常见镜像操作1234567891011121314151617181920212223242526272829303132333435361.获取镜像docker pull ubuntu:16.042.运行镜像,进入容器docker run -it --rm \\ubuntu:16.04 \\bash3. 退出容器exit4.列出镜像docker image ls5.查看镜像、容器、数据卷所占用的空间docker system df6.虚悬镜像docker image ls -f dangling=true7.删除虚悬镜像docker image prune8.中间层镜像docker image ls -a9.部分镜像列出docker image ls ubuntudocker image ls ubuntu:16.04docker image ls -f since=mongo:3.2docker image ls -f label=com.example.version=0.110. 特定格式显示docker image ls -qdocker image ls --format &quot;{{.ID}}: {{.Repository}}&quot;docker image ls --format &quot;table {{.ID}}\\t{{.Repository}}\\t{{.Tag}}&quot; 镜像构建1234567891011$ mkdir mynginx$ cd mynginx$ touch DockerfileDockerfile内容:FROM nginxRUN echo &apos;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&apos; &gt; /usr/share/nginx/html/index.html在Dockerfile所在的目录执行docker build -t nginx:v3 . 常见容器操作123456789101112131415161718192021222324252627282930311.新建并启动容器docker run ubuntu:16.04 /bin/echo &apos;Hello World!&apos;2.在终端打开容器docker run -t -i ubuntu:16.04 /bin/bash （其中，-t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， -i 则让容器的标准输入保持打开）3. 启动已经终止的容器docker container start --help4.后台运行容器docker run -d ubuntu:16.04 /bin/sh -c &quot;while true ;do echo hello world;sleep 1;done&quot;5.查看运行容器docker container ls6.查看容器的运行日志docker logs container iddocker logs 583d0660cf1364e7e52a996e0ccab94c2c9e1b88a831233e5777adf6d46846017.终止一个容器docker container stop container iddocker container stop 583d0660cf138.查看所有的容器docker container ls -a9.启动已经停止的容器docker container start container iddocker container start 583d0660cf1310.重启容器docker container restart 进入容器两种方式123456789101. attach 命令docker run -dit ubuntudocker container lsdocker attach 14bf可以看出在退出容器是这个容器也会被停止。2. exec命令 docker exec -it f2a8 bash这个命令退出时不会停止容器。所有推荐使用这个命令进入容器中进行操作。 数据卷123456789101112131415161718192021222324251.创建数据卷docker volume create my-vol2.查看所有数据卷docker volume ls3.查看某个数据卷的具体信息docker volume inspect my-vol4.启动一个挂载数据卷的容器docker run -d -P \\&gt; --name web \\&gt; --mount source=my-vol,target=/webapp \\&gt; training/webapp \\&gt; python app.py5.删除数据卷docker volume rm my-vol如果报错：Error response from daemon: remove my-vol: volume is in use - [e6520e12082e077ff427d2ec87dc32c61bf30414570009a82363c706d5e2072e]则需要先停止正在使用数据卷的容器，并删除它。docker stop e6520e12082edocker rm e6520e12082e6.清除无主的数据卷docker volume prune docker 构建 Tomcat123456789101.搜索Tomcat镜像docker search tomcat2.拉取Tomcat镜像docker pull tomcat3.运行容器docker run --name tomcat -p 8080:8080 -v $PWD/test:/usr/local/tomcat/webapps/test -d tomcat命令说明：-p 8080:8080：将容器的8080端口映射到主机的8080端口-v $PWD/test:/usr/local/tomcat/webapps/test：将主机中当前目录下的test挂载到容器的/test docker 构建 mysql123456789101112131415161718191.搜索mysql镜像docker search mysql2.拉取mysql镜像docker pull mysql3.运行mysql镜像，构建mysql容器docker run -p 3306:3306 --name mysql \\-v /usr/local/docker/mysql/conf:/etc/mysql \\-v /usr/local/docker/mysql/logs:/var/log/mysql \\-v /usr/local/docker/mysql/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=123456 \\-d mysql命令参数：-p 3306:3306：将容器的3306端口映射到主机的3306端口-v /usr/local/docker/mysql/conf:/etc/mysql：将主机当前目录下的 conf 挂载到容器的 /etc/mysql-v /usr/local/docker/mysql/logs:/var/log/mysql：将主机当前目录下的 logs 目录挂载到容器的 /var/log/mysql-v /usr/local/docker/mysql/data:/var/lib/mysql：将主机当前目录下的 data 目录挂载到容器的 /var/lib/mysql-e MYSQL\\_ROOT\\_PASSWORD=123456：初始化root用户的密码","link":"/2019/10/23/Docker-学习笔记/"},{"title":"Intellij IDEA 问题记录","text":"Intellij IDEA 问题记录 问题一、123456789101112131415161718Intellij idea Language level和Java Compiler版本自动变化问题该问题主要是由于刷新pom.xml文件时，IDEA的这两个参数就会恢复成默认值解决办法：在pom.xml中加入以下配置&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 问题二、 123Unchecked assignment: &apos;java.util.HashMap&apos; to &apos;java.util.HashMap&lt;java.lang.String,java.lang.String&gt;修改为： Map&lt;String, String&gt; params = new HashMap&lt;&gt;(7);","link":"/2019/08/15/Intellij-IDEA-问题记录/"},{"title":"Linux 常用命令","text":"linux系统中清空文件内容的三种方法1.使用vi/vim命令打开文件后，输入”%d”清空，后保存即可。但当文件内容较大时，处理较慢，命令如下： vim file_name:%d:wq 2.使用cat命令情况，命令如下： cat /dev/null &gt; file_name 3.使用echo命令清空，此时会在文件中写入一个空行“\\n”，命令如下： echo “”&gt;file_name 推荐使用cat命令 vim 跳转最后一行和跳到行首第一种方式： :$ 跳到最后一行:1 跳到第一行 第二种方式： shift+g 跳到最后一行gg 跳转到第一行 查找文件关键字内容 grep -r “test” /data/reports 搜索命令 命令 作用 /string 向前搜索指定字符串；搜索时忽略大小写 :set ic n 搜索指定字符串的下一个出现位置 :%s/old/new/g 全文替换指定字符串 :n1,n2s/old/new/g 在一定范围内替换指定字符串 端口占用查询：123netstat -anlp | grep 80lsof -i:80 查看内存使用情况1free -h 查看磁盘使用情况 1df -h 查看文件创建数量 1df -i 删除某个文件的小文件12find . -type f -deletefind . -type d -print -delete 查看占用线程最大的程序1ps -eLf | wc -l 修改服务最大线程数12echo 1000000 &gt; /proc/sys/kernel/pid_max：修改pid_max值为1000000echo &quot;kernel.pid_max=1000000 &quot; &gt;&gt; /etc/sysctl.confsysctl -p：设置永久生效 监控java线程1ps -eLf | grep java | wc -l 监控网络客户连接数：1netstat -n | grep tcp | grep 侦听端口 | wc -l linux 将本地文件传给其他服务器123scp -r -P 2122 /home/lyxbdw/basedata/spark-2.4.3-bin-hadoop2.7.tgz root@lyxbdw-01:/usr/local/spark注意第二个路径与冒号之间不能有空格 查看某个目录下大文件1find / -size +50M | xargs du -h 查看CPU情况：1cat /proc/cpuinfo |grep &quot;model name&quot; &amp;&amp; cat /proc/cpuinfo |grep &quot;physical id&quot; 查看内存大小1cat /proc/meminfo | grep MemTotal 查看pid下的进程1ps -aux |grep -v grep|grep 31938 查看Linux的核数12345[bdcopt@iZwz920kp0myowdvn8v0zbZ device]$ cat /proc/cpuinfo | grep &quot;physical id&quot; | sort | uniq | wc -l1[bdcopt@iZwz920kp0myowdvn8v0zbZ device]$ cat /proc/cpuinfo | grep &quot;core id&quot; | sort | uniq | wc -l2[bdcopt@iZwz920kp0myowdvn8v0zbZ device]$ cat /proc/cpuinfo | grep &quot;process&quot; | sort | uniq | wc -l","link":"/2019/05/27/Linux-常用命令/"},{"title":"Linux 磁盘挂载","text":"在实际的生产应用中，经常需要对服务器的磁盘进行挂载。故总结下相关的操作，以备不时之需。1、fdisk -l 查看磁盘信息 2、fdisk /dev/vda 分区初始化 各个参数解析： m 显示所有命令列表 p 显示硬盘分割情形，打印分区表 a 设定硬盘启动区 n 设定新的硬盘分割区4.1 e 硬盘为延伸分割区4.2 p 硬盘为主要分割区5.d 删除硬盘分割区属性6.q 结束不存在硬盘分割区属性7.w 结束并写入硬盘分割区属性umount /opt 卸载挂载mke2fs -t ext4 /dev/vda1 # ext4创建文件系统df -h :查看当前硬盘使用情况","link":"/2019/05/07/Linux-磁盘挂载/"},{"title":"JSON数据处理（gson 和 fastjson）","text":"GSON 和 fastjson 总结1.JSON两种数据类型 json对象， object -&gt; {key:value,key:value,…} json数组，array -&gt; [value,value,…] 2.gson 解析 json对象 分为直接解析和通过java类来解析1234567&lt;!-- Gson: Java to Json conversion --&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.5&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; 1234567891011121314151617181920212223242526272829303132331.直接解析 //json对象形式解析 String data_json = &quot;{\\n&quot; + &quot; \\&quot;sex\\&quot;: &apos;男&apos;,\\n&quot; + &quot; \\&quot;hobby\\&quot;:[\\&quot;baskte\\&quot;,\\&quot;tennis\\&quot;],\\n&quot; + &quot; \\&quot;introduce\\&quot;: {\\n&quot; + &quot; \\&quot;name\\&quot;:\\&quot;tom\\&quot;,\\n&quot; + &quot; \\&quot;age\\&quot;:23\\n&quot; + &quot; }\\n&quot; + &quot; }&quot;; //json 解析器，解析json数据 JsonParser parser = new JsonParser(); JsonElement element = parser.parse(data_json); if (element.isJsonObject()) { //转化为对象 JsonObject object = element.getAsJsonObject(); //1.当value为String时，取出String值 String sex = object.get(&quot;sex&quot;).getAsString(); //2.当value为数组array,取出array JsonArray hobbies = object.getAsJsonArray(&quot;hobby&quot;); for (int i = 0; i &lt; hobbies.size(); i++) { String hobby = hobbies.get(i).getAsString(); System.out.println(&quot;hobby: &quot; + hobby); } //3.当value为object时，取出object JsonObject introduce = object.getAsJsonObject(&quot;introduce&quot;); String name = introduce.get(&quot;name&quot;).getAsString(); int age = introduce.get(&quot;age&quot;).getAsInt(); System.out.println(&quot;name: &quot; + name + &quot;;age: &quot; + age); 1234567891011121314151617181920212223242.通过java类来解析public class People { private String name; private int age; public String address; public int salary; @Override public String toString() { return &quot;&quot; + &quot;name=&apos;&quot; + name + &apos;\\&apos;&apos; + &quot;, age=&quot; + age + &quot;, address=&apos;&quot; + address + &apos;\\&apos;&apos; + &quot;, salary=&quot; + salary ; }//====================================================== //3.借助java类来生成对应的java对象来解析数据 String object_json = &quot;{\\&quot;name\\&quot;:\\&quot;tom\\&quot;,\\&quot;salary\\&quot;:12999}&quot;; Gson gson = new Gson(); People people = gson.fromJson(object_json, People.class); System.out.println(people.toString()); 3.gson 解析 json数组1234567891011121314151617181920212223242526272829303132333435// json数组对象解析 String array_data_json = &quot;[\\n&quot; + &quot; \\&quot;cake\\&quot;,\\n&quot; + &quot; 2,\\n&quot; + &quot; {\\&quot;brother\\&quot;:\\&quot;tom\\&quot;,\\&quot;sister\\&quot;:\\&quot;lucy\\&quot;},\\n&quot; + &quot; [\\&quot;red\\&quot;,\\&quot;orange\\&quot;]\\n&quot; + &quot;]&quot;; // json解析器，解析json数据 JsonParser parser_array = new JsonParser(); JsonElement element_array = parser_array.parse(array_data_json); // json属于数组类型 if (element_array.isJsonArray()) { JsonArray array = element_array.getAsJsonArray(); // 1. value为string时，取出string String array_1 = array.get(0).getAsString(); System.out.println(&quot;array_1:&quot; + array_1); // 2. value为int时，取出int int array_2 = array.get(1).getAsInt(); System.out.println(&quot;array_2:&quot; + array_2); // 3. value为object时，取出object JsonObject array_3 = array.get(2).getAsJsonObject(); String brother = array_3.get(&quot;brother&quot;).getAsString(); String sister = array_3.get(&quot;sister&quot;).getAsString(); System.out.println(&quot;brother:&quot; + brother + &quot;;sister:&quot; + sister); // 4. value为array时，取出array JsonArray array_4 = array.get(3).getAsJsonArray(); for (int i = 0; i &lt; array_4.size(); i++) { System.out.println(array_4.get(i).getAsString()); } } 123456789101112132.java 数组解析当数据为数组时，对应java中是数组类型[&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;]String json2 = &quot;[\\&quot;apple\\&quot;, \\&quot;pear\\&quot;, \\&quot;banana\\&quot;]&quot;;Gson gson2 = new Gson();// 传入的java类型是String[].classString[] fruits = gson2.fromJson(json2, String[].class); 对于简单数组可以反序列化List类型String json2 = &quot;[\\&quot;apple\\&quot;, \\&quot;pear\\&quot;, \\&quot;banana\\&quot;]&quot;;Gson gson2 = new Gson();List&lt;String&gt; fruitList = gson2.fromJson(json2, new TypeToken&lt;List&lt;String&gt;&gt;(){}.getType()); 注意： 1.json数据与java类字段名不一样，可以添加注解@SerializedName() 12345@SerializedName(&quot;money&quot;)private String salary;@SerializedName({&quot;money&quot;, &quot;salary&quot;}) // 可以有多个备选值private String salary; 2.限定字段是否需要序列化12@Expose(serialize=false,deserialize=false)private String name; 3.复合对象处理123456789101112131415161718{ &quot;name&quot;: &quot;tom&quot;, &quot;age&quot;: 0, &quot;money&quot;: 2999, &quot;hobbies&quot;: [ &quot;basket&quot;, &quot;tennis&quot; ], &quot;collections&quot;: { &quot;2&quot;: &quot;paint&quot;, &quot;3&quot;: &quot;mouse&quot; }}//一般而言，当value是数组时，对应java类中也是数组；当value 是对象时，对应java类中的map(k-v对)private List&lt;String&gt; hobbies;private Map&lt;Integer, String&gt; collections; 4.通过泛型来处理1234567891011如json数据格式相近{&quot;code&quot;:&quot;0&quot;,&quot;message&quot;:&quot;success&quot;,&quot;data&quot;:{}}{&quot;code&quot;:&quot;0&quot;,&quot;message&quot;:&quot;success&quot;,&quot;data&quot;:[]}则动态生成java对象public class Result&lt;T&gt;{ public int code; public String message; public T data; // getter、setter} 4. gson将java对象转为json字符串12345Gson gson2 = new Gson();String json2 = gson2.toJson(entry2);System.out.println(json2);对于非值属性，即引用属性，如hobbies、collections，如果没有设置值的话，在序列化后的json数据中，是不会出现的。而如果是值属性的话，没有设置值的情况下，在json数据中会是使用java中的默认值。 5.fastjson解析java对象 Json是一种轻量级的数据交换格式，采用一种“键：值”对的文本格式来存储和表示数据，在系统交换数据过程中常常被使用，是一种理想的数据交换语言。fastjson.jar是阿里爸爸开发的一款专门用于Java开发的包，可以方便的实现json对象与JavaBean对象的转换，实现JavaBean对象与json字符串的转换，实现json对象与json字符串的转换。 例子1234567891011121314151617181920212223242526272829package com.gree.bdc.jsonchange;import com.alibaba.fastjson.JSON;import com.gree.bdc.entity.EnamellingPullingMachineStatus;import com.gree.bdc.entity.PullingRecord;import com.gree.bdc.util.DateUtils;import java.util.UUID;public class JsonChange { public static void main(String[] args) { String jsonString = &quot;{\\&quot;CUR_DV\\&quot;:\\&quot;110.60\\&quot;,\\&quot;CUR_LSP\\&quot;:\\&quot;155.71\\&quot;,\\&quot;DQSX_WE\\&quot;:\\&quot;128.72\\&quot;,\\&quot;HP_PO\\&quot;:\\&quot;0\\&quot;,\\&quot;JLHP_TM\\&quot;:\\&quot;261.07\\&quot;,\\&quot;MachineCode\\&quot;:\\&quot;123\\&quot;,\\&quot;MachineType\\&quot;:1,\\&quot;PX_IT\\&quot;:\\&quot;1.00\\&quot;,\\&quot;PX_SP\\&quot;:\\&quot;198.26\\&quot;,\\&quot;QBX_LR\\&quot;:\\&quot;0.70\\&quot;,\\&quot;SDCX_LSP\\&quot;:\\&quot;158.00\\&quot;,\\&quot;SDGZ_LSP\\&quot;:\\&quot;158.00\\&quot;,\\&quot;SFHP_WE\\&quot;:\\&quot;270.00\\&quot;,\\&quot;SXZL_CUR\\&quot;:\\&quot;50.52\\&quot;,\\&quot;SXZL_SE\\&quot;:\\&quot;50.00\\&quot;,\\&quot;SXZL_SW\\&quot;:\\&quot;50.00\\&quot;,\\&quot;Sub_Oven_Index\\&quot;:1,\\&quot;Type_Data\\&quot;:3,\\&quot;YPX_CUD\\&quot;:\\&quot;8\\&quot;,\\&quot;YP_TY\\&quot;:\\&quot;8\\&quot;,\\&quot;YSX_RSP\\&quot;:\\&quot;0.00\\&quot;,\\&quot;ZL_EN\\&quot;:\\&quot;1\\&quot;,\\&quot;ZPX_CUD\\&quot;:\\&quot;8\\&quot;,\\&quot;ZP_TY\\&quot;:\\&quot;8\\&quot;,\\&quot;ZSX_RSP\\&quot;:\\&quot;140.21\\&quot;}&quot;; EnamellingPullingMachineStatus enamellingPullingMachineStatus = JSON.parseObject( jsonString, EnamellingPullingMachineStatus.class ); PullingRecord pullingRecord = new PullingRecord(); String id = UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;); String machine_index = enamellingPullingMachineStatus.getMachineCode() + (enamellingPullingMachineStatus.getSubOvenIndex() + 1);; Float weight = enamellingPullingMachineStatus.getCurrentTakeUpWeight();; String record_time = DateUtils.getFormatTimeNow(); pullingRecord.setId( id ); pullingRecord.setMachine_index( machine_index ); pullingRecord.setWeight( weight ); pullingRecord.setRecord_time( record_time ); String pullingRecordJson = JSON.toJSONString( pullingRecord ); System.out.println( pullingRecordJson ); }} 参考文献：GSON Json详解以及fastjson使用教程","link":"/2019/12/18/JSON数据处理（gson-和-fastjson）/"},{"title":"Mysql 优化原理 笔记","text":"Mysql 逻辑框架 MYSQL 逻辑框架整体分为三层，最上层为客户端层，并非Mysql独有，诸如：连接处理，授权认证，安全等。 MYSQL 大多数核心服务均在这中间这一层，包括查询解析、分析、优化、缓存、内置函数。所有的跨存储引擎也在这层实现，如：存储过程，触发器和视图等 最下层为存储引擎。负责Mysql中数据存储和提取。 Mysql查询过程 Mysql查询优化，首先得了解Mysql是如何优化和执行查询的，然后在实际的工作中就是遵循一些原则让Mysql的优化器能够按照预想的合理的方式运行而已。 参考：https://mp.weixin.qq.com/s/OeKXHpnk72kp37E6z97xMA","link":"/2019/03/16/Mysql-优化原理-笔记/"},{"title":"MySQL 语句记录","text":"sqlserver 统计具有重复字段的记录：1SELECT product_id, COUNT(*) AS sumCount FROM dat_bill_201811 GROUP BY product_id HAVING sumCount &gt; 1; 统计出a表有的b表没有的数据：12select a.* FROM A a left outer join B b on a.qq = b.qqWHERE b.qq is null; mysql 统计具有重复字段的记录(hive也可以)：1select username,count(*) as count from hk_test group by username having count&gt;1; MySQL8.x 连接 jdbc.driverClass=com.mysql.cj.jdbc.Driverjdbc.connectionURL=jdbc:mysql://127.0.0.1:3306/spring-cloud-itoken-service-admin?serverTimezone=GMT&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falsejdbc.username=rootjdbc.password=123456 主要关注的驱动不同，新加了cj这个东东，然后可能时区问题新加serverTimezone=GMT","link":"/2019/05/16/MySQL-语句记录/"},{"title":"NEO4J 图数据库使用APOC数据导入","text":"Neo4j 数据导入 一、安装与部署 直接在官网下载安装包安装，解压即可。 二、下载相应的jar包 1.sqlserver 数据导入neo4j的jar包 apoc-3.4.0.1-all.jar mssql-jdbc-6.2.2.jre8.jar sqljdbc4-4.0.jar 2.mysql 数据导入neo4j的jar包 apoc-3.3.0.1-all.jar mysql-connector-java-8.0.8-dmr.jar 3.将对应jar包放在安装目录plugins文件目录里，然后conf目录里的neo4j.conf的后面加上 1234dbms.security.procedures.unrestricted=apoc.*apoc.import.file.enabled=trueapoc.export.file.enabled=true 4.restart neo4j,运行return apoc.version(),若有版本号，则成功。 三、导数据 12345678910111213import org.neo4j.driver.v1.*;public class Connect{public static void main(String[] args){ Driver driver = GraphDatabase.driver(&quot;bolt://localhost:7687&quot;,AuthTokens.basic(&quot;neo4j&quot;,&quot;neo4j&quot;)); Session session = driver.session(); String cypher=&quot;create constraint on (n:ITEM) ASSERT n.itemid is unique&quot;; //创建唯一索引，这样可以更快的导入数据 Session.run(cypher); cypher=&quot;CALL apoc.periodic.iterate(\\&quot;CALL apoc.load.jdbc(&apos;jdbc:sqlserver://localhost;username=name;password=word;database=db;characterEncoding=utf-8&apos;,\\\\\\&quot;SELECT * FROM TABLE1\\\\\\&quot;)\\&quot;,\\&quot;MERGE(n:ITEM{itemid:row.mitemid}) with * MERGE(m:ITEM{itemid:row.itemid}) with * create p=(n)-[r:rel{rels:row.rels}]-&gt;(m)\\&quot;,{batchSize:10000,iterateList:true})&quot;; //连接sqlserve数据库和设计创建neo4j图数据库数据模型 Session.run(cypher); session.close(); driver.close(); }} mysql数据库类似,不再赘述。 补充：1.使用neo4j-import导入数据的命令 1neo4j-admin import --nodes:item &quot;nodes.csv&quot; --relationships:rel &quot;rel_header.csv,rel.csv&quot; --ignore-missing-nodes 2.apoc 导出命令 1234call apoc.export.cypher.query(&quot;MATCH (p1:Person)-[r:KNOWS]-&gt;(p2:Person) RETURN p1,r,p2&quot;,&quot;/tmp/friendships.cypher&quot;,{format:&apos;plain&apos;,cypherFormat:&apos;updateStructure&apos;})` 参考： http://neo4j-contrib.github.io/neo4j-apoc-procedures/#_export_import12call apoc.export.cypher.query(&quot;match (n:lable) where not (n)--() and n.properties = &apos;400&apos; return distinct(n)&quot;,&quot;C://User/Desktop/test&quot;,{format:&apos;plain&apos;,cypherFormat:&apos;create&apos;}) 3.不用解压也能导数据load csv 123load csv from &quot;file:/twitter-2010.txt.gz&quot; as line fieldterminator &apos; &apos; with toInt(line[0]) as id,toInt(line[1]) as id1 return id,id1 limit 10using periodic commit 1000load csv from &quot;file:/twitter-2010.txt.gz&quot; as line fieldterminator &apos; &apos; create (item:ITEM{id:line[0],item:line[1]}) 常见问题总结： 问题一、apoc 导数据 时会漏掉一些数据，总是导不全？有一些批次失败。报错信息：{ “Cannot merge node using null property value for dsca”: 8} { “total”: 52562339, “committed”: 52554339, “failed”: 8000, “errors”: { “Cannot merge node using null property value for dsca”: 8 12具体原因：是dsca字段有null值造成的，可以看出导数据时需要先对数据清洗，清除脏数据。操作：将为null的数据转为&apos;null&apos;字符串。 问题二：Caused by: org.neo4j.driver.v1.exceptions.ClientException: Failed to invoke procedure apoc.periodic.iterate: Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: Connection reset数据库连接被重置可能原因有很多：可能数据库关闭了连接，也可能是防火墙和网络的原因，还有一些其他的异常可以通过try{}cathch来捕捉异常来解决。 问题三：neo4j由于查询数据量过多导致数据库挂掉。neo4j可能会因一次性查询数据太多，而挂掉。进入neo4j目录bin下。cd到bin目录下，执行启动命令：./neo4j stop另外neo4j还有其他命令，执行方式相同：neo4j { console | start | stop | restart | status }如果，./neo4j stop不能停止neo4j，用kill -s 9 强制杀掉进程。 然后，调用./neo4j start 启动neo4j 问题四：neo4j 导数据时程序未出现报错信息，但是一直卡着，数据导入失败。可以查看进程：jps、ps -p pid,直接将所有的关于neo4j的进程kill ,再重新执行数据导入程序在查看进行可以看到有两个进程CommunityEntryPoint和jar.可能造成的原因是由于上次导完数据后的链接数据库为关闭。 hive数据迁移到neo4j操作 下载与hive版本一致的jar包，如（hadoop-common-2.7.3.jar，hive-exec-1.2.1.jar，hive-jdbc-1.2.1.jar，hive-metastore-1.2.1.jar，hive-service-1.2.1.jar，httpclient-4.4.jar，httpcore-4.4.jar，libfb303-0.9.2.jar，libthrift-0.9.3.jar） 将jar包放入到plugin 目录中 重启neo4j,./neo4j restart 加载驱动，call apoc.load.driver(‘org.apache.hive.jdbc.HiveDriver’) 执行查询，call apoc.load.jdbc(‘jdbc:hive2://ip:1000/database;user=hdfs;password=hdfs’,’select name,age from user’) YIELD rowRETURN row.name, row.age; 注意： 1.hive 查询的时候需要将字段查出来，例如：select name，age from user ,如果使用 select * from user 则返回的数据为null2.hive 表中数据若是太大可能需要查询很久才能，会将数据全部查询出来才才开始进行图的构建。 问题五：报错：Node(934582) already exists with label ITEM and property itemid = ‘980_A700100171’原因：由于节点重复导致，由于itemid相同，但是节点其他属性不同，无法完成节点合并解决办法：去掉重复的节点，或者不创建唯一的itemID，但是这样数据导入速度会变慢，故建议采用去掉重复的节点id。 问题六：org.neo4j.driver.v1.exceptions.TransientException: Timeout waiting for database to become available and allow new transactions. Waited 1s. 1 reasons for blocking: Database availableorg.neo4j.driver.v1.exceptions.TransientException: Timeout waiting for database to become available and allow new transactions. Waited 1s. 1 reasons for blocking: Database available.1超时了，应该是数据量大、查询复杂度高导高 Neo4j反应不过来了 neo4j 查询数据库，返回json格式123456789Session session=driver.session();StatementResult result=session.run(sql)Function&lt;Record, Person&gt; function = a -&gt; {Person person = new Person();person.setName(a.get(&quot;name&quot;));return person;};List&lt;Person&gt; persons = result.list(function);String json = JSON.toJSONString(persons)","link":"/2019/01/14/NEO4J-图数据库使用APOC数据导入/"},{"title":"NIFI 安装部署","text":"::: hljs-center NIFI 安装部署 ::: 一、NIFI 下载 NIFI 下载链接：NIFI Downloads 选择与自己环境匹配的安装包 若是下载速度过慢，可以选择另一个下载地址：NIFI 下载 二、NIFI 安装 将下载的安装包放到 E:\\software\\ ，然后解压安装，非常简单 修改配置文件E:\\software\\nifi-1.9.2-bin\\nifi-1.9.2\\conf\\nifi.properties，更改端口号 三 、NIFI 运行 进入E:\\software\\nifi-1.9.2-bin\\nifi-1.9.2\\bin 目录，双击run-nifi.bat，若是linux则sh nifi.sh 打开浏览器，进入http://localhost:8081/nifi/，可能需要等会页面加载慢","link":"/2019/09/09/NIFI-安装部署/"},{"title":"RDD常用算子","text":"1.mapPartitionsWithIndex:独立运行在每个分片上，并带有分区的编号。 12345val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9),2) rdd1.saveAsTextFile(&quot;hdfs://master:8020/sfz&quot;) def func1(index:Int,iter: Iterator[(Int)]) : Iterator[String] = {iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;,val: &quot; + x +&quot;]&quot;).iterator} rdd1.mapPartitionsWithIndex(func1).collect 2.aggregate:先局部操作，再整体操作。 12345678910val rdd2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;),2)def func2(index: Int,iter: Iterator[(String)]) : Iterator[String] = {iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;,val: &quot; + x +&quot;]&quot;).iterator}rdd2.mapPartitionsWithIndex(func2).collectrdd2.aggregate(&quot;&quot;)(_+_,_+_)val rdd3 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;4567&quot;),2)rdd3.aggregate(&quot;&quot;)((x,y) =&gt; math.max(x.length,y.length).toString,(x,y) =&gt; x+y)val rdd5 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;&quot;,&quot;345&quot;),2)rdd5.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length,y.length).toString,(x,y) =&gt; x+y) 3.aggregateByKey：将key值相同的，先局部操作，再整体操作。和reduceByKey内部实现差不多 12val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;, 4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 2) pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect","link":"/2018/08/15/RDD常用算子/"},{"title":"Spark Windows10 安装部署","text":"Spark centos 安装部署 一、Scala 安装 jdk安装不用多说，一般没什么问题，下载Scala安装包，Scala安装包下载链接，选择自己安装的版本。123cd /usr/localmkdir scalatar -zxvf scala-2.11.8.tgz 解压安装后配置环境变量123456789vim /etc/profile export JAVA_HOME=/usr/local/java/jdk1.8.0_11 export SCALA_HOME=/usr/local/scala/scala-2.11.8 export SPARK_HOME=/usr/local/spark/spark-2.4.3-bin-hadoop2.7 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:${SCALA_HOME}/lib:${SPARK_HOME}/lib export PATH=${JAVA_HOME}/bin:$PATH:${SCALA_HOME}/bin:$PATH:${SPARK_HOME}/bin export SPARK_SSH_OPTS=&quot;-p 2122&quot; 主要是配置SCALA_HOME参数，然后运行source /etc/profile,并执行scala 二、Spark安装 Spark安装也没什么技术含量，直接下载spark安装包，spark下载链接，选择自己安装版本。1234cd /usr/localmkdir sparkcd sparktar -zxvf spark-2.4.3-bin-hadoop2.7.tgz 同样解压后配置环境变量，主要是SPARK_HOME配置，再次执行source /etc/profile,并执行spark-shell. 三、spark standalone模式部署 进入到配置目录12345678910111213141516cd /usr/local/spark/spark-2.4.3-bin-hadoop2.7/confmv slaves.template slavesmv spark-env.sh.template spark-env.shvim spark-env.sh添加以下配置# spark-env.shexport SCALA_HOME=/usr/local/scala/scala-2.11.8export JAVA_HOME=/usr/local/java/jdk1.8.0_11# 本地安装绑定export SPARK_MASTER_IP=127.0.0.1#export SPARK_LOCAL_IP=127.0.0.1export SPARK_MASTER_PORT=7077export SPARK_WORKER_MEMORY=8Gexport SPARK_EXECUTOR_CORES=4export SPARK_LOG_DIR=/lvm/data1/spark/log#export master=spark://10.7.20.191:7077 然后进入sbin目录,启动集群12cd /usr/local/spark/spark-2.4.3-bin-hadoop2.7/sbin./start-all.sh 注意若是服务器之间的免密登录端口不是22端口，则需要在 /etc/profile文件添加配置，改到对应端口1export SPARK_SSH_OPTS=&quot;-p 2122&quot; 至此spark安装完成。 Spark命令提交程序1nohup spark-submit --class com.gree.cn.location.LocationMap --master spark://lyxbdw-02:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar &gt; /lvm/data1/spark/log/location.log &amp; 执行脚本1234#!/bin/shresult_time=$(date +%Y%m%d)echo &quot;小车定位程序&quot;nohup spark-submit --class com.gree.cn.location.LocationMap --master spark://lyxbdw-02:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar &gt; /lvm/data1/spark/log/location/${result_time}_result.log &amp; spark slave 启动 1./start-slave.sh spark://masterip:7077","link":"/2019/08/12/Spark-Windows10-安装部署/"},{"title":"flink 基础","text":"一、Apache Flink定义：Apache Flink 是一个分布式大数据处理引擎，可以对有限流和无线流进行有状态和无状态进行计算，能够在各种集群环境上部署，对各种大小的数据集进行快速计算。 flink 启动命令1234linux./bin/start-cluster.shwindows.\\start-cluster.bat flink maven 项目依赖java版本：123456789101112&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; Scala版本：123456789101112&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; flink 关联 kafka 依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.10_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt;&lt;/dependency&gt; flink 推荐使用shade插件打包maven项目12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;artifactSet&gt; &lt;excludes&gt; &lt;exclude&gt;com.google.code.findbugs:jsr305&lt;/exclude&gt; &lt;exclude&gt;org.slf4j:*&lt;/exclude&gt; &lt;exclude&gt;log4j:*&lt;/exclude&gt; &lt;/excludes&gt; &lt;/artifactSet&gt; &lt;filters&gt; &lt;filter&gt; &lt;!-- Do not copy the signatures in the META-INF folder. Otherwise, this might cause SecurityExceptions when using the JAR. --&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;mainClass&gt;my.programs.main.clazz&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; flink 基础 API 概念 DataSet and DataStream: Flink具有特殊类DataSet并DataStream在程序中表示数据。您可以将它们视为可以包含重复项的不可变数据集合。在DataSet数据有限的情况下，对于一个DataStream元素的数量可以是无界的。 flink 一般处理步骤 获得一个execution environment， 加载/创建初始数据， 指定此数据的转换， 指定放置计算结果的位置， 触发程序执行","link":"/2019/08/30/flink-基础/"},{"title":"Scala 安装问题","text":"1、此时不应有 \\scala\\bin。由于Scala 安装路径中包含空格，需要重新安装。","link":"/2019/07/10/Scala-安装问题/"},{"title":"flink sink mysql 和 hbase","text":"直接上例子，本人亲测有效1.搭建kafka(单机)，hbase（单机）2.引入依赖主要包括 flink-connector-kafka-0.11_2.11、hbase-client、mysql-connector-java、gson1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.gree.bdc&lt;/groupId&gt; &lt;artifactId&gt;flink-source&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;flink-source&lt;/name&gt; &lt;description&gt;Demo project for flink-source&lt;/description&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;flink.version&gt;1.8.0&lt;/flink.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- flink 快速开始--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt; &lt;version&gt;${flink.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- flink 结束--&gt; &lt;!-- log4j 开始 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- log4j 结束--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--插件 --&gt; &lt;!-- Gson: Java to Json conversion --&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.5&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 3.项目结构4.创建MysqlSink12345678910111213141516171819202122232425262728293031323334353637383940package com.gree.bdcimport java.sql.{Connection, Driver, DriverManager}import com.google.gson.Gsonimport com.gree.bdc.KafkaToSinkStreaming.Studentimport org.apache.flink.configuration.Configurationimport org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}class MysqlSink (url:String, user:String, pwd:String ) extends RichSinkFunction[String]{ var conn:Connection = _ override def open(parameters: Configuration): Unit ={ super.open(parameters) Class.forName(&quot;com.mysql.jdbc.Driver&quot;) conn = DriverManager.getConnection(url,user,pwd) conn.setAutoCommit(false) } override def invoke(value: String,context:SinkFunction.Context[_]): Unit ={// super.invoke(value) val gson = new Gson() val student = gson.fromJson(value,classOf[Student]) println(value) val person = conn.prepareStatement(&quot;replace into student(name,age,sex,sid) values(?,?,?,?)&quot;) person.setString(1,student.name) person.setString(2,student.age.toString) person.setString(3,student.sex) person.setString(4,student.sid) person.execute() conn.commit() } override def close(): Unit = { super.close() conn.close() }} 5.创建HbaseSink12345678910111213141516171819202122232425262728293031323334353637383940414243package com.gree.bdcimport com.google.gson.Gsonimport com.gree.bdc.KafkaToSinkStreaming.Studentimport org.apache.flink.configuration.Configurationimport org.apache.flink.streaming.api.functions.sink.{RichSinkFunction, SinkFunction}import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}import org.apache.hadoop.hbase.client.{Connection, ConnectionFactory, Put}import org.apache.hadoop.hbase.util.Bytesclass HBaseSink(tableName:String, family:String) extends RichSinkFunction[String] { var conn:Connection = _ override def open(parameters: Configuration): Unit = { super.open(parameters) val conf = HBaseConfiguration.create() conf.set(&quot;hbase.zookeeper.quorum&quot;,&quot;172.16.247.131&quot;) conn = ConnectionFactory.createConnection(conf) } override def invoke(value: String, context: SinkFunction.Context[_]): Unit ={ val gson = new Gson() val student = gson.fromJson(value,classOf[Student]) println(value) println(student) val table = conn.getTable(TableName.valueOf(tableName)) val put = new Put(Bytes.toBytes(student.sid)) put.addColumn(Bytes.toBytes(family),Bytes.toBytes(&quot;name&quot;),Bytes.toBytes(student.name)) put.addColumn(Bytes.toBytes(family),Bytes.toBytes(&quot;age&quot;),Bytes.toBytes(student.age)) put.addColumn(Bytes.toBytes(family),Bytes.toBytes(&quot;sex&quot;),Bytes.toBytes(student.sex)) table.put(put) table.close() } override def close(): Unit ={ super.close() conn.close() }} 6.创建KafkaToSinkStreaming123456789101112131415161718192021222324252627282930313233343536package com.gree.bdcimport java.util.Propertiesimport org.apache.flink.api.common.serialization.SimpleStringSchemaimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironmentimport org.apache.flink.streaming.connectors.kafka.{FlinkKafkaConsumer010, FlinkKafkaConsumer011}/*flink 读取 kafka数据sink mysql 和 hbase */object KafkaToSinkStreaming { def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val prop = new Properties() prop.setProperty(&quot;bootstrap.servers&quot;,&quot;localhost:9092&quot;) prop.setProperty(&quot;group.id&quot;,&quot;test&quot;) val input = env.addSource(new FlinkKafkaConsumer010[String](&quot;test&quot;, new SimpleStringSchema(),prop)) //自定义MysqlSink类。将数据写入到mysql val mysqlSink = new MysqlSink(&quot;jdbc:mysql://localhost:3306/test?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot;, &quot;root&quot;, &quot;123456&quot;) input.addSink(mysqlSink) //自定义HBaseSink类，将数据Sink到HBase val hBaseSink = new HBaseSink(&quot;student&quot;,&quot;sfz&quot;) input.addSink(hBaseSink) env.execute(&quot;KafkaToSinkStreaming&quot;) } case class Student(name: String,age: Int,sex:String,sid:String)} 7.遇到问题总结 1.NoSuchColumnFamilyException主要是创建hbase table 是 familycloumn 和表对应的列族相同 2.java.net.UnknownHostExceptionhosts文件中未配置主机名和ip对应表123vim /etc/hosts172.16.247.131 kubernetes-master 参考文章：Flink读取Kafka数据Sink到MySQL和HBase数据库","link":"/2019/12/05/flink-sink-mysql-和-hbase/"},{"title":"hive 笔记","text":"hive设置队列三种方式： set mapred.job.queue.name=queue3;SET mapreduce.job.queuename=queue3;set mapred.queue.names=queue3;","link":"/2019/04/04/hive-笔记/"},{"title":"flink 基础知识","text":"一、Flink核心概念和基础 Flink 是一个框架和分布式处理引擎。用于对有界和无界数据流进行有状态计算。并且Flink提供了数据分布和容错机制以及资源管理等核心功能。 flink 提供了诸多高抽象层API以便用户进行分布式任务编写： DateSet API ,对静态数据进行批处理，将静态数据抽象成分布式的数据集，用户可以方便对分布式数据集进行处理。 DateStream API ,对数据流进行流处理操作，将流数据抽象成分布式数据流。 Table API ,对结果化数据进行查询操作，将流式数据抽象成关系表，并通过类SQL的DSL对关系表进行操作 flink的特性： 支持高吞吐、低延迟、高性能的流处理支持带有事件时间的窗口（Window）操作支持有状态计算的Exactly-once语义支持高度灵活的窗口（Window） 操作支持具有Backpressure功能的持续流模型支持基于轻量级分布式快照实现容错（snapshot）一个运行时同时支持Batch on Streaming处理和Streaming 处理Flink在JVM实现了自己的内存管理支持迭代计算支持程序自动优化：避免特定情况下：Shuffle、排序等昂贵操作，中间结果有必要进行缓存 二、Flink 与 spark streaming 区别Flink 是标准的实时处理引擎，基于事件驱动，而spark streaming 是微批处理（Micro-Batch） 参考：Flink面试通关手册","link":"/2019/12/06/flink-基础知识/"},{"title":"java 基本知识","text":"1.泛型 通过泛型可以定义类型安全的数据结构（类型安全），而无须使用实际的数据类型（可扩展）。这能够显著提高性能并得到更高质量的代码（高性能），因为您可以重用数据处理算法，而无须复制类型特定的代码（可重用）。 2.java.sql.SQLException: Unknown system variable ‘query_cache_size’12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; 3.java.sql.SQLException: The server time zone value ‘ÖÐ¹ú±ê×¼Ê±¼ä’ is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the serverTimezone configuration property) to use a more specifc time zone value if you want to utilize time zone support. 1jdbc:mysql://localhost:3306/springboot_jpa?charset=utf8mb4&amp;useSSL=false&amp;serverTimezone=GMT 4.java.lang.Exception: No runnable methods 测试时 未在test方法上加上 @test 注解，同时保证注解依赖org.junit.test 5.STRING… 类型后面三个点(String…)，是从Java 5开始，Java语言对方法参数支持一种新写法，叫可变长度参数列表，其语法就是类型后跟…，表示此处接受的参数为0到多个Object类型的对象，或者是一个Object[]。 java 面向对象编程三条主线 1.java类以及类的成员（属性，方法，构造器，代码块）2.java三大特性：封装（封装与隐藏，主要通过权限修饰符来控制），继承和多态3.java关键字：this 构造器：每一个类都有构造器，构造器的主要作用就是创建对象，和初始化一些属性值，构造器没有返回值继承：子类继承父类，子类拥有父类的所有结构，主要为：属性和方法，子类还可以拥有自己的属性和方法。关键字：package，import","link":"/2019/11/29/java-基本知识/"},{"title":"jar 包相关知识","text":"1、查看jar包 1ps -aux | grep java 2、jar 包后台运行 12345678nohup java -jar xxx.jar &gt; xxx.log &amp;nohup java -jar analysisbasedata-0.0.1-SNAPSHOT.jar &gt; analysisbasedata-0.0.1-SNAPSHOT.lognohup java -jar /home/lyxbdw/basedata/comsumer/analysisbasedata-0.0.1-SNAPSHOT.jar &gt; /home/lyxbdw/basedata/comsumer/analysisbasedata.log &amp;nohup java -jar /home/lyxbdw/basedata/server/basedata-0.0.1-SNAPSHOT.jar &gt; /home/lyxbdw/basedata/server/basedata.log &amp; 3、杀死进程 1kill -9 pid 4、查找文件内容1grep &quot;17:46:49&quot; nohup.out","link":"/2019/06/14/jar-包相关知识/"},{"title":"jupyter 安装","text":"选择管理员运行cmd,执行以下命令： pip install jupyter 进入目录 C:\\Program Files\\Python37\\Scripts&gt;，运行命令： cd C:\\Program Files\\Python37\\Scripts jupyter notebook 注意管理员权限问题。 http://localhost:8888/tree 第三方库numpy安装 pip install numpy pip install -U scikit-learn pip install -U pandasql pip install graphviz pip install pydotplus","link":"/2019/01/21/jupyter-安装/"},{"title":"kafka相关的常规操作","text":"1、启动服务器12345cd /usr/local/kafka/kafka_2.11-2.2.0bin/zookeeper-server-start.sh -daemon config/zookeeper.propertiesbin/kafka-server-start.sh -daemon config/server.properties 2、创建主题创建一个名为“test”的主题，它只包含一个分区，只有一个副本：1bin/kafka-topics.sh --create --bootstrap-server lyxbdw-01:9092 --replication-factor 1 --partitions 1 --topic test 查看主题：1bin/kafka-topics.sh --list --bootstrap-server lyxbdw-01:9092 test 3、生产者发送消息1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 4.kafka生产者创建消息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103package com.gree.cn.basedata.kafka_netty;import java.io.IOException;import java.io.InputStream;import java.util.Properties;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;public final class KafkaProducerSingleton { private static KafkaProducer&lt;Integer, Object&gt; kafkaProducer; private String topic; private KafkaProducerSingleton() { } /** * 静态内部类 * * @author tanjie * */ private static class LazyHandler { private static final KafkaProducerSingleton instance = new KafkaProducerSingleton(); } /** * 单例模式,kafkaProducer是线程安全的,可以多线程共享一个实例 * * @return */ public static final KafkaProducerSingleton getInstance() { return LazyHandler.instance; } /** * kafka生产者进行初始化 * * @return KafkaProducer */ public void init(String topic) { this.topic = topic; if (null == kafkaProducer) { Properties props = new Properties(); InputStream inStream = null; try { inStream = this.getClass().getClassLoader() .getResourceAsStream(&quot;producer.properties&quot;); props.load(inStream); kafkaProducer = new KafkaProducer&lt;Integer, Object&gt;(props); } catch (IOException e) { e.printStackTrace(); } finally { if (null != inStream) { try { inStream.close(); } catch (IOException e) { e.printStackTrace(); } } } } } /** * 通过kafkaProducer发送消息 * 具体消息值 */ public void sendKafkaMessage( String message) { /** * 1、如果指定了某个分区,会只讲消息发到这个分区上 2、如果同时指定了某个分区和key,则也会将消息发送到指定分区上,key不起作用 * 3、如果没有指定分区和key,那么将会随机发送到topic的分区中 4、如果指定了key,那么将会以hash&lt;key&gt;的方式发送到分区中 */ ProducerRecord&lt;Integer, Object&gt; record = new ProducerRecord&lt;Integer, Object&gt;( topic, message); // send方法是异步的,添加消息到缓存区等待发送,并立即返回，这使生产者通过批量发送消息来提高效率 // kafka生产者是线程安全的,可以单实例发送消息 kafkaProducer.send(record); } /** * kafka实例销毁 */ public void close() { if (null != kafkaProducer) { kafkaProducer.close(); } } public String getTopic() { return topic; } public void setTopic(String topic) { this.topic = topic; }} 然后调用用producer实例1234KafkaProducerSingleton kafkaProducerSingleton = KafkaProducerSingleton .getInstance(); kafkaProducerSingleton.init(&quot;topic0619&quot;); kafkaProducerSingleton.sendKafkaMessage(msg+DateUtils.getFormatTimeNow()); 注意配置producer.properties文件 #指定节点列表bootstrap.servers=10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092 #指定序列化处理类key.serializer=org.apache.kafka.common.serialization.IntegerSerializervalue.serializer=org.apache.kafka.common.serialization.StringSerializeracks=0 #buffer.memory=33554432 #是否压缩compression.type=snappy #是否重试retries=0batch.size=500request.timeout.ms=5000 kafka 启动123456bin/zookeeper-server-start.sh -daemon config/zookeeper.propertiesbin/kafka-server-start.sh -daemon config/server.properties.\\zookeeper-server-start.bat ..\\..\\config\\zookeeper.properties.\\kafka-server-start.bat ..\\..\\config\\server.properties","link":"/2019/04/30/kafka相关的常规操作/"},{"title":"kafka 集群安装 部署","text":"kafka 集群安装部署 安装kafka首先需要安装zookeeper集群，这里使用的是kafka自带的zookeeper,不推荐使用（我也不知道为啥，别人都这么说）。 jdk 安装jdk 自己下载吧，后解压12cd /usr/local/javatar -zxvf jdk-8u11-linux-x64.tar.gz 配置环境变量123456vi /etc/profile#jdk环境变量配置 export JAVA_HOME=/usr/local/java/jdk1.8.0_11 export JRE_HOME=${JAVA_HOME}/jre export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib export PATH=${JAVA_HOME}/bin:$PATH echo “JAVA_HOME=/usr/local/java/jdk1.8.0_11/“ &gt;&gt; /etc/environment 然后配置其他机器scp -r -P 2122 root@lyxbdw-02:/usr/local/java /usr/local zookeepeer集群安装123cd /usr/local/kafkatar -xzf kafka_2.12-2.2.0.tgzcd kafka_2.12-2.2.0 配置zookeeper配置文件 zookeeper.properties12345678910vim config/zookeeper.properties====================================================dataDir=/usr/local/kafka/kafka_2.11-2.2.0/zookeeperTime=2000 initLimit=10 syncLimit=5server.1=lyxbdw-01:2888:3888 server.2=lyxbdw-02:2888:3888 server.3=lyxbdw-03:2888:3888 然后在/usr/local/kafka/kafka_2.11-2.2.0/zookeeper 创建myid文件编辑123vi myid==================1 其他机器复制 zookeeper.properties到相应的目录，并设置不同的myid值每台机器都启动1bin/zookeeper-server-start.sh -daemon config/zookeeper.properties kafka集群安装这里主要就是配置相关的配置信息，server.properties123456vim config/server.properties=====================================broker.id=1host.name=lyxbdw-02log.dirs=/tmp/kafka-logszookeeper.connect=lyxbdw-01:2181,lyxbdw-02:2181,lyxbdw-03:2181 然后进入到log日志目录下，修改meta.properties文件123vim /tmp/kafka-logs/meta.properties======================================broker.id=1 其他机器类似，最后启动kafak 服务1bin/kafka-server-start.sh -daemon config/server.properties 到此，使用kafka自带的zookeeper安装就完成了。 当然，在实际的生成应用中，需要注意以下事项：1、数据目录需要放在磁盘的目录下2、进程启动最好是在后台启动 -daemon 命令可以实现。","link":"/2019/04/30/kafka-集群安装-部署/"},{"title":"lambda 表达式","text":"1.什么是lambda表达式 简单讲就是实现将一个代码块赋值给一个变量。如将右边代码块赋给左边变量： 2.lambda表达式的作用 可以使代码变得简洁如我们在不使用lambda表达式实现接口，并调用它时与使用lambda表达式对比 参考：zhihu.com/question/20125256/answer/324121308","link":"/2019/12/03/lambda-表达式/"},{"title":"kylin jdbc 连接","text":"kylin jdbc 连接 一 、kylin 简介 Kylin是ebay开发的一套OLAP系统，与Mondrian不同的是，它是一个MOLAP系统，主要用于支持大数据生态圈的数据分析业务，它主要是通过预计算的方式将用户设定的多维立方体缓存到HBase中。 二、引入pom文件 &lt;dependency&gt; &lt;groupId&gt;org.apache.kylin&lt;/groupId&gt; &lt;artifactId&gt;kylin-jdbc&lt;/artifactId&gt; &lt;version&gt;1.6.0&lt;/version&gt; &lt;/dependency&gt; 三 、例子 import org.apache.kylin.jdbc.Driver; import org.datanucleus.state.LifeCycleState; import java.sql.Connection; import java.sql.ResultSet; import java.sql.Statement; import java.util.ArrayList; import java.util.List; import java.util.Properties; public class KylinJdbc { public static void main(String[] args) throws Exception { Driver driver = (Driver) Class.forName(&quot;org.apache.kylin.jdbc.Driver&quot;).newInstance(); Properties info = new Properties();info.put(&quot;user&quot;, &quot;BI&quot;);info.put(&quot;password&quot;, &quot;16de#+ui9&quot;); Connection conn = driver.connect(&quot;jdbc:kylin://10.214.234.111:7070/Test_kylin&quot;, info); Statement state = conn.createStatement(); String sqlStr = &quot;&quot;; ResultSet resultSet = state.executeQuery(sqlStr); List list = new ArrayList(); while (resultSet.next()){ list.add(resultSet.getString(&quot;1&quot;)); } list.forEach(citg -&gt; System.out.println(citg)); } }","link":"/2019/01/18/kylin-jdbc-连接/"},{"title":"mybatis","text":"1、什么是mybatis MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生类型、接口和 Java 的 POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。 2、Exception in thread “main” tk.mybatis.mapper.MapperException: 无法获取实体类com.gree.cn.sparkstreamingkafka.entity.BasestationInfo对应的表名导包选择：import tk.mybatis.spring.annotation.MapperScan; 3.tk.mybatis与springboot结合使用案例（1）引入依赖1234567891011121314151617&lt;!-- 数据库连接--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 数据库连接 end --&gt; 可以看出tk.mybatis常常结合druid德鲁伊使用 （2）引入插件12345678910111213141516171819202122&lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;configuration&gt; &lt;configurationFile&gt;${basedir}/src/main/resources/generator/generatorConfig.xml&lt;/configurationFile&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;${mysql.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;3.4.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; 可以看见接下来需要配置${basedir}/src/main/resources/generator/generatorConfig.xml配置文件 （3）在下图目录下创建generatorConfig.xml配置文件，并设置相应的参数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;&gt;&lt;generatorConfiguration&gt; &lt;!-- 引入数据库连接配置 --&gt; &lt;properties resource=&quot;jdbc.properties&quot;/&gt; &lt;context id=&quot;Mysql&quot; targetRuntime=&quot;MyBatis3Simple&quot; defaultModelType=&quot;flat&quot;&gt; &lt;property name=&quot;beginningDelimiter&quot; value=&quot;`&quot;/&gt; &lt;property name=&quot;endingDelimiter&quot; value=&quot;`&quot;/&gt; &lt;!-- 配置 tk.mybatis 插件 --&gt; &lt;plugin type=&quot;tk.mybatis.mapper.generator.MapperPlugin&quot;&gt; &lt;property name=&quot;mappers&quot; value=&quot;com.gree.cn.mybatistest.mapper.MyMapper&quot;/&gt; &lt;/plugin&gt; &lt;!-- 配置数据库连接 --&gt; &lt;jdbcConnection driverClass=&quot;${jdbc.driverClass}&quot; connectionURL=&quot;${jdbc.connectionURL}&quot; userId=&quot;${jdbc.username}&quot; password=&quot;${jdbc.password}&quot;&gt; &lt;/jdbcConnection&gt; &lt;!-- 配置实体类存放路径 --&gt; &lt;javaModelGenerator targetPackage=&quot;com.gree.cn.mybatistest.entity&quot; targetProject=&quot;src/main/java&quot;/&gt; &lt;!-- 配置 XML 存放路径 --&gt; &lt;sqlMapGenerator targetPackage=&quot;mapper&quot; targetProject=&quot;src/main/resources&quot;/&gt; &lt;!-- 配置 DAO 存放路径 --&gt; &lt;javaClientGenerator targetPackage=&quot;com.gree.cn.mybatistest.mapper&quot; targetProject=&quot;src/main/java&quot; type=&quot;XMLMAPPER&quot;/&gt; &lt;!-- 配置需要指定生成的数据库和表，% 代表所有表 --&gt; &lt;table catalog=&quot;bluetooth_location&quot; tableName=&quot;basestation_info&quot;&gt; &lt;!-- mysql 配置 --&gt; &lt;!-- &lt;generatedKey column=&quot;id&quot; sqlStatement=&quot;Mysql&quot; identity=&quot;true&quot;/&gt;--&gt; &lt;/table&gt; &lt;table catalog=&quot;bluetooth_location&quot; tableName=&quot;buffer_area_info&quot;&gt; &lt;!-- mysql 配置 --&gt; &lt;!-- &lt;generatedKey column=&quot;id&quot; sqlStatement=&quot;Mysql&quot; identity=&quot;true&quot;/&gt;--&gt; &lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 可以在上面的配置文件中需要创建数据库连接jdbc.properties配置文件,如下： 12345# MySQL 8.x: com.mysql.cj.jdbc.Driverjdbc.driverClass=com.mysql.cj.jdbc.Driverjdbc.connectionURL=jdbc:mysql://localhost:3306/bluetooth_location?serverTimezone=GMT&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falsejdbc.username=rootjdbc.password=lyxbdw 然后配置实体类存放路径 配置 XML 存放路径 配置 DAO 存放路径 配置需要指定生成的数据库和表 （4）这些配置完成后，将创建通用的父级接口MyMapper需要注意这个文件的创建目录，注释中有解释12345678910111213package com.gree.cn.mybatistest.tk.mybatis.mapper;import tk.mybatis.mapper.common.Mapper;import tk.mybatis.mapper.common.MySqlMapper;/** * * 自己的 Mapper * 特别注意，该接口不能被扫描到，否则会出错 * @param &lt;T&gt; */public interface MyMapper&lt;T&gt; extends Mapper&lt;T&gt;, MySqlMapper&lt;T&gt; {} （5）最后再在application.properties配置实体类路径和xml配置文件路径12mybatis.mapper-locations=classpath:mapper/*.xmlmybatis.type-aliases-package=com.gree.cn.mybatistest.entity 并在主程序入口添加注解@MapperScan(basePackages = “com.gree.cn.mybatistest.mapper”)扫描mapper包需要注意的是引入的依赖为 import tk.mybatis.spring.annotation.MapperScan; 再点击下图中的mybatis-generator:generate 前期的准备工作就完成。当然在使用前有两点需要注意下1、自动生成的mapper文件可能不会集成通用的父类mapper接口，手动引入jar包2、生成的实体类中表名可能会有两个点，需要手动删除一个 4、tk.mybatis查询数据库12345678910111213141516171819202122232425262728293031323334package com.gree.cn.mybatistest.service.impl;import com.gree.cn.mybatistest.entity.BasestationInfo;import com.gree.cn.mybatistest.entity.BufferAreaInfo;import com.gree.cn.mybatistest.mapper.BasestationInfoMapper;import com.gree.cn.mybatistest.mapper.BufferAreaInfoMapper;import com.gree.cn.mybatistest.service.MybatisTest;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import tk.mybatis.mapper.entity.Example;import java.util.List;@Servicepublic class MybatisTestimpl implements MybatisTest { @Autowired private BufferAreaInfoMapper bufferAreaInfoMapper; @Autowired private BasestationInfoMapper basestationInfoMapper; @Override public void mybatistest() { Example example = new Example(BufferAreaInfo.class); example.createCriteria().andEqualTo(&quot;stationxcoordinate&quot;, 811) .andEqualTo(&quot;stationycoordinate&quot;, 130); List&lt;BufferAreaInfo&gt; bufferAreaInfos = bufferAreaInfoMapper.selectByExample(example); BasestationInfo uu = basestationInfoMapper.selectByPrimaryKey(&quot;E07DEACF031C&quot;); System.out.println(bufferAreaInfos.size()); System.out.println(uu.getStationid()); }} 大概步骤就是注入mapper,然后使用mapper调用sql算子进行查询，其他的都类似。","link":"/2019/06/17/mybatis/"},{"title":"neo4j 连接 spark","text":"今天使用neo4j连接sparkneo4j版本3.4spark版本1.6.0 (1) 首先，需要添加jar包 neo4j-spark-connector_2.10-1.0.0-RC1.jar或者添加maven依赖123org.neo4j.spark neo4j-spark-connector_2.10 1.0.0-RC1 (2) 设置spark连接信息val conf : SparkConf = new SparkConf().setAppName(“InitSpark”).setMaster(“local[*]”) conf.set(“spark.neo4j.bolt.url”,”bolt://localhost:7687”)conf.set(“spark.neo4j.bolt.user”,”neo4j”)conf.set(“spark.neo4j.bolt.password”,”123456”) 简单例子 123456object Neo4jDataFrameTest { def main(args: Array[String]): Unit = { val sQLContext = InitSpark.getSqlContext val query =&quot; MATCH p=(m:ITEM)&lt;-[r:rel*]-(n:ITEM) where m.item = &apos;84010420&apos; and ALL(c in r where c.exdt&gt;&apos;2018-12-15&apos;)&quot; + &quot; with m.item as mark,length(r) as len, last(r).number as num,reduce(s=1.0 ,x in rels(p)| s*tofloat(x.number)) as nums, last(r).mitem as mitem , last(r) as bom &quot; + &quot; return mark,bom.erpid as erpid, bom.virtual as virtual,mitem,bom.item as item,bom.pono as pono,&quot; + &quot; bom.comment as comment, bom.warehouse as warehouse, bom.exdt as exdt, bom.indt as indt,tofloat(num) as num,nums,len &quot; val df = Neo4jDataFrame.withDataType(sQLContext,query,Seq.empty,&quot;mark&quot; -&gt; StringType,&quot;erpid&quot; -&gt;StringType,&quot;virtual&quot;-&gt;LongType, &quot;mitem&quot;-&gt;StringType,&quot;item&quot;-&gt;StringType,&quot;pono&quot;-&gt;LongType,&quot;comment&quot;-&gt;StringType,&quot;warehouse&quot;-&gt;StringType,&quot;exdt&quot;-&gt;StringType, &quot;indt&quot; -&gt;StringType,&quot;num&quot;-&gt;DoubleType,&quot;nums&quot;-&gt; DoubleType ,&quot;len&quot;-&gt;LongType) df.show(1000) }} 12345678object Neo4jConnectSparkGraph { def main(args: Array[String]): Unit = { val sc = InitSpark.getSC val sQLContext = InitSpark.getSqlContext val cypher = &quot;match (n:ITEM) return n.item limit 10&quot; val neo = Neo4jRowRDD(sc,cypher) } }","link":"/2019/03/11/neo4j-连接-spark/"},{"title":"mysql 8.0以后授权","text":"mysql 授权操作 12345678910111213首先创建一个用户create user &apos;root&apos;@&apos;172.16.247.129&apos; identified by &apos;lyxbdw&apos;;然后进行授权 grant all privileges on *.* to &apos;root&apos;@&apos;172.16.247.129&apos; -&gt; ;刷新权限flush privileges;开启管理员权限，重启mysql![image.png](/images/2019/08/14/6a9ca990-be6f-11e9-8d50-b3f800759cdf.png) 查看某个用户权限 1show grants for &apos;root&apos;@&apos;172.16.247.129&apos;;","link":"/2019/08/14/mysql-8-0以后授权/"},{"title":"linux （ubuntu） hbase 单机安装","text":"1.jdk 安装 下载jdk-8u212-linux-x64.tar.gz安装包，执行命令123cd /usr/localmkdir javatar -zxvf jdk-8u212-linux-x64.tar.gz 配置JAVA_HOME等参数1vim /etc/profile export JAVA_HOME=/usr/local/java/jdk1.8.0_212export HBASE_HOME=/usr/local/hbase/hbase-2.1.0export JRE_HOME=/usr/local/java/jdk1.8.0_212/jreexport PATH=${JAVA_HOME}/bin:$PATH:${HBASE_HOME}/bin:$PATH 保存，使生效1source /etc/profile 2.hbase 安装下载 hbase-2.1.0-bin.tar.gz 安装包，执行命令123cd /usr/localmkdir hbasetar -zxvf hbase-2.1.0-bin.tar.gz 配置HBASE_HOME等参数。如上参数。 修改配置文件12cd /usr/local/hbase/hbase-2.1.0/confvim hbase-env.sh 新增配置参数 export JAVA_HOME=/usr/local/java/jdk1.8.0_212export HBASE_MANAGES_ZK=true 配置hbase数据目录12cd /usr/local/hbase/hbase-2.1.0/confvim hbase-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:///usr/local/hbase/hbase-tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动habse1234cd /usr/local/hbase/hbase-2.1.0/bin./start-hbase.shhbase shell","link":"/2019/12/04/linux-（ubuntu）-hbase-单机安装/"},{"title":"maven 打包相关记录","text":"Maven项目打包这里主要记录常用的打包方式（将依赖打包进jar包）。 一、spring boot 项目对于spring boot项目只需要添加spring boot 和maven整合的插件就可以 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;!-- spring boot 依赖 --&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- spring boot end --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 二、普通的java项目对于普通的java项目需要添加额外的依赖才能够将依赖打包进jar中12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.gree.cn.location.LocationMap&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 三、scala 项目打包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;scala-compile-first&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;**/*.scala&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;scala-test-compile&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.gree.cn.write2mysql.Write2mysql&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt;","link":"/2019/09/05/maven-打包相关记录/"},{"title":"scala 正则匹配","text":"::: hljs-center scala正则匹配 ::: 中文字符匹配 1234//匹配中文字符,正则表达式val regrex =&quot;[\\u4e00-\\u9fa5]&quot;.r//匹配字符串中第一个中文字符val matches = regrex.findFirstMatchIn(dsca_part1).mkString(&quot;&quot;) 常规括号加数字匹配 1234567891011121314def get_dsca2(dsac_part2:String): String ={ var dsca_use = dsac_part2 //匹配左括号加5个数字，其中括号需要转义 val partment = &quot;\\\\([0-9]{5}&quot;.r //匹配多个括号，？代表多次匹配 val partment3 = &quot;\\\\((.*?)\\\\)&quot;.r val result = partment3.findAllMatchIn(dsca_use) result.foreach(x =&gt;{ if (partment.findAllMatchIn(x.toString()).isEmpty){ dsca_use = dsca_use.split(&quot;\\\\(&quot; +x.toString()+&quot;\\\\)&quot;)(0) } }) return dsca_use } 常用的正则表达式Scala 的正则表达式继承了 Java 的语法规则，Java 则大部分使用了 Perl 语言的规则。 下表我们给出了常用的一些正则表达式规则： 表达式 匹配规则^ 匹配输入字符串开始的位置。$ 匹配输入字符串结尾的位置。. 匹配除”\\r\\n”之外的任何单个字符。[…] 字符集。匹配包含的任一字符。例如，”[abc]”匹配”plain”中的”a”。[^…] 反向字符集。匹配未包含的任何字符。例如，”[^abc]”匹配”plain”中”p”，”l”，”i”，”n”。\\A 匹配输入字符串开始的位置（无多行支持）\\z 字符串结尾(类似$，但不受处理多行选项的影响)\\Z 字符串结尾或行尾(不受处理多行选项的影响)re 重复零次或更多次re+ 重复一次或更多次re? 重复零次或一次re{ n} 重复n次re{ n,}re{ n, m} 重复n到m次a|b 匹配 a 或者 b(re) 匹配 re,并捕获文本到自动命名的组里(?: re) 匹配 re,不捕获匹配的文本，也不给此分组分配组号(?&gt; re) 贪婪子表达式\\w 匹配字母或数字或下划线或汉字\\W 匹配任意不是字母，数字，下划线，汉字的字符\\s 匹配任意的空白符,相等于 [\\t\\n\\r\\f]\\S 匹配任意不是空白符的字符\\d 匹配数字，类似 [0-9]\\D 匹配任意非数字的字符\\G 当前搜索的开头\\n 换行符\\b 通常是单词分界位置，但如果在字符类里使用代表退格\\B 匹配不是单词开头或结束的位置\\t 制表符\\Q 开始引号：\\Q(a+b)3\\E 可匹配文本 “(a+b)3”。\\E 结束引号：\\Q(a+b)3\\E 可匹配文本 “(a+b)*3”。 正则表达式实例实例 描述. 匹配除”\\r\\n”之外的任何单个字符。[Rr]uby 匹配 “Ruby” 或 “ruby”rub[ye] 匹配 “ruby” 或 “rube”[aeiou] 匹配小写字母 ：aeiou[0-9] 匹配任何数字，类似 [0123456789][a-z] 匹配任何 ASCII 小写字母[A-Z] 匹配任何 ASCII 大写字母[a-zA-Z0-9] 匹配数字，大小写字母[^aeiou] 匹配除了 aeiou 其他字符[^0-9] 匹配除了数字的其他字符\\d 匹配数字，类似: [0-9]\\D 匹配非数字，类似: [^0-9]\\s 匹配空格，类似: [ \\t\\r\\n\\f]\\S 匹配非空格，类似: [^ \\t\\r\\n\\f]\\w 匹配字母，数字，下划线，类似: [A-Za-z0-9_]\\W 匹配非字母，数字，下划线，类似: [^A-Za-z0-9_]ruby? 匹配 “rub” 或 “ruby”: y 是可选的ruby* 匹配 “rub” 加上 0 个或多个的 y。ruby+ 匹配 “rub” 加上 1 个或多个的 y。\\d{3} 刚好匹配 3 个数字。\\d{3,} 匹配 3 个或多个数字。\\d{3,5} 匹配 3 个、4 个或 5 个数字。\\D\\d+ 无分组： + 重复 \\d(\\D\\d)+/ 分组： + 重复 \\D\\d 对([Rr]uby(, )?)+ 匹配 “Ruby”、”Ruby, ruby, ruby”，等等注意上表中的每个字符使用了两个反斜线。这是因为在 Java 和 Scala 中字符串中的反斜线是转义字符。所以如果你要输出 ..，你需要在字符串中写成 .\\. 来获取一个反斜线","link":"/2019/05/30/scala-正则匹配/"},{"title":"redis 数据库","text":"Redis 数据库 Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Hash), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。 Redis 安装参考redis基础教程 一、客户端模式：123cd /usr/local/redis/redis-5.0.5/src./redis-cli --rawhvals materialinfo 二、springboot 与 redis 整合1.引入依赖123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; 2.配置依赖文件12345678# redisspring.redis.host = 172.80.122.33spring.redis.port = 6379spring.redis.database = 0spring.redis.timeout = 100msspring.redis.lettuce.pool.max-active = 8spring.redis.lettuce.pool.max-idle = 8spring.redis.lettuce.pool.min-idle = 0spring.redis.lettuce.pool.max-wait = 100ms 3.使用redisTemplate1234@Autowired private RedisTemplate&lt;String,String&gt; redisTemplate;redisTemplate.opsForHash().entries(&quot;scada:dashboard:enamelling_line_status&quot;); redis-client操作redis数据1.引入依赖123456&lt;!-- Redis客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; 2.配置数据库连接池1234567891011121314private static JedisPool jedisPool; private Jedis jedis; static { JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(20); config.setMaxIdle(5); config.setMaxWaitMillis(1000); config.setMinIdle(2); config.setTestOnBorrow(false); jedisPool = new JedisPool(config, &quot;127.0.0.1&quot;, 6379); } private static synchronized Jedis getJedis(){ return jedisPool.getResource(); } 3.操作数据库12jedis = getJedis();jedis.hvals(&quot;materialinfo&quot;); redis client 命令操作对String操作命令 set(Key,value) :给数据库中名称为Key的String赋值value get(Key):返回数据库中名称为Key的String的value getset(key, value)：给名称为 key 的 string 赋予上一次的 value mget(key1, key2,…, key N)：返回库中多个 string 的 value setnx(key, value)：添加 string，名称为 key，值为 value setex(key, time, value)：向库中添加 string，设定过期时间 time mset(key N, value N)：批量设置多个 string 的值 msetnx(key N, value N)：如果所有名称为 key i 的 string 都不存在 incr(key)：名称为 key 的 string 增 1 操作 incrby(key, integer)：名称为 key 的 string 增加 integer decr(key)：名称为 key 的 string 减 1 操作 decrby(key, integer)：名称为 key 的 string 减少 integer append(key, value)：名称为 key 的 string 的值附加 value substr(key, start, end)：返回名称为 key 的 string 的 value 的子串 对List操作命令 rpush(key, value)：在名称为 key 的 list 尾添加一个值为 value 的元素 lpush(key, value)：在名称为 key 的 list 头添加一个值为 value 的元素 llen(key)：返回名称为 key 的 list 的长度 lrange(key, start, end)：返回名称为 key 的 list 中 start 至 end 之间的元素 ltrim(key, start, end)：截取名称为 key 的 list lindex(key, index)：返回名称为 key 的 list 中 index 位置的元素 lset(key, index, value)：给名称为 key 的 list 中 index 位置的元素赋值 lrem(key, count, value)：删除 count 个 key 的 list 中值为 value 的元素 lpop(key)：返回并删除名称为 key 的 list 中的首元素 rpop(key)：返回并删除名称为 key 的 list 中的尾元素 blpop(key1, key2,… key N, timeout)：lpop 命令的 block 版本。 brpop(key1, key2,… key N, timeout)：rpop 的 block 版本。 rpoplpush(srckey, dstkey)：返回并删除名称为 srckey 的 list 的尾元素，并将该元素添加到名称为 dstkey 的 list 的头部 对Set操作命令 sadd(key, member)：向名称为 key 的 set 中添加元素 member srem(key, member) ：删除名称为 key 的 set 中的元素 member spop(key) ：随机返回并删除名称为 key 的 set 中一个元素 smove(srckey, dstkey, member) ：移到集合元素 scard(key) ：返回名称为 key 的 set 的基数 sismember(key, member) ：member 是否是名称为 key 的 set 的元素 sinter(key1, key2,…key N) ：求交集 sinterstore(dstkey, (keys)) ：求交集并将交集保存到 dstkey 的集合 sunion(key1, (keys)) ：求并集 sunionstore(dstkey, (keys)) ：求并集并将并集保存到 dstkey 的集合 sdiff(key1, (keys)) ：求差集 sdiffstore(dstkey, (keys)) ：求差集并将差集保存到 dstkey 的集合 smembers(key) ：返回名称为 key 的 set 的所有元素 srandmember(key) ：随机返回名称为 key 的 set 的一个元素 对 Hash 操作的命令 hset(key, field, value)：向名称为 key 的 hash 中添加元素 field hget(key, field)：返回名称为 key 的 hash 中 field 对应的 value hmget(key, (fields))：返回名称为 key 的 hash 中 field i 对应的 value hmset(key, (fields))：向名称为 key 的 hash 中添加元素 field hincrby(key, field, integer)：将名称为 key 的 hash 中 field 的 value 增加 integer hexists(key, field)：名称为 key 的 hash 中是否存在键为 field 的域 hdel(key, field)：删除名称为 key 的 hash 中键为 field 的域 hlen(key)：返回名称为 key 的 hash 中元素个数 hkeys(key)：返回名称为 key 的 hash 中所有键 hvals(key)：返回名称为 key 的 hash 中所有键对应的 value hgetall(key)：返回名称为 key 的 hash 中所有的键（field）及其对应的 value","link":"/2019/08/22/redis-数据库/"},{"title":"spark standalone  运行原理","text":"一 、spark standalone 运行模式 standalone 运行模式 是由客户端，主节点和多个worker节点组成。Worker节点可以通过ExecutorRunner运行在当前节点上的CoarseGrainedExecutorBackend进程，每个Worker节点上存在一个或多个CoarseGrainedExecutorBackend进程，每个进程包含一个Executor对象。 该对象持有一个线程池,每个线程可以执行一个task。12345671.启动应用程序，在SparkContext启动过程中，先初始化DAGScheduler 和 TaskSchedulerImpl两个调度器， 同时初始化SparkDeploySchedulerBackend，并在其内部启动DriverEndpoint 和 ClientEndpoint2.ClientEndpoint向Master注册应用程序。Master收到注册消息后把应用放到待运行应用列表，使用自己的资源调度算法分配Worker资源给应用程序。3.应用程序获得Worker时，Master会通知Worker中的WorkerEndpoint创建CoarseGrainedExecutorBackend进程，在该进程中创建执行容器Executor。4.Executor创建完毕后发送消息到Master 和 DriverEndpoint。在SparkContext创建成功后， 等待Driver端发过来的任务。5.SparkContext分配任务给CoarseGrainedExecutorBackend执行，在Executor上按照一定调度执行任务(这些任务就是自己写的代码)6.CoarseGrainedExecutorBackend在处理任务的过程中把任务状态发送给SparkContext，SparkContext根据任务不同的结果进行处理。如果任务集处理完毕后，则继续发送其他任务集。7.应用程序运行完成后，SparkContext会进行资源回收。 二、spark standalone两种提交模式1. standalone-client 任务提交模式提交命令：12345678# Run on a Spark standalone cluster in client deploy mode./bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master spark://207.184.161.138:7077 \\ --executor-memory 20G \\ --total-executor-cores 100 \\ /path/to/examples.jar \\ 1000 例子：12345#!/bin/shresult_time=$(date +%Y%m%d)echo &quot;小车定位程序&quot;nohup spark-submit --class com.gree.cn.location.LocationMap --master spark://lyxbdw-02:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 --executor-memory 4G --total-executor-cores 12 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar &gt; /lvm/data1/spark/log/location/${result_time}_result.log &amp; standalone client 模式任务流程 执行流程 client 模式提交任务后，会在客户端启动Driver进程。 Driver 会向Master申请启动Application启动资源。 资源申请成功后，Driver端会将task发送到worker端执行。 worker端执行成功后将执行结果返回给Driver端 1. standalone-cluster 任务提交模式提交命令：12345678910# Run on a Spark standalone cluster in cluster deploy mode with supervise./bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master spark://207.184.161.138:7077 \\ --deploy-mode cluster \\ --supervise \\ --executor-memory 20G \\ --total-executor-cores 100 \\ /path/to/examples.jar \\ 1000 例子：12345#!/bin/shresult_time=$(date +%Y%m%d)echo &quot;小车定位程序&quot;spark-submit --class com.gree.cn.location.LocationMap --master spark://lyxbdw-02:7077 --deploy-mode cluster --supervise --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 --executor-memory 4G --total-executor-cores 12 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar 注意：Standalone-cluster提交方式，应用程序使用的所有jar包和文件，必须保证所有的worker节点都要有，因为此种方式，spark不会自动上传包。 standalone-cluster模式提交任务 执行流程： 客户端使用命令spark-submit –deploy-mode cluster 后会启动spark-submit进程 此进程为Driver向Master 申请资源。 Master会随机在一台Worker节点来启动Driver进程。 Driver启动成功后，spark-submit关闭，然后Driver向Master申请资源。 Master接收到请求后，会在资源充足的Worker节点上启动Executor进程。 Driver分发Task到Executor中执行。 建议：在平时调试时使用client模式，在正式生产环境时建议使用cluster模式 参考文章：1、大话Spark(5)-三图详述Spark Standalone/Client/Cluster运行模式2、Spark中Standalone的两种提交模式（Standalone-client模式与Standalone-cluster模式）","link":"/2019/08/26/spark-standalone-运行原理/"},{"title":"shell 编写 脚本常用命令","text":"统计test.sh 文件命令的行数sed -n ‘$=’ /home/hadoop/test/test.sh shell 脚本日志写入追加时间文件夹date_time=date +'%Y-%m-%d' &amp;&amp; sh /home/greesj2b/jiaoben/bomrank.sh &gt;&gt; /home/greesj2b/jiaoben/bomrank/log/bomrank_$date_time.log 2&gt;&amp;1 hive 查看表锁情况 show locks; 对表解锁unlock table tablename; 关闭锁机制set hive.support.concurrency=false;默认为true 问题一 syntax error: unexpected end of file原因：最后发现我的脚本是在window环境下编写的，然后传到linux服务器上的，这时候问题来了，doc下的文本内容格式和unix下的格式有所不同，比如dos文件传输到unix系统时,会在每行的结尾多一个^M结束符。（我的就是这个原因）解决办法： 123vim serverDeploy.sh:set fileformat=unix:wq","link":"/2019/03/15/shell-编写-脚本常用命令/"},{"title":"spark sql  java 自定义函数","text":"::: hljs-center java自定义函数 ::: //自定义函数将信号强度转距离 //其中udf+数字中数字代表入参个数，最后一个参数代表出参的数据类型 //出现序列化问题 123456spark.udf().register(&quot;transsinglepower&quot;, new UDF1&lt;Integer,Double&gt;() { @Override public Double call(Integer single_poer) throws Exception { Double mesure_distinct = Math.pow(10, (Math.abs(single_poer) - 59) / (10 * 2.0)); return mesure_distinct; }}, DataTypes.DoubleType);","link":"/2019/07/18/spark-sql-java-自定义函数/"},{"title":"spark 常用参数设置","text":"1234// task数量设置spark.sql.shuffle.partitions 50//并行度设置spark.default.parallelism 10 spark提交部署程序脚本12345#!/bin/shresult_time=$(date +%Y%m%d)echo &quot;小车定位程序&quot;spark-submit --class com.gree.cn.location.LocationMap --master spark://lyxbdw-02:7077 --deploy-mode cluster --supervise --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3 --executor-memory 4G --total-executor-cores 12 /home/lyxbdw/basedata/spark/structstreaminglocation-0.0.1-SNAPSHOT-jar-with-dependencies.jar","link":"/2019/08/09/spark-常用参数设置/"},{"title":"spring boot 注解","text":"问题一、 Could not autowire. No beans of ‘RestTemplate’ type found 在mapper 类添加注解@Repository 即可解决，简单有效 方法2：在mapper文件上加@Component注解，把普通pojo实例化到spring容器中","link":"/2019/07/16/spring-boot-注解/"},{"title":"spark 常用 api","text":"1.spark 中采用部分关联字段 12//可以看出Temp_khcp 字段包含 first_chart 字段 ，通过部分关联来实现表连接 val alphabet_Beign = number_Beign.join(custom_file_regular, col(&quot;Temp_khcp&quot;).startsWith(col(&quot;first_chart&quot;)), &quot;left&quot;).drop(&quot;first_chart&quot;).na.fill(&quot;&quot;, Seq(&quot;before_file&quot;))","link":"/2019/11/26/spark-常用-api/"},{"title":"spring boot jpa","text":"spring boot jpa jpa 常用于数据库访问 1.依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.gree.bdc&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-jpa&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;jpa-test&lt;/name&gt; &lt;description&gt;jpa-test&lt;/description&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- spring boot begin--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--mysql 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.11&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log4j--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt; &lt;version&gt;1.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring boot jpa --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2.application.properties 配置，创建入口类。12345678910111213#通用数据源配置spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driverspring.datasource.url=jdbc:mysql://localhost:3306/springboot_jpa?charset=utf8mb4&amp;useSSL=false&amp;serverTimezone=GMTspring.datasource.username=rootspring.datasource.password=123456# Hikari 数据源专用配置spring.datasource.hikari.maximum-pool-size=20spring.datasource.hikari.minimum-idle=5# JPA 相关配置spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialectspring.jpa.show-sql=truespring.jpa.hibernate.ddl-auto=create 1234567891011package com.gree.bdc.jpa;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class JpaApplication { public static void main(String[] args) { SpringApplication.run(JpaApplication.class,args); }} 3.创建实体123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.gree.bdc.jpa.entity;import javax.persistence.Column;import javax.persistence.Entity;import javax.persistence.Id;import javax.persistence.Table;@Entity@Table(name = &quot;AUTH_USER&quot;)public class UserDO { @Id private Long id; @Column(length = 32) private String name; @Column(length = 32) private String account; @Column(length = 64) private String pwd; public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getAccount() { return account; } public void setAccount(String account) { this.account = account; } public String getPwd() { return pwd; } public void setPwd(String pwd) { this.pwd = pwd; }} 4.写一个DAO继承Repository或他的子类，执行入口类创建数据库表。后注释掉配置文件 spring.jpa.hibernate.ddl-auto=create 123456789package com.gree.bdc.jpa.repository;import com.gree.bdc.jpa.entity.UserDO;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.stereotype.Repository;@Repositorypublic interface UserDAO extends JpaRepository&lt;UserDO,Long&gt; {} 5.使用DAO调用其操作数据库方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.gree.bdc.jpa;import com.gree.bdc.jpa.entity.UserDO;import com.gree.bdc.jpa.repository.UserDAO;import org.junit.After;import org.junit.Before;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;import java.util.List;@RunWith(SpringRunner.class)@SpringBootTestpublic class UserDOTest { @Autowired private UserDAO userDAO; @Before public void before(){ UserDO userDO = new UserDO(); userDO.setId(1L); userDO.setName(&quot;风清扬&quot;); userDO.setAccount(&quot;fengqy&quot;); userDO.setPwd(&quot;123456&quot;); userDAO.save(userDO); userDO = new UserDO(); userDO.setId(3L); userDO.setName(&quot;东方不败&quot;); userDO.setAccount(&quot;bubai&quot;); userDO.setPwd(&quot;123456&quot;); userDAO.save(userDO); userDO.setId(5L); userDO.setName(&quot;向问天&quot;); userDO.setAccount(&quot;wentian&quot;); userDO.setPwd(&quot;123456&quot;); userDAO.save(userDO); } @Test public void testAdd(){ UserDO userDO = new UserDO(); userDO.setId(2L); userDO.setName(&quot;任我行&quot;); userDO.setAccount(&quot;renwox&quot;); userDO.setPwd(&quot;123456&quot;); userDAO.save(userDO); userDO = new UserDO(); userDO.setId(4L); userDO.setName(&quot;令狐冲&quot;); userDO.setAccount(&quot;linghuc&quot;); userDO.setPwd(&quot;123456&quot;); userDAO.save(userDO); } @After public void after(){ userDAO.deleteById(1L); List&lt;UserDO&gt; users = userDAO.findAll(); for(UserDO user: users){ System.out.println(user.getName()); } }} 参考文档：spring boot jpa","link":"/2019/11/30/spring-boot-jpa/"},{"title":"spring cloud","text":"::: hljs-center spring cloud 常见错误以及解决办法::: 问题 1：找不到主类或加载不到主类解决办法：尝试 mvn clean ，mvn install 问题2：Module “spring-cloud-itoken”must not contain source root “C:\\Users\\hadoop\\IdeaProjects\\spring-cloud-itoken\\spring-cloud-itoken-admin\\src\\main\\java”.The root already belongs to module “spring-cloud-itoken-admin”解决办法：这类问题通常是由于多模块项目，删除了root 模块的src 目录造成的 所有找到root模块，然后将其右侧的sources 下的多余文件找到并删除即可。 问题3 ：java程序找不到包解决办法： 重新设置module sdk 的路径一般能够解决。 问题4 ：org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘com.sfz.spring.cloud.itoken.service.admin.test.admin.AdminServiceTest’: Unsatisfied dependency expressed through field ‘adminService’; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type ‘com.sfz.spring.cloud.itoken.service.admin.service.AdminService’ available: expected at least 1 bean which qualifies as autowire candidate. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)} 解决办法啊： 在 serviceImpl 加上注解 @Service 问题5 ： java.lang.IllegalStateException: Failed to load ApplicationContext 解决办法 ：未启动ConfigApplication,验证方法：打开http://127.0.0.1:8888/spring-cloud-itoken-service-admin/prod 问题6 ：Caused by: java.net.ConnectException: Connection refused: connect解决办法： 未加载注解 @ActiveProfiles(value = “prod”) 问题7 ：Failed to execute goal on project spring-cloud-itoken-web-admin: Could not resolve dependencies for project com.sfz:spring-cloud-itoken-web-admin:jar:1.0.0-SNAPSHOT: Failure to find com.sfz:spring-cloud-itoken-common-web:jar:1.0.0-SNAPSHOT in https://oss.sonatype.org/content/repositories/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of sonatype-repos-s has elapsed or updates are forced -&gt; [Help 1]解决办法： 在父项目下有的子项目在首次运行clean 和install前应该先运行父项目的clean和install 问题8：Caused by: java.lang.ClassNotFoundException: com.netflix.hystrix.contrib.javanica.aop.aspectj.HystrixCommandAspect解决办法：新增依赖1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;","link":"/2019/05/12/spring-cloud/"},{"title":"图数据集群搭建","text":"NEO4J高可用集群搭建 高可用的neo4j集群主要采用了主从的结构，来保证集群的容错能力和应变能力，同时也保证了了集群在读取密集型的数据的场景下可横向的扩展能力。同时，它还支持缓存分区，使得NEO4J高可用性集群比neo4j单实例具有更大的负载能力。但HA集群很快要不支持了。 好了，话不多说，如果看过前一篇文章https://blog.csdn.net/fffsssfff6/article/details/81215416, 完成了前半部分的一些基本准备，那么就可以直接进行HA集群搭建。若没有准备，则需要完成至JDK安装的步骤。下面就开始了： 一、首先 下载neo4j企业版的安装包。可以参考https://blog.csdn.net/xubo245/article/details/50033003. 执行下面命令123456789101112 http://dist.neo4j.org/neo4j-enterprise-3.4.0-unix.tar.gz``` 或者前往neo4j官网下载 ：https://neo4j.com/download/ .二、然后将安装包解压后分别传入到 /opt/neo4j 目录下。``` bashtar -zxvf neo4j-enterprise-3.4.0-unix.tar.gzscp -r neo4j-enterprise-3.4.0 root@master: /opt/neo4jscp -r neo4j-enterprise-3.4.0 root@slave1: /opt/neo4jscp -r neo4j-enterprise-3.4.0 root@slave2: /opt/neo4j 三 、修改配置文件neo4j.conf(重要) master节点：12345dbms.mode=HAha.server_id=1ha.initial_hosts=172.16.247.135:5001,172.16.247.132:5001,172.16.247.136:5001dbms.connectors.default_listen_address=0.0.0.0 slave1节点： 12345dbms.mode=HAha.server_id=2ha.initial_hosts=172.16.247.135:5001,172.16.247.132:5001,172.16.247.136:5001dbms.connectors.default_listen_address=0.0.0.0 slave2节点： 1234dbms.mode=HAha.server_id=3ha.initial_hosts=172.16.247.135:5001,172.16.247.132:5001,172.16.247.136:5001dbms.connectors.default_listen_address=0.0.0.0 四、启动HA集群,分别进入neo4j 目录下执行 1234./bin/neo4j start./bin/neo4j start./bin/neo4j start 五、进入localhost：7474查看集群信息","link":"/2018/08/14/图数据集群搭建/"},{"title":"修炼指南","text":"MAS学习法 认知：我们只有把知识转化为自己的语言，它才真正编程我们自己的东西。这个转换的过程就叫做认知的过程。如何提高我们的学习吸收能力，就要做到知行合一。如果说认知是大脑，那么工具就是我们的双手。所以我们需要把握两点原则: 1、不要重复造轮子 也即尽量选择已有的第三方工具来完成我们的项目，因为大部分的业务场景都能找到相关的工具来解决，这样既省时又省力。2、工具决定效率 在工作中选择工具的原则是选择使用者最多的工具。因为：Bug少、文档全、案例多。同时找到适合的工具，这样就能大大提高我们的工作效率。 选择好工具后就需要大量的积累。一般而言，我们很难记住大段的知识和相关指令。但是我们能够记住自己曾经做过的相关的项目，题目和故事。这就需要多练，练熟练透它。正所谓熟能生巧。量变引起质变。 总结一下几点 1、记录下自己每天的认知2、这些认知对应工具的那些操作3、勤加练习，巩固知识。","link":"/2019/02/18/修炼指南/"},{"title":"spring 注解 笔记","text":"@Controller和@RestController的区别知识点：@RestController注解相当于@ResponseBody ＋ @Controller合在一起的作用。 1) 如果只是使用@RestController注解Controller，则Controller中的方法无法返回jsp页面，或者html，配置的视图解析器 InternalResourceViewResolver不起作用，返回的内容就是Return 里的内容。 2) 如果需要返回到指定页面，则需要用 @Controller配合视图解析器InternalResourceViewResolver才行。 如果需要返回JSON，XML或自定义mediaType内容到页面，则需要在对应的方法上加上@ResponseBody注解。 例如： 1.使用@Controller 注解，在对应的方法上，视图解析器可以解析return 的jsp,html页面，并且跳转到相应页面 若返回json等内容到页面，则需要加@ResponseBody注解 @RequestMapping 和 @GetMapping @PostMapping 区别@GetMapping是一个组合注解，是@RequestMapping(method = RequestMethod.GET)的缩写。 @PostMapping是一个组合注解，是@RequestMapping(method = RequestMethod.POST)的缩写。","link":"/2019/04/04/spring-注解-笔记/"},{"title":"学习笔记","text":"chcp 查看windows系统编码格式 chcp 65001 将Windows格式编码设为utf-8模式 设置 hue 指定队列进行查询操作 set mapred.job.queue.name=QueueA; 设置 hue 使用 spark 引擎查询 set spark.master=yarn-cluster; //默认即为yarn-cluster模式，该参数可以不配置set hive.execution.engine=spark;set spark.eventLog.enabled=true;set spark.eventLog.dir=hdfs://cdh5/tmp/sparkeventlog;set spark.executor.memory=1g;set spark.executor.instances=50; //executor数量，默认貌似只有2个set spark.driver.memory=1g;set spark.serializer=org.apache.spark.serializer.KryoSerializer; host文件配置： C:\\Windows\\System32\\drivers\\etc 后台运行Java程序nohup java -jar basedata-1.0-SNAPSHOT.jar &amp; 7z Linux 解压命令7za x ***.7z restful api linux 测试get方法 ： curl “http://sysapp.gree.com/GreeMesOpenApi/GreeMesApi/api/services/app/SupTimeZoneJXDetailService/GetSupTimeZoneJXDetail?mlotno=&quot;9840021904060061&quot;&quot;post方法 ： curl -d ‘info={“mlotno”:”9840021904060061”}’ http://sysapp.gree.com/GreeMesOpenApi/GreeMesApi/api/services/app/SupTimeZoneJXDetailService/GetSupTimeZoneJXDetail 注意空格。 在windows下执行java包命令1java -jar xx.jar 后台执行命令123将jdk 重新命名项目名（如bom.exe）bom.exe -jar xx.jar","link":"/2019/01/18/学习笔记/"},{"title":"数据挖掘实战（2）： 信用卡诈骗分析","text":"信用卡诈骗分析目标：通过以往的交易数据分析出每笔交易是否正常，是否存在盗刷的风险。学习目标： 学习逻辑回归 信用卡欺诈属于二分类问题，欺诈交易在所有交易中比例很小，对于这种数据不平衡情况，到底采用什么样的模型评估标准更准确； 完成信用卡欺诈分析实战项目，并通过数据可视化对数据探索和模型评估进一步加强了解。 构建逻辑回归分类器 逻辑回归（logistic回归），主要解决二分类问题。在逻辑回归中使用Logistic函数也称Sigmoid函数。 函数形如S状： 算法实现理解：我们要实现一个二分类任务，0为不发生，1为发生。而通过对历史样本学习得到一个新的模型，当新的样本给出时，预测出结果。这里得到是y的一个预测概率。通常我们认为概率大于50%则发生，反之，则不发生。 在sklearn中，使用LogisticRegression()函数构建逻辑回归分类器，函数常用的构造参数： penalty ：惩罚项，取值为l1或l2，默认为l2。当模型参数满足高斯分布时用l2，当模型参数满足拉普拉斯分布时用l1; solver: 代表的是逻辑回归损失函数的优化方法。有5个参数可选，分别为liblinear、lbfgs、newton-cg、sag或saga.默认为liblinear,s适用于数据量较小的数据集，当数据量较大的时候可以选用sag或saga方法。 max_iter:算法收敛最大迭代次数，默认为10. n_jobs:拟合和预测的时候CPU的核数，默认为1.","link":"/2019/03/15/数据挖掘实战（2）：-信用卡诈骗分析/"},{"title":"数据挖掘实战（1） ： 信用卡违约率分析","text":"数据挖掘常遇见的问题，也是核心问题： 如何选择各种分类器，到底选择哪个分类算法？ 如何优化分类器参数，以便得到更好的分类器？ 三个目标： 创建各种分类器，包括已经掌握的SVM、决策树、KNN分类器，以及随机森林分类器； 掌握GridSearchCV工具，优化算法模型的参数； 使用Pipeline 管道机制进行流水线作业。（数据规范或者数据降维） 构建随机森林分类器 随机森林（Random Forest,简称 RF），他实际是一个包含多个决策树分类器，每个子分类器都是一颗CART分类回归树。所以随机森林既可以做分类也可以做回归任务。做分类时，输出的结果是每个子分类器中分类结果最多的那个；做回归时，输出的结果是每个子分类的任务输出结果的平均值。在sklearn中，使用RandomForestClassifier()构造随机森林模型，常用的参数有： 使用 GridSearchCV 工具对模型参数进行调优 GridSearchCV 是Python 的参数自动搜索模块，只要告诉它想要的调优的参数有哪些以及参数的取值范围，它就会把所以的情况都跑一遍，然后告诉我们哪个参数最优，结果如何。GridSearchCV(estimator,param_grid,cv = None,scoring=None)构造参数的自动搜索模块，主要参数说明； 简单例子：12345678910from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_irisrf = RandomForestClassifier()parameters = {&quot;n_estimators&quot;:range(1,11)}iris = load_iris()clf = GridSearchCV(estimator=rf,param_grid=parameters,cv =5)clf.fit(iris.data,iris.target)print(&quot;最优分数：%.4lf&quot; %clf.best_score_)print(&quot;最优参数：&quot;,clf.best_params_) 使用 Pipeline 管道机制进行流水作业Python 有一种 Pipeline 管道机制。管道机制就是让我们把每一步都按照顺序下来，从而创建Pipeline流水作业。每一步都采用（’名称’，步骤）的方式来表示。 下面使用Pipeline管道机制，用随机森林对iris数据集做分类任务。先用StandardScaler方法对数据规范化，然后使用随机森林分类： 123456789101112131415161718192021from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import GridSearchCVfrom sklearn.datasets import load_irisfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipelinerf = RandomForestClassifier()parameters = {&quot;randomforestclassifier__n_estimators&quot;: range(1,11)}iris = load_iris()pipeline = Pipeline([ (&apos;scaler&apos;,StandardScaler()), (&apos;randomforestclassifier&apos;,rf) ]) clf = GridSearchCV(estimator=pipeline,param_grid=parameters)clf.fit(iris.data,iris.target)print(&quot;最优分数：%.4lf&quot; %clf.best_score_)print(&quot;最优参数：&quot;,clf.best_params_) 对信用卡违约率进行分析 数据来源github上下载即可 ：信用卡违约数据这个数据集是台湾某银行05年4月到9月的信用卡数据，数据集一共25个字段，具体含义如下： 现在针对这个数据集构建一个信用卡违约率分类器。项目的流程如下： 加载数据 准备阶段：探索数据，采用数据可视化方式可以让我们对数据有更加直观的了解 分类阶段：之所以把数据规范化放到这个阶段，是因为我们采用了Pipeline 管道机制。而为了找到合适的分类算法以及相适应的分类器参数，需要多试几个分类器以及采用GridSearchCV工具找到最优参数具体代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# -*- coding: utf-8 -*-# 信用卡违约率分析import pandas as pdfrom sklearn.model_selection import learning_curve, train_test_split,GridSearchCVfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipelinefrom sklearn.metrics import accuracy_scorefrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom matplotlib import pyplot as pltimport seaborn as sns# 数据加载data = data = pd.read_csv(&apos;./UCI_Credit_Card.csv&apos;)# 数据探索print(data.shape) # 查看数据集大小print(data.describe()) # 数据集概览# 查看下一个月违约率的情况next_month = data[&apos;default.payment.next.month&apos;].value_counts()print(next_month)df = pd.DataFrame({&apos;default.payment.next.month&apos;: next_month.index,&apos;values&apos;: next_month.values})plt.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;] # 用来正常显示中文标签plt.figure(figsize = (6,6))plt.title(&apos;信用卡违约率客户\\n (违约：1，守约：0)&apos;)sns.set_color_codes(&quot;pastel&quot;)sns.barplot(x = &apos;default.payment.next.month&apos;, y=&quot;values&quot;, data=df)locs, labels = plt.xticks()plt.show()# 特征选择，去掉 ID 字段、最后一个结果字段即可data.drop([&apos;ID&apos;], inplace=True, axis =1) #ID 这个字段没有用target = data[&apos;default.payment.next.month&apos;].valuescolumns = data.columns.tolist()columns.remove(&apos;default.payment.next.month&apos;)features = data[columns].values# 30% 作为测试集，其余作为训练集train_x, test_x, train_y, test_y = train_test_split(features, target, test_size=0.30, stratify = target, random_state = 1) # 构造各种分类器classifiers = [ SVC(random_state = 1, kernel = &apos;rbf&apos;), DecisionTreeClassifier(random_state = 1, criterion = &apos;gini&apos;), RandomForestClassifier(random_state = 1, criterion = &apos;gini&apos;), KNeighborsClassifier(metric = &apos;minkowski&apos;),]# 分类器名称classifier_names = [ &apos;svc&apos;, &apos;decisiontreeclassifier&apos;, &apos;randomforestclassifier&apos;, &apos;kneighborsclassifier&apos;,]# 分类器参数classifier_param_grid = [ {&apos;svc__C&apos;:[1], &apos;svc__gamma&apos;:[0.01]}, {&apos;decisiontreeclassifier__max_depth&apos;:[6,9,11]}, {&apos;randomforestclassifier__n_estimators&apos;:[3,5,6]} , {&apos;kneighborsclassifier__n_neighbors&apos;:[4,6,8]},] # 对具体的分类器进行 GridSearchCV 参数调优def GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, param_grid, score = &apos;accuracy&apos;): response = {} gridsearch = GridSearchCV(estimator = pipeline, param_grid = param_grid, scoring = score) # 寻找最优的参数 和最优的准确率分数 search = gridsearch.fit(train_x, train_y) print(&quot;GridSearch 最优参数：&quot;, search.best_params_) print(&quot;GridSearch 最优分数： %0.4lf&quot; %search.best_score_) predict_y = gridsearch.predict(test_x) print(&quot; 准确率 %0.4lf&quot; %accuracy_score(test_y, predict_y)) response[&apos;predict_y&apos;] = predict_y response[&apos;accuracy_score&apos;] = accuracy_score(test_y,predict_y) return response for model, model_name, model_param_grid in zip(classifiers, classifier_names, classifier_param_grid): pipeline = Pipeline([ (&apos;scaler&apos;, StandardScaler()), (model_name, model) ]) result = GridSearchCV_work(pipeline, train_x, train_y, test_x, test_y, model_param_grid , score = &apos;accuracy&apos;)","link":"/2019/03/14/数据挖掘实战（1）-：-信用卡违约率分析/"},{"title":"数据分析全景图笔记","text":"数据分析的三个重要组成部分： 1、数据采集。（源头） 2、数据挖掘。（核心，主要是挖掘数据的商业价值） 3、数据可视化。（直观感受数据分析结果） 数据采集 主要是各种各样的数据源打交道，然后使用不用的工具来进行采集。 数据挖掘 第二部分主要熟悉数据挖掘的基本流程、十大算法、以及背后的数学基础。 数据可视化 数据可视化可以帮我们很好的理解数据的结构，以及分析结果的呈现。主要有两种方法实现数据的可视化。 第一种就是使用Python。在Python对数据进行清洗、挖掘的过程中，可以使用Matplotlib,Seaborn等第三方库进行呈现。 第二种就是使用第三方工具。若已经生成了csv文件，想采用所见即所得的方式呈现，可以采用微图、DataV、Data GIF Maker等第三方工具。","link":"/2019/02/18/数据分析全景图笔记/"},{"title":"数据挖掘实战（3） ： 如何对比特币走势进行预测","text":"时间序列分析： 时间序列分析模型建立了观察结果与时间变化的关系，能够帮助我们预测未来一段时间内的结果变化情况。 需要掌握的目标： 了解时间序列预测的概念，以及常用的算法，包括AR、MA、ARMA、ARIMA模型等； 掌握并使用ARMA模型工具，对一个时间序列数据进行建模和预测； 对比特币的历史数据进行时间序列建模，并预测未来6个月的走势。 时间序列预测 关于时间序列，可以简单理解为按照时间组成的数字序列。在时间序列预测模型中，经典模型有：AR、MA、ARIMA、ARMA。 AR(Auto Regressive) 又叫自回归模型。它认为过去若干个时刻的点通过线性组合，再加上白噪声就可以预测未来某个时刻的点。白噪声可以理解成一个期望为0，方差为常数的纯随机过程。AR模型还存在一个阶数，称为 AR（p）模型，也叫做 p 阶自回归模型。他指的是通过这个时刻的前p个点，通过线性组合再加上白噪声来预测当前时刻点的值。 MA（Moving Average）,又叫滑动平均模型。它与 AR模型大同小异，AR模型是历史时序值的线性组合，MA是通过历史白噪声点进行线性组合来 影响当前时刻点。同样MA模型也存在一个阶数，称为MA(q)阶模型，也叫做q阶移动平均模型。 ARMA（Auto Regressive Moving Average）,又叫自回归滑动平均模型，也是AR和MA两者的混合。相较于ARMA,ARIMA多了个差分的过程，作用是对不平稳数据进行差分平稳，在差分平稳后再进行建模。ARMA(p,q) 两个阶数，ARIMA（p,d,q）三个阶数，其中d 表示差分阶数。 ARMA 模型工具 引入相关工具包1from statsmodels.tsa.arima_model import ARMA 然后通过ARMA(endog,order,exog=None)创建ARMA类，参数说明： endog: endogenous variable,代表内生变量，又叫非政策性变量，它由模型决定，不被政策左右，可以说是我们想要分析的变量，或者说是我们这次项目需要用到的变量。 order ：代表是p和q的值，也就是ARMA中的阶数。 exog: exogenous variables,代表外生变量。外生变量和内生变量一样是经济模型中重要变量。相较于内生变量而言，外生变量有称为政策性变量，在经济机制内受外部的影响，不是我们模型要研究的变量。 代码例子：12345678910111213141516171819202122232425262728import pandas as pdimport matplotlib.pyplot as pltimport statsmodels.api as smfrom statsmodels.tsa.arima_model import ARMAfrom statsmodels.graphics.api import qqplot# 创建数据data = [5922, 5308, 5546, 5975, 2704, 1767, 4111, 5542, 4726, 5866, 6183, 3199, 1471, 1325, 6618, 6644, 5337, 7064, 2912, 1456, 4705, 4579, 4990, 4331, 4481, 1813, 1258, 4383, 5451, 5169, 5362, 6259, 3743, 2268, 5397, 5821, 6115, 6631, 6474, 4134, 2728, 5753, 7130, 7860, 6991, 7499, 5301, 2808, 6755, 6658, 7644, 6472, 8680, 6366, 5252, 8223, 8181, 10548, 11823, 14640, 9873, 6613, 14415, 13204, 14982, 9690, 10693, 8276, 4519, 7865, 8137, 10022, 7646, 8749, 5246, 4736, 9705, 7501, 9587, 10078, 9732, 6986, 4385, 8451, 9815, 10894, 10287, 9666, 6072, 5418]data = pd.Series(data)data_index = sm.tsa.datetools.dates_from_range(&apos;1901&apos;,&apos;1990&apos;)# 绘制数据图data.index = pd.Index(data_index)data.plot(figsize=(12,8))plt.show()# 创建 ARMA 模型arma = ARMA(data,(7,0)).fit()print(&apos;AIC: %0.4lf&apos; %arma.aic)# 模型预测predict_y = arma.predict(&apos;1990&apos;,&apos;2000&apos;)# 预测结果绘制fig,ax = plt.subplots(figsize=(12,8))ax = data.ix[&apos;1901&apos;:].plot(ax=ax)predict_y.plot(ax= ax)plt.show() 如何判断一个模型是否合适？ 需要引入AIC准则，也叫赤池准则，数值越小代表模型拟合的越好。 比特币预测123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# -*- coding: utf-8 -*-# 比特币走势预测，使用时间序列 ARMAimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom statsmodels.tsa.arima_model import ARMAimport warningsfrom itertools import productfrom datetime import datetimewarnings.filterwarnings(&apos;ignore&apos;)# 数据加载df = pd.read_csv(&apos;./bitcoin_2012-01-01_to_2018-10-31.csv&apos;)# 将时间作为 df 的索引df.Timestamp = pd.to_datetime(df.Timestamp)df.index = df.Timestamp# 数据探索print(df.head())# 按照月，季度，年来统计df_month = df.resample(&apos;M&apos;).mean()df_Q = df.resample(&apos;Q-DEC&apos;).mean()df_year = df.resample(&apos;A-DEC&apos;).mean()# 按照天，月，季度，年来显示比特币的走势fig = plt.figure(figsize=[15, 7])plt.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;] # 用来正常显示中文标签plt.suptitle(&apos;比特币金额（美金）&apos;, fontsize=20)plt.subplot(221)plt.plot(df.Weighted_Price, &apos;-&apos;, label=&apos;按天&apos;)plt.legend()plt.subplot(222)plt.plot(df_month.Weighted_Price, &apos;-&apos;, label=&apos;按月&apos;)plt.legend()plt.subplot(223)plt.plot(df_Q.Weighted_Price, &apos;-&apos;, label=&apos;按季度&apos;)plt.legend()plt.subplot(224)plt.plot(df_year.Weighted_Price, &apos;-&apos;, label=&apos;按年&apos;)plt.legend()plt.show()# 设置参数范围ps = range(0, 3)qs = range(0, 3)parameters = product(ps, qs)parameters_list = list(parameters)# 寻找最优 ARMA 模型参数，即 best_aic 最小results = []best_aic = float(&quot;inf&quot;) # 正无穷for param in parameters_list: try: model = ARMA(df_month.Weighted_Price,order=(param[0], param[1])).fit() except ValueError: print(&apos;参数错误:&apos;, param) continue aic = model.aic if aic &lt; best_aic: best_model = model best_aic = aic best_param = param results.append([param, model.aic])# 输出最优模型result_table = pd.DataFrame(results)result_table.columns = [&apos;parameters&apos;, &apos;aic&apos;]print(&apos;最优模型: &apos;, best_model.summary())# 比特币预测df_month2 = df_month[[&apos;Weighted_Price&apos;]]date_list = [datetime(2018, 11, 30), datetime(2018, 12, 31), datetime(2019, 1, 31), datetime(2019, 2, 28), datetime(2019, 3, 31), datetime(2019, 4, 30), datetime(2019, 5, 31), datetime(2019, 6, 30)]future = pd.DataFrame(index=date_list, columns= df_month.columns)df_month2 = pd.concat([df_month2, future])df_month2[&apos;forecast&apos;] = best_model.predict(start=0, end=91)# 比特币预测结果显示plt.figure(figsize=(20,7))df_month2.Weighted_Price.plot(label=&apos;实际金额&apos;)df_month2.forecast.plot(color=&apos;r&apos;, ls=&apos;--&apos;, label=&apos;预测金额&apos;)plt.legend()plt.title(&apos;比特币金额（月）&apos;)plt.xlabel(&apos;时间&apos;)plt.ylabel(&apos;美金&apos;)plt.show()","link":"/2019/03/18/数据挖掘实战（3）-：-如何对比特币走势进行预测/"},{"title":"日志切割脚本","text":"123456789101112131415161718192021222324252627282930313233#!/bin/shlog_dir=&apos;/home/lyxbdw/basedata/server/split&apos;monitor_file=$1echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;` &apos;初始化切割日志文件为：&apos; $monitor_file &gt;&gt; /home/lyxbdw/basedata/server/split/split.logif [ ! $1 ]then echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;输入参数为空，使用默认路径：/home/lyxbdw/basedata/server/basedata.log&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log monitor_file=/home/lyxbdw/basedata/server/basedata.log #文件的绝对路径filog_name=`basename $monitor_file`echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;切割文件名称为：&apos;$log_name &gt;&gt; /home/lyxbdw/basedata/server/split/split.logfile_size=`du $monitor_file | awk &apos;{print $1}&apos;`echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;切割文件大小为：&apos;$file_size &gt;&gt; /home/lyxbdw/basedata/server/split/split.logif [ $file_size -ge 1024000 ]then echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;文件大小大于 1024000 开始切割&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log if [ ! -d $log_dir ] then echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;日志切割路径不存在，新建....&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log mkdir /home/lyxbdw/basedata/server/split #创建保存切割文件目录,这个路径可以自行修改，保存到你想要的目录 fi echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;开始切割日志....&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;开始切割日志....&apos; cp $monitor_file /home/lyxbdw/basedata/server/$log_name-`date +%Y%m%d%H%M%S`.log #保存日志文件 echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;日志切割完成。&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;日志切割完成。&apos; echo `date &apos;+%Y-%m-%d-%H:%M:%S&apos;`&apos;文件切割&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log #记录切割日志 echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;开始清空源文件...&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;开始清空源文件...&apos; echo &quot;&quot; &gt; $monitor_file #清空tomcat的log/catalina.out文件内容 echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;清空源文件完成。&apos; &gt;&gt; /home/lyxbdw/basedata/server/split/split.log echo `date &apos;+%Y-%m-%d %H:%M:%S&apos;`&apos;清空源文件完成。&apos;fi","link":"/2019/07/02/日志切割脚本/"},{"title":"时间工具类","text":"::: hljs-center 时间工具类 ::: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200package com.gree.cn.basedata.utils;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Calendar;import java.util.Date;/** * 日期时间工具类 * @author Administrator * */public class DateUtils { public static final SimpleDateFormat TIME_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); public static final SimpleDateFormat DATE_FORMAT = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); public static final SimpleDateFormat DATEKEY_FORMAT = new SimpleDateFormat(&quot;yyyyMMdd&quot;); public static final SimpleDateFormat DATE_NOW = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;); /** * 获取某个时刻的时间信息 * */ public static String getTimeNow(){ return DATE_NOW.format(new Date()); } /** * 获取格式化某个时间 */ public static String getFormatTimeNow(){ return TIME_FORMAT.format(new Date()); } /** * 判断一个时间是否在另一个时间之前 * @param time1 第一个时间 * @param time2 第二个时间 * @return 判断结果 */ public static boolean before(String time1, String time2) { try { Date dateTime1 = TIME_FORMAT.parse(time1); Date dateTime2 = TIME_FORMAT.parse(time2); if(dateTime1.before(dateTime2)) { return true; } } catch (Exception e) { e.printStackTrace(); } return false; } /** * 判断一个时间是否在另一个时间之后 * @param time1 第一个时间 * @param time2 第二个时间 * @return 判断结果 */ public static boolean after(String time1, String time2) { try { Date dateTime1 = TIME_FORMAT.parse(time1); Date dateTime2 = TIME_FORMAT.parse(time2); if(dateTime1.after(dateTime2)) { return true; } } catch (Exception e) { e.printStackTrace(); } return false; } /** * 计算时间差值（单位为秒） * @param time1 时间1 * @param time2 时间2 * @return 差值 */ public static int minus(String time1, String time2) { try { Date datetime1 = TIME_FORMAT.parse(time1); Date datetime2 = TIME_FORMAT.parse(time2); long millisecond = datetime1.getTime() - datetime2.getTime(); return Integer.valueOf(String.valueOf(millisecond / 1000)); } catch (Exception e) { e.printStackTrace(); } return 0; } /** * 获取年月日和小时 * @param datetime 时间（yyyy-MM-dd HH:mm:ss） * @return 结果（yyyy-MM-dd_HH） */ public static String getDateHour(String datetime) { String date = datetime.split(&quot; &quot;)[0]; String hourMinuteSecond = datetime.split(&quot; &quot;)[1]; String hour = hourMinuteSecond.split(&quot;:&quot;)[0]; return date + &quot;_&quot; + hour; } /** * 获取当天日期（yyyy-MM-dd） * @return 当天日期 */ public static String getTodayDate() { return DATE_FORMAT.format(new Date()); } /** * 获取昨天的日期（yyyy-MM-dd） * @return 昨天的日期 */ public static String getYesterdayDate() { Calendar cal = Calendar.getInstance(); cal.setTime(new Date()); cal.add(Calendar.DAY_OF_YEAR, -1); Date date = cal.getTime(); return DATE_FORMAT.format(date); } /** * 格式化日期（yyyy-MM-dd） * @param date Date对象 * @return 格式化后的日期 */ public static String formatDate(Date date) { return DATE_FORMAT.format(date); } /** * 格式化时间（yyyy-MM-dd HH:mm:ss） * @param date Date对象 * @return 格式化后的时间 */ public static String formatTime(Date date) { return TIME_FORMAT.format(date); } /** * 解析时间字符串 * @param time 时间字符串 * @return Date */ public static Date parseTime(String time) { try { return TIME_FORMAT.parse(time); } catch (ParseException e) { e.printStackTrace(); } return null; } /** * 格式化日期key * @param date * @return */ public static String formatDateKey(Date date) { return DATEKEY_FORMAT.format(date); } /** * 格式化日期key * @param datekey * @return */ public static Date parseDateKey(String datekey) { try { return DATEKEY_FORMAT.parse(datekey); } catch (ParseException e) { e.printStackTrace(); } return null; } /** * 格式化时间，保留到分钟级别 * yyyyMMddHHmm * @param date * @return */ public static String formatTimeMinute(Date date) { SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMddHHmm&quot;); return sdf.format(date); }}","link":"/2019/06/14/时间工具类/"},{"title":"朴素贝叶斯","text":"使用概率分布进行分类 学习朴素贝叶斯分类器 解析RSS源数据 使用朴素贝叶斯来分析不同地区的态度通用贝叶斯公式 朴素贝叶斯优点：在数据较少的情况下仍然有效，可以处理多类别问题。缺点：对于输入数据的准备方式较为敏感适应数据类型：标称型数据贝叶斯决策理论的核心思想：选择高概率所对应的类别朴素贝叶斯它是一个简单但是及其强大的预测算法，之所以称为朴素贝叶斯是因为他有个非常强硬的假设，即它假设输入的每个变量都是独立的 。条件概率： 朴素贝叶斯文档分类应用一般过程 收集数据 准备数据：需要数值型或者布尔型数据 分析数据：在有大量特征时，绘制特征作用不大，此时使用直方图效果更好 训练算法：计算不同的独立特征的条件概率 测试算法：计算错误率 使用算法：一个常见的朴素贝叶斯应用是文档分类。 程序清单 1 ： 词表到向量的转换函数 123456789101112131415161718192021def loadDataSet(): postingList = [[&apos;my&apos;,&apos;dog&apos;,&apos;has&apos;,&apos;flea&apos;,\\ &apos;problems&apos;,&apos;help&apos;,&apos;please&apos;], [&apos;maybe&apos;,&apos;not&apos;,&apos;take&apos;,&apos;him&apos;,\\ &apos;I&apos;,&apos;love&apos;,&apos;him&apos;]]classVec = [0,1] # 1代表有侮辱性文字，0代表正常言论return postingList,classVecdef createVocabList(dataSet): vocabSet = set([]) for document in dataSet: vocabSet = vocabSet | set(document) return list(vocabSet)def setOfWords2Vec(vocabList,inputSet):returnVec = [0]*len(vocabList)for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else:print &quot;the Word : %s is not in my Vocabulary!&quot; % wordreturn returnVec 训练算法函数伪代码：123456789计算每个类别中的文档数目对每篇训练文档： 对每个类别： 如果词条出现文档中 --&gt; 增加该词条的计数值 增加所有词条的计数值 对每个类别： 对每个词条： 将该词条的数目除以总词条数目得到条件概率 返回每个类别的条件概率 程序清单2 朴素贝叶斯分类器训练函数123456789101112131415161718def trainNB0(trainMatrix,trainCategory): # 初始化概率 numTrainDocs = len(trainMatrix) numWords = len(trainMatrix[0]) pAbusive = sum(trainCategory)/float(numTrainDocs) p0Num = zeros(numWords);plNum = zeros(numWords) p0Denom = 0.0;plDenom = 0.0 # 向量相加 for i in range(numTrainDocs): if trainCategory[i] ==1; p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) p1Vect = p1Num/p1Denom p0Vect = p0Num/p0Denom return p0Vect,p1Vect,pAbusive 简单案例离散数据案例（判断性别【高，中，中】） 可知 共有三个属性，用A代表属性，A1,A2,A3分别表示 身高 = 高、体重 = 中、鞋码 = 中。 共有两个类别，用C代表类别，C1,C2 分别表示为 男、女，未知情况用C~j~ 表示。补充： （1）条件概率： P(A|B)=P(AB)/P(B) （P(B)&gt;0） (2) 乘法公式： - P(AB)=P(A|B)P(B)=P(B|A)P(A) - P(A1A2...An-1An)=P(A1)P(A2|A1)P(A3|A1A2)...P(An|A1A2...An-1) （n≥2，P(A1A2...An-1) &gt; 0） （3）全概率公式: 如果事件组B1，B2，…. 满足 1.B1，B2….两两互斥，即 Bi ∩ Bj = ∅ ，i≠j ， i,j=1，2，….，且P(Bi)&gt;0,i=1,2,….; 2.B1∪B2∪….=Ω ，则称事件组 B1,B2,…是样本空间Ω的一个划分 设 B1,B2,…是样本空间Ω的一个划分，A为任一事件，则： 现在需要求得在 A1,A2,A3 下 C~j~ 的概率： 由于P(A1A2A3) 固定，故求P(C~j~|A1A2A3)最大等价于P(A1A2A3|C~j~)P(C~j~)最大值。","link":"/2019/03/13/朴素贝叶斯/"},{"title":"蓝牙信标定位","text":"::: hljs-center 蓝牙信标定位::: 一、蓝牙信标信号强度转换 计算公式： d = 10^((abs(RSSI) - A) / (10 * n))其中：d - 计算所得距离RSSI - 接收信号强度（负值）A - 发射端和接收端相隔1米时的信号强度 n - 环境衰减因子 代码实现：123//信号强度转距离Integer signal_power = (Integer.valueOf(values.substring(34, 36), 16).shortValue()) -256 ;Double mesure_distinct = Math.pow(10, (Math.abs(signal_power) - 59) / (10 * 2.0)); 二 、base数据执行程序 1.基站数据分析 cd /home/lyxbdw/basedata/comsumer/target/java -jar analysisbasedata-1.0-SNAPSHOT.jar//后台运行程序nohup java -jar analysisbasedata-1.0-SNAPSHOT.jar &amp; 2.基站数据接收 cd /home/lyxbdw/basedata/server/target/java -jar basedata-1.0-SNAPSHOT.jar//后台运行nohup java -jar basedata-1.0-SNAPSHOT.jar &amp; 基站分析数据 nohup java -jar /home/lyxbdw/basedata/comsumer/analysisbasedata-0.0.1-SNAPSHOT.jar &gt; /home/lyxbdw/basedata/comsumer/analysisbasedata.log &amp; 基站接收数据 nohup java -jar /home/lyxbdw/basedata/server/basedata-0.0.1-SNAPSHOT.jar &gt; /home/lyxbdw/basedata/server/basedata.log &amp; 3.Linux jar 包后台运行12#!/bin/bashnohup java -jar rocketmq-console-ng-1.0.0.jar --server.port=12581 --rocketmq.config.namesrvAddr=127.0.0.1:9876 &gt; log.out &amp; 解释：1、目的是使rocketmq-console-ng-1.0.0.jar在后台运行；2、nohup 就是linux后台挂起的命令3、“&gt; log.out” 指定日志输出位置为log.out","link":"/2019/05/23/蓝牙信标定位/"},{"title":"校验工具类","text":"1、异或校验1234567ByteBuf out = Unpooled.buffer(); byte[] b = new byte[out.readableBytes()];// 进行异或校验byte temp=b[3]; for (int i = 4; i &lt;b.length-1; i++) { temp ^=b[i]; }","link":"/2019/06/15/校验工具类/"},{"title":"spark structed streaming 读取kafka数据","text":"structed streaming : Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. Structed streaming 官方文档 2.4.3官方文档 一、部分关于如何连接kafka数据，并做计算。首先引入pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.gree.cn&lt;/groupId&gt; &lt;artifactId&gt;structstreaminglocation&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;StructStreamingLocation&lt;/name&gt; &lt;description&gt;Demo project for StructStreamingLocation&lt;/description&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- spark 相关依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- sparkStreaming kafka 整合--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- sparkStreaming kafka end --&gt; &lt;!-- struct streaming kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- struct streaming kafka 结束--&gt; &lt;!-- spark 相关依赖 --&gt; &lt;!-- 数据库连接--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 数据库连接 end --&gt; &lt;!-- Redis客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 常用工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.2&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;!-- 常用工具 end --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.gree.cn.location.LocationMap&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 然后编写相应的代码逻辑：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package com.gree.cn.location;import com.gree.cn.entity.*;import com.gree.cn.sink.BaseHeartSink;import com.gree.cn.sink.LabelSink;import com.gree.cn.sink.MaterialInfoSink;import com.gree.cn.sink.MaterialRedisSink;import com.gree.cn.utils.DruidPoolUtils;import org.apache.spark.api.java.function.MapFunction;import org.apache.spark.sql.*;import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;import org.apache.spark.sql.streaming.StreamingQuery;import org.apache.spark.sql.streaming.StreamingQueryException;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.util.ArrayList;import java.util.Collections;import java.util.Properties;import static org.apache.spark.sql.functions.*;public class LocationMap { public static void main(String[] args) { //创建SparkSession SparkSession spark = SparkSession .builder() .appName(&quot;小车定位程序&quot;) //.master(&quot;local[2]&quot;) .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.local.dir&quot;, &quot;/lvm/data1/bluetooth_location/spark/location&quot;) .config(&quot;spark.ui.port&quot;, &quot;8083&quot;) .config(&quot;spark.sql.shuffle.partitions&quot;, &quot;50&quot;) .config(&quot;spark.default.parallelism&quot;, &quot;12&quot;) .getOrCreate(); //读取kafka数据 Dataset&lt;Row&gt; kafkaValue = spark.readStream() .format(&quot;kafka&quot;) .option(&quot;kafka.bootstrap.servers&quot;, &quot;10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092&quot;) .option(&quot;subscribe&quot;, &quot;topic0619&quot;) //.option(&quot;kafka.bootstrap.servers&quot;,&quot;172.16.247.129:9092&quot;) //.option(&quot;subscribe&quot;,&quot;test06&quot;) .option(&quot;startingOffsets&quot;, &quot;latest&quot;) .load() .selectExpr(&quot;CAST(value AS STRING)&quot;); //基站数据包括 接收时间，基站ID，信标ID，信号强度 Dataset&lt;BaseTable&gt; baseTableDs = kafkaValue.as(Encoders.STRING()).map( (MapFunction&lt;String, BaseTable&gt;) values -&gt; { BaseTable baseTable = new BaseTable(); //具体逻辑 ... ... ... return baseTable; } , ExpressionEncoder.javaBean(BaseTable.class)); //获取信标电量表数据 Dataset&lt;LabelData&gt; labelData = baseTableDs .filter(col(&quot;orderName&quot;).equalTo(&quot;66&quot;)) .select(col(&quot;labelID&quot;), col(&quot;electricity&quot;), col(&quot;baseID&quot;),col(&quot;recDatetime&quot;)) .as(ExpressionEncoder.javaBean(LabelData.class)); /*StreamingQuery labelDataStart =*/ labelData.repartition(2, col(&quot;labelID&quot;)) .writeStream() .outputMode(&quot;append&quot;) .foreach(new LabelSink()) .start(); //labelData.writeStream().format(&quot;console&quot;).start();} 二、如何将数据sink到mysql12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.gree.cn.sink;import com.gree.cn.entity.LabelData;import com.gree.cn.utils.DruidPoolUtils;import org.apache.spark.sql.ForeachWriter;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.SQLException;public class LabelSink extends ForeachWriter&lt;LabelData&gt; { /** * sink 信标数据到MySQL中 * */ private PreparedStatement statement; private Connection connection; public boolean open(long arg0, long arg1) { return true; } public void process(LabelData labelData) { try { connection = DruidPoolUtils.getConnection(); String insertSql = &quot; REPLACE INTO bluetooth_location.beacon_info (beaconId,electricity,stationId,updateTime) VALUES(?,?,?,?) &quot;; statement = connection.prepareStatement(insertSql); statement.setString(1,labelData.getLabelID()); statement.setInt(2,labelData.getElectricity()); statement.setString(3,labelData.getBaseID()); statement.setString(4,labelData.getRecDatetime()); statement.executeUpdate(); } catch (SQLException e) { e.printStackTrace(); }finally { DruidPoolUtils.close(connection,statement); } } @Override public void close(Throwable arg0) { }} 三、数据库连接池配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package com.gree.cn.utils;import com.alibaba.druid.pool.DruidDataSourceFactory;import javax.sql.DataSource;import java.sql.Connection;import java.sql.ResultSet;import java.sql.SQLException;import java.sql.Statement;import java.util.Properties;/** * 数据库连接池工具类 */public class DruidPoolUtils { //创建成员变量 private static DataSource dataSource; //加载配置文件 static { try { Properties properties = new Properties(); //加载类路径 properties.load(DruidPoolUtils.class.getResourceAsStream(&quot;/druid.properties&quot;)); //读取属性文件，创建连接池 dataSource = DruidDataSourceFactory.createDataSource(properties); }catch (Exception e){ e.printStackTrace(); } } //获取数据源 public static DataSource getDataSource(){ return dataSource; } //获取连接对象 public static Connection getConnection(){ try { return dataSource.getConnection(); }catch (SQLException e){ throw new RuntimeException(e); } } //释放资源 public static void close(Connection connection, Statement statement, ResultSet resultSet){ if (resultSet != null ){ try { resultSet.close(); }catch (SQLException e){ e.printStackTrace(); } } if (statement != null){ try { statement.close(); }catch (SQLException e){ e.printStackTrace(); } } if (connection != null){ try { connection.close(); }catch (SQLException e){ e.printStackTrace(); } } } public static void close(Connection connection,Statement statement){ close(connection,statement,null); }} druid.properties driverClassName=com.mysql.cj.jdbc.Driverurl=jdbc:mysql://localhost:3306?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falseusername=rootpassword=lyxbdwinitialSize=5maxActive=20maxWait=3000minIdle=3 Scala编写structed stremaing引入pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.gree.cn&lt;/groupId&gt; &lt;artifactId&gt;write2mysql&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;write2mysql&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- 常用插件 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.6&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 常用插件 结束 --&gt; &lt;!-- spark 相关依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spark streaming kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spark streaming kafka 结束--&gt; &lt;!-- struct streaming kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- struct streaming kafka 结束--&gt; &lt;!-- spark 相关依赖 结束--&gt; &lt;!-- 数据库连接--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.15&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 数据库连接 end --&gt; &lt;!-- log 日志--&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log 日志结束--&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;scala-compile-first&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;**/*.scala&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;scala-test-compile&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.gree.cn.write2mysql.Write2mysql&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 与java的差别主要是打包插件不同，编写相应逻辑123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.gree.cn.write2mysqlimport com.gree.cn.sink.MysqlSinkimport org.apache.spark.sql.SparkSession/** * 原始数据保存 * @author sfz * Time 2019-08-08 */object Write2mysql { def main(args: Array[String]): Unit = { //创建SparkSession val spark = SparkSession .builder() .appName(&quot;write2mysql&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.local.dir&quot;, &quot;/lvm/data1/bluetooth_location/spark/write2mysql&quot;) .config(&quot;spark.ui.port&quot;, &quot;8084&quot;) .getOrCreate() //引入隐式转换，保证String可以序列化 import spark.implicits._ //读取kafka数据 val kafkaValue = spark.readStream .format(&quot;kafka&quot;) //.option(&quot;kafka.bootstrap.servers&quot;, &quot;10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092&quot;) // .option(&quot;subscribe&quot;, &quot;topic0619&quot;) .option(&quot;kafka.bootstrap.servers&quot;,&quot;localhost:9092&quot;) .option(&quot;subscribe&quot;,&quot;test06&quot;) .option(&quot;startingOffsets&quot;, &quot;latest&quot;) .load() .selectExpr(&quot;CAST(value AS STRING)&quot;) //将kafka数据转化为BaseTable结构 val baseData = kafkaValue.as[String].map(values =&gt; {//具体的解析数据逻辑。。。。。。。。。 BaseTable.apply(frameHeader, equipment, frameLength, orderName, baseID, labelID, signalPower, electricity, datetime, checkName, recDatetime) } else { BaseTable.apply(frameHeader, equipment, frameLength, orderName, baseID77, labelID, signalPower, electricity, datetime, checkName, recDatetime) } }).as[BaseTable] //将数据sink到mysql数据库中 val query = baseData.writeStream .foreach(new MysqlSink) .outputMode(&quot;append&quot;) .start() query.awaitTermination() } //创建BaseTable结构 case class BaseTable(frameHeader: String, equipment: String, frameLength: String, orderName: String, baseId: String, labelId: String, signalPower: Integer, electricity: Integer, datetime: String, checkName: String, recDatetime: String)} 接着sink到mysql 数据库中1234567891011121314151617181920212223242526272829303132333435363738394041package com.gree.cn.sinkimport java.sql.{Connection, PreparedStatement, SQLException}import com.gree.cn.utils.{DateUtils, MysqlPoolUtils}import com.gree.cn.write2mysql.Write2mysql.BaseTableimport org.apache.spark.sql.ForeachWriterclass MysqlSink() extends ForeachWriter[BaseTable](){ var conn:Connection = _ var statement : PreparedStatement = _ override def open(partitionId: Long, epochId: Long): Boolean = { conn = MysqlPoolUtils.getConnection.get conn.setAutoCommit(false) true } override def process(baseTable: BaseTable): Unit = { val insertSql = &quot; &quot; statement = conn.prepareStatement(insertSql) statement.setString(1, baseTable.frameHeader) 。。。。。。 statement.setString(11, baseTable.recDatetime) statement.executeUpdate() } override def close(errorOrNull: Throwable): Unit = { try{ if ( statement != null &amp;&amp; conn != null) { statement.close() conn.close() } } catch { case e: SQLException =&gt; e.printStackTrace() } }} 最后配上数据库连接池123456789101112131415161718192021222324252627282930313233343536373839package com.gree.cn.utilsimport java.sql.Connectionimport java.util.Propertiesimport com.alibaba.druid.pool.DruidDataSourceFactoryimport javax.sql.DataSourceimport org.apache.log4j.Logger/** * @author sfz * MySql 数据库连接池 */object MysqlPoolUtils {private val log = Logger.getLogger(MysqlPoolUtils.getClass.getName)val dataSource:Option[DataSource] = { try { val druidProps = new Properties() //获取Druid连接池配置文件 val druidConfig = getClass.getResourceAsStream(&quot;/application.properties&quot;) //加载配置文件 druidProps.load(druidConfig) Some(DruidDataSourceFactory.createDataSource(druidProps)) }catch { case error :Exception =&gt; log.error(&quot;Error Create Mysql Connection &quot;,error) None }} //获取连接 def getConnection:Option[Connection] = { dataSource match { case Some(ds) =&gt; Some(ds.getConnection) case None =&gt; None } }} application.properties driver-class-name = com.mysql.cj.jdbc.Driverurl=jdbc:mysql://localhost:3306/base_data?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falseusername=rootpassword=lyxbdwfilters=stat,configinitialSize=2maxActive=20minIdle=2maxWait=60000timeBetweenEvictionRunsMillis=60000minEvictableIdleTimeMillis=300000validationQuery=SELECT 1testWhileIdle=truetestOnBorrow=falsetestOnReturn=falseremoveAbandoned=truelogAbandoned=truepoolPreparedStatements=falseremoveAbandonedTimeout=1800maxOpenPreparedStatements=100","link":"/2019/08/27/spark-structed-streaming-读取kafka数据/"},{"title":"netty 遇到的坑","text":"一、 进制转换1. 十进制转为十六进制12int valueTen = 328;String strHex = Integer.toHexString(valueTen) 2. 十六进制转十进制12String strHex = &quot;0214010977&quot;int valueTen = Integer.pareseInt(strHex,16) 二、数据类型转换1. String字符串转Int整型123String str = &quot;2019&quot;;int i = Integer.pareseInt(str);int j = Integer.valuesOf(str).intValue(); 2.Int整型转String字符串类型123int i = 2019;String str1 = i+&quot;&quot;;String str2 = String.valueOf(i); 3.String转 byte[]1234567public static byte[] strToByteArray(String str) { if (str == null) { return null; } byte[] byteArray = str.getBytes(); return byteArray;} 4.byte[]转String1234567public static String byteArrayToStr(byte[] byteArray) { if (byteArray == null) { return null; } String str = new String(byteArray); return str;} 5. byte[]转十六进制String12345678910111213public static String byteArrayToHexStr(byte[] byteArray) { if (byteArray == null){ return null; } char[] hexArray = &quot;0123456789ABCDEF&quot;.toCharArray(); char[] hexChars = new char[byteArray.length * 2]; for (int j = 0; j &lt; byteArray.length; j++) { int v = byteArray[j] &amp; 0xFF; hexChars[j * 2] = hexArray[v &gt;&gt;&gt; 4]; hexChars[j * 2 + 1] = hexArray[v &amp; 0x0F]; } return new String(hexChars);} 6. 十六进制String转byte[]123456789101112131415public static byte[] hexStrToByteArray(String str){ if (str == null) { return null; } if (str.length() == 0) { return new byte[0]; } byte[] byteArray = new byte[str.length() / 2]; for (int i = 0; i &lt; byteArray.length; i++){ String subStr = str.substring(2 * i, 2 * i + 2); byteArray[i] = ((byte)Integer.parseInt(subStr, 16)); } return byteArray;} 三、字节数组操作1. 合并数组123456789101112 /*** 合并byte[]数组 （不改变原数组）* @param byte_1* @param byte_2* @return 合并后的数组*/ public byte[] byteMerger(byte[] byte_1, byte[] byte_2){ byte[] byte_3 = new byte[byte_1.length+byte_2.length]; System.arraycopy(byte_1, 0, byte_3, 0, byte_1.length); System.arraycopy(byte_2, 0, byte_3, byte_1.length, byte_2.length); return byte_3; } 2. 截取数组123456789101112 /** * 截取byte数组 不改变原数组 * @param b 原数组 * @param off 偏差值（索引） * @param length 长度 * @return 截取后的数组 */public byte[] subByte(byte[] b,int off,int length){ byte[] b1 = new byte[length]; System.arraycopy(b, off, b1, 0, length); return b1;} 四、时间字符串转时间戳格式12345678910111213141516public class TimeFormatTest { public static void main(String[] args) throws ParseException { String time = &quot;2019-5-23 9:24:1&quot;; SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); java.util.Date date_util = sdf.parse(time); //转换为util.date java.sql.Date date_sql = new java.sql.Date(date_util.getTime());//转换为sql.date System.out.println(date_util); System.out.println(date_sql); String date = sdf.format(date_sql); System.out.println(date); date = sdf.format(date_util); System.out.println(date); } } 五、 netty和springboot整合引入pom.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.gree.cn&lt;/groupId&gt; &lt;artifactId&gt;basedata&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;basedata&lt;/name&gt; &lt;description&gt;get base data&lt;/description&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- spring boot begin--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- springboot 和 netty 整合 socket--&gt; &lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.31.Final&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring boot 和 netty整合 socket 结束--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.20&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt; &lt;version&gt;5.2.0&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;!--kafka 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.44&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log4j--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt; &lt;version&gt;1.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 其中主要引入包为：12345&lt;dependency&gt; &lt;groupId&gt;io.netty&lt;/groupId&gt; &lt;artifactId&gt;netty-all&lt;/artifactId&gt; &lt;version&gt;4.1.31.Final&lt;/version&gt; &lt;/dependency&gt; 第一步创建NettyTcoServer服务端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.gree.cn.basedata.server;import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelOption;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;import io.netty.util.concurrent.Future;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;import javax.annotation.PreDestroy;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;@Componentpublic class NettyTcpServer { private static final Logger log = LoggerFactory.getLogger(NettyTcpServer.class); //boss事件轮询线程组 //处理Accept连接事件的线程，这里线程数设置为1即可，netty处理链接事件默认为单线程，过度设置反而浪费cpu资源 private EventLoopGroup boss = new NioEventLoopGroup(1); //worker事件轮询线程组 //处理hadnler的工作线程，其实也就是处理IO读写 。线程数据默认为 CPU 核心数乘以2 private EventLoopGroup worker = new NioEventLoopGroup(); @Autowired ServerChannelInitializer serverChannelInitializer; @Value(&quot;${netty.tcp.client.port}&quot;) private Integer port; //与客户端建立连接后得到的通道对象 private Channel channel; /** * 存储client的channel * key:ip，value:Channel */ public static Map&lt;String, Channel&gt; map = new ConcurrentHashMap&lt;String, Channel&gt;(); /** * 开启Netty tcp server服务 * * @return */ public ChannelFuture start() { //启动类 ServerBootstrap serverBootstrap = new ServerBootstrap(); serverBootstrap.group(boss, worker)//组配置，初始化ServerBootstrap的线程组 .channel(NioServerSocketChannel.class)///构造channel通道工厂//bossGroup的通道，只是负责连接 .childHandler(serverChannelInitializer)//设置通道处理者ChannelHandler////workerGroup的处理器 .option(ChannelOption.SO_BACKLOG, 1024)//socket参数，当服务器请求处理程全满时，用于临时存放已完成三次握手请求的队列的最大长度。如果未设置或所设置的值小于1，Java将使用默认值50。 .childOption(ChannelOption.SO_KEEPALIVE, true);//启用心跳保活机制，tcp，默认2小时发一次心跳 //Future：异步任务的生命周期，可用来获取任务结果 ChannelFuture channelFuture1 = serverBootstrap.bind(port).syncUninterruptibly();//绑定端口，开启监听，同步等待 if (channelFuture1 != null &amp;&amp; channelFuture1.isSuccess()) { channel = channelFuture1.channel();//获取通道 log.info(&quot;Netty tcp server start success, port = {}&quot;, port); } else { log.error(&quot;Netty tcp server start fail&quot;); } return channelFuture1; } /** * 停止Netty tcp server服务 */ @PreDestroy public void destroy() { if (channel != null) { channel.close(); } try { Future&lt;?&gt; future = worker.shutdownGracefully().await(); if (!future.isSuccess()) { log.error(&quot;netty tcp workerGroup shutdown fail, {}&quot;, future.cause()); } Future&lt;?&gt; future1 = boss.shutdownGracefully().await(); if (!future1.isSuccess()) { log.error(&quot;netty tcp bossGroup shutdown fail, {}&quot;, future1.cause()); } } catch (InterruptedException e) { e.printStackTrace(); } log.info(&quot;Netty tcp server shutdown success&quot;); }} 第二步编写通道初始化流程1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.gree.cn.basedata.server;import com.gree.cn.basedata.utils.decodeutil.MyDecoder;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.socket.SocketChannel;import io.netty.handler.codec.DelimiterBasedFrameDecoder;import io.netty.handler.codec.string.StringDecoder;import io.netty.handler.codec.string.StringEncoder;import io.netty.handler.timeout.IdleStateHandler;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import java.util.concurrent.TimeUnit;@Componentpublic class ServerChannelInitializer extends ChannelInitializer&lt;SocketChannel&gt; { @Autowired ServerChannelHandler serverChannelHandler; @Override protected void initChannel(SocketChannel socketChannel) throws Exception { ChannelPipeline pipeline = socketChannel.pipeline(); //IdleStateHandler心跳机制,如果超时触发Handle中userEventTrigger()方法 pipeline.addLast(&quot;idleStateHandler&quot;, new IdleStateHandler(10, 0, 0, TimeUnit.MINUTES)); //自定义编码器 byte[] delimiterByte = new byte[2]; delimiterByte[0] = 0x02; delimiterByte[1] = 0x14; ByteBuf delimiterCode = Unpooled.copiedBuffer(delimiterByte); pipeline.addLast(new DelimiterBasedFrameDecoder(64,delimiterCode)); pipeline.addLast(&quot;decoder&quot;,new MyDecoder()); //字符串编解码器 pipeline.addLast( new StringDecoder(), new StringEncoder()); //自定义Handler pipeline.addLast(&quot;serverChannelHandler&quot;, serverChannelHandler); }} 第三步自定义处理数据，并存入kafka123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182package com.gree.cn.basedata.server;import com.gree.cn.basedata.kafka_netty.KafkaProducerSingleton;import com.gree.cn.basedata.utils.DateUtils;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandler;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import io.netty.handler.timeout.IdleState;import io.netty.handler.timeout.IdleStateEvent;import lombok.extern.slf4j.Slf4j;import org.springframework.stereotype.Component;import java.io.InputStream;import java.util.Properties;@Component@ChannelHandler.Sharable@Slf4jpublic class ServerChannelHandler extends SimpleChannelInboundHandler&lt;Object&gt; { /** * @param ctx 通道 * @param msg 消息 * @throws Exception 异常消息 */ @Override protected void channelRead0(ChannelHandlerContext ctx, Object msg) throws Exception { //kafka 生产者配置 Properties properties = new Properties(); //加载配置文件 InputStream is = ServerChannelHandler.class.getResourceAsStream(&quot;/producer.properties&quot;); properties.load(is); String substring = msg.toString().substring(8, 10); // 返回时间戳 if (substring.equals(&quot;77&quot;)) { short yeardate = Short.parseShort(DateUtils.getTimeNow().substring(0, 4)); int monthdate = Integer.parseInt(DateUtils.getTimeNow().substring(4, 6)); int daydate = Integer.parseInt(DateUtils.getTimeNow().substring(6, 8)); int hoursdate = Integer.parseInt(DateUtils.getTimeNow().substring(8, 10)); int mindate = Integer.parseInt(DateUtils.getTimeNow().substring(10, 12)); int seconddate = Integer.parseInt(DateUtils.getTimeNow().substring(12, 14)); byte[] test = new byte[13]; test[0] = 0x02; test[1] = 0x14; test[2] = 0x01; test[3] = 0x09; test[4] = 0x77; test[5] = (byte) (0x00FF &amp; yeardate); test[6] = (byte) (0x00FF &amp; (yeardate &gt;&gt; 8)); test[7] = ((byte) monthdate); test[8] = ((byte) daydate); test[9] = ((byte) hoursdate); test[10] = ((byte) mindate); test[11] = ((byte) seconddate); //生成异或校验码 byte temp = test[3]; for (int i = 4; i &lt; test.length; i++) { temp ^= test[i]; } test[12] = temp; ByteBuf out = Unpooled.buffer(); // ByteOrder order; // order = ByteOrder.LITTLE_ENDIAN; //out.order(order); out.writeBytes(test); ctx.channel().writeAndFlush(out); } //然后将传入的数据全部写入到kafka内，并加上接收时间 // VoteProduceSendmsg voteProduceSendmsg = new VoteProduceSendmsg(); // voteProduceSendmsg.sendMsg(msg+DateUtils.getFormatTimeNow(),properties); // ExecutorService executorService = Executors.newFixedThreadPool(2);// executorService.submit(new HandlerProducer(msg)); KafkaProducerSingleton kafkaProducerSingleton = KafkaProducerSingleton .getInstance(); kafkaProducerSingleton.init(&quot;topic0619&quot;); kafkaProducerSingleton.sendKafkaMessage(msg+DateUtils.getFormatTimeNow()); //kafkaProducerSingleton.close(); ctx.channel().flush(); } /** * 活跃的、有效的通道 * 第一次连接成功后进入的方法 * * @param ctx 有效的通道 * @throws Exception 异常 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { super.channelActive(ctx); log.info(&quot;tcp client &quot; + getRemoteAddress(ctx) + &quot; connect success&quot;); //往channel map中添加channel信息 NettyTcpServer.map.put(getIPString(ctx), ctx.channel()); } /** * 不活动的通道 * 连接丢失后执行的方法（client端可据此实现断线重连） * * @param ctx 不活动的通道 * @throws Exception 异常 */ @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception { //删除Channel Map中的失效Client log.info(&quot;不活动通道关闭!&quot;); NettyTcpServer.map.remove(getIPString(ctx)); ctx.close(); } /** * 异常处理 * @param ctx 通道 * @throws Exception 异常 */ @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { super.exceptionCaught(ctx, cause); //发生异常，关闭连接 log.error(&quot;引擎 {} 的通道发生异常，即将断开连接&quot;, getRemoteAddress(ctx)); ctx.close();//再次建议close } /** * 心跳机制，超时处理 * @param ctx 通道 * @param evt 事件 * @throws Exception 异常 */ @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception { String socketString = ctx.channel().remoteAddress().toString(); if (evt instanceof IdleStateEvent) { IdleStateEvent event = (IdleStateEvent) evt; if (event.state() == IdleState.READER_IDLE) { log.info(&quot;Client: &quot; + socketString + &quot; READER_IDLE 读超时&quot;); ctx.disconnect();//断开 } else if (event.state() == IdleState.WRITER_IDLE) { log.info(&quot;Client: &quot; + socketString + &quot; WRITER_IDLE 写超时&quot;); ctx.disconnect(); } else if (event.state() == IdleState.ALL_IDLE) { log.info(&quot;Client: &quot; + socketString + &quot; ALL_IDLE 总超时&quot;); ctx.disconnect(); } } } /** * 获取client对象：ip+port * * @param ctx 通道 * @return 返回值 */ private String getRemoteAddress(ChannelHandlerContext ctx) { String socketString = &quot;&quot;; socketString = ctx.channel().remoteAddress().toString(); return socketString; } /** * 获取client的ip * @param ctx 通道 * @return 返回值 */ private String getIPString(ChannelHandlerContext ctx) { String ipString = &quot;&quot;; String socketString = ctx.channel().remoteAddress().toString(); int colonAt = socketString.indexOf(&quot;:&quot;); ipString = socketString.substring(1, colonAt); return ipString; }} 12创建necat .\\nc.exe -l -p 9000","link":"/2019/05/22/netty-遇到的坑/"},{"title":"蓝牙信标定位项目总结","text":"一、项目简介： 蓝牙信标定位项目又称物流容器信息化管理系统。主要是为了解决缩短物理配送周期，提高物流配送效率问题，采用蓝牙定位技术,实时定位小车的位置以及监控小车的状态信息，达到及时优化和调整配送流程的效果。以下是蓝牙基站数据处理时序图： 数据处理流程： 作为服务端，需要接受来自客户端tcp/ip数据。(netty) 数据接收并校验成功后作出相应的动作，一部分数据需要返回时间戳，所有校验成功的数据都写入kafka。(kafka producer) 消费kafka中的数据，主要有三个部分，一是信标电量信息采集，二是基站状态信息的采集，三是小车位置的计算和物料信息的绑定（structured streaming/flink） 读取redis小车位置以及状态信息，通过websocket每隔五秒准实时推送一次最新数据到前端。(websocket) 说明：1.使用netty作为数据接入使用可以参考：netty整合spring boot2.数据接收后需要写入kafka集群中，关于kafka集群部署可以参考：kafka集群安装部署3.kafka安装后，数据写入kafka,需要创建kafka producer，可以参考：kafka相关操作4.读取kafka数据，并做相应的计算，需要用到structed streaming/flink技术，相应的需要部署spark 集群或flink集群，spark集群部署可以参考：spark 集群安装部署5.structed streaming 读取kafka 数据并做实时计算可以参考 ：structed streaming 读取kafka 数据","link":"/2019/08/26/蓝牙信标定位项目总结/"},{"title":"flink 提交任务","text":"flink 提交任务flink本地提交任务 一般在本地测试时，需要在本地自己电脑提交flink任务，本地提交分两种： 1.通过命令行的方式提交flink任务：1./bin/flink run ./examples/batch/WordCount.jar 2.通过网页ip:8081提交任务点击 add new 上传jar包 选择想要执行的jar包，点击主类，设置并行度，点击submit提交按钮 flink集群提交任务 说明：flink集群采用oozie编写shell脚本在yarn上提交flink任务，其中flink版本1.8.1，oozie(5.1.0-cdh6.3.0),hue(4.2.0-cdh6.3.0)并且集群采用了kerberos和ldap安全认证，由于oozie并未集成flink任务提交，所以需要配置kerberos认证 123456789101112131.上传jar包，flink配置文件以及kerberos的keytab到自己的工作目录2.配置kerberos认证，编辑flink-conf.yaml配置文件，新增配置security.kerberos.login.use-ticket-cache: falsesecurity.kerberos.login.keytab: ./conf/260164.keytabsecurity.kerberos.login.principal: 260164@GREE.IO3.编写shell脚本#!/bin/bashenv -i FLINK_CONF_DIR=./conf /lvm/data3/flink/flink-1.8.1/bin/flink run -m yarn-cluster -yqu root.dev1 -ynm test-flink./test.jar重新指定flink配置环境文件目录 -m 指定提交模式，-yqu 指定提交yarn队列，-ynm 指定flink任务名称。最后指定要运行的jar包需要注意的是shell脚本尽量自己手动写一遍，避免出现一些格式编码等奇怪问题4.创建oozie shell任务指定conf配置文件目录，shell脚本，以及上传的jar包指定shell的队列，默认是default队列 flink 集成 kafka 集群提交任务 由于cdh集群集成了kerberos 以及kafka集群也集成了kerberos 所以需要配置kafkad的jaas.conf文件和krb5文件123456789101112131415161718KafkaClient { com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true useKeyTab=true storeKey=true renewTicket=true keyTab=&quot;./conf/xx.keytab&quot; principal=&quot;xx@GREE.IO&quot;;};Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true storeKey=true keyTab=&quot;./conf/xx.keytab&quot; principal=&quot;xx@GREE.IO&quot;;}; krb5.conf一般都不需要更改，主要是配置kafka读取到jaas.conf文件，flink的配置则按照flink集群提交的方法完成就可以。123456789101112131415161718192021222324252627# Configuration snippets may be placed in this directory as wellincludedir /etc/krb5.conf.d/[logging]default = FILE:/lvm/data3/var/log/krb5libs.logkdc = FILE:/lvm/data3/var/log/krb5kdc.logadmin_server = FILE:/lvm/data3/var/log/kadmind.log[libdefaults]dns_lookup_realm = falsedns_lookup_kdc = falseticket_lifetime = 24hrenew_lifetime = 7dforwardable = trueudp_preference_limit = 1default_realm = GREE.IOpkinit_anchors = /etc/pki/tls/certs/ca-bundle.crt[realms]GREE.IO = { kdc = cdh-master01 admin_server = cdh-master01}[domain_realm].cdh-master01 = GREE.IOcdh-master01 = GREE.IO","link":"/2019/12/19/flink-提交任务/"},{"title":"Flink 基本概念（有状态流式处理引擎）","text":"一、什么是有状态处理 传统批次处理方法 可以看出传统批处理的方法是采用持续收取数据，以时间作为划分多个批次的依据，在周期性执行批次处理。但当出现需要就算每个小时出现事件转换次数的需求时，并且事件转换跨越了所定义的时间划分，也即两个事件出现在了，两个不同的批次当中，传统批处理会将中介运算结果带到下一个批次进行计算 ，还有出现接收的事件顺序颠倒时，传统批处理仍会将中介状态带到下一批次的运算结果中。 理想方法 第一点，要有理想方法，这个理想方法是引擎必须要有能力可以累积状态和维护状态，累积状态代表着过去历史中接收过的所有事件，会影响到输出。第二点，时间，时间意味着引擎对于数据完整性有机制可以操控，当所有数据都完全接收到后，输出计算结果。第三点，理想方法模型需要实时产生结果，但更重要的是采用新的持续性数据处理模型来处理实时数据，这样才最符合Continuous data 的特性。 流式处理 流式处理简单来讲即有一个无穷无尽的数据源在持续收取数据，以代码作为数据处理的基础逻辑，数据源的数据经过代码处理后产生出结果，然后输出，这就是流式处理的基本原理。 分布式流式处理 假设Input Streams 有很多个使用者，每个使用者都有自己的ID，如果计算每个使用者出现的次数，我们需要让同一个使用者的出现事件流到同一运算代码，这跟其他批次需要做Group by 是同样的概念，所以跟Stream 一样需要做分区，设定相应的Key，然后让同样的 Key 流到同一个 Computation instance 做同样的运算。 有状态分布式流式处理 如图，上述代码中定义了变数X，X 在数据处理过程中会进行读和写，在最后输出结果时，可以依据变数X 决定输出的内容，即状态X 会影响最终的输出结果。这个过程中，第一个重点是先进行了状态Co-partitioned key by，同样的 Key 都会流到Computation instance，与使用者出现次数的原理相同，次数即所谓的状态，这个状态一定会跟同一个Key 的事件累积在同一个 Computation instance。类似于根据输入流的Key 重新分区的状态，当分区进入 Stream 之后，这个 Stream 会累积起来的状态也变成 Copartiton .第二个重点是embeded local state backend。有状态分散式流式处理的引擎，状态可能会累积到非常大，当 Key 非常多时，状态可能就会超出单一节点的 Memory 的负荷量，这时候状态必须有状态后端去维护它；在这个状态后端在正常状况下，用In-memory 维护即可。 二、有状态流式处理的挑战 状态容错需要解决下面三个问题： 如何确保状态拥有精确一次（exactly-once guarantee）容错保证 如何在分布式场景下替多个拥有本地状态的运算子产生一个全域一致的快照（global consistent snapshot） 如何在不中断运算的前提下产生快照1.1 简单场景的精确一次容错方法 先考虑最简单的使用场景，如无限流的数据进入，后面单一的Process 进行运算，每处理完一笔计算即会累积一次状态，这种情况下如果要确保Process 产生精确一次的状态容错，每处理完一笔数据，更改完状态后进行一次快照，快照包含在队列中并与相应的状态进行对比，完成一致的快照，就能确保精确一次。 1.2 分布式状态容错 Flink作为分布式的处理引擎，在分布式的场景下，进行多个本地状态的运算，只产生一个全域一致的快照，如需要在不中断运算值的前提下产生全域一致的快照，就涉及到分散式状态容错。 关于Global consistent snapshot，当Operator 在分布式的环境中，在各个节点做运算，首先产生Global consistent snapshot 的方式就是处理每一笔数据的快照点是连续的，这笔运算流过所有的运算值，更改完所有的运算值后，能够看到每一个运算值的状态与该笔运算的位置，即可称为Consistent snapshot，当然，Global consistent snapshot 也是简易场景的延伸。 容错恢复 首先了解一下Checkpoint，上面提到连续性快照每个Operator 运算值本地的状态后端都要维护状态，也就是每次将产生检查点时会将它们传入共享的DFS 中。当任何一个Process 挂掉后，可以直接从三个完整的Checkpoint 将所有的运算值的状态恢复，重新设定到相应位置。Checkpoint的存在使整个Process 能够实现分散式环境中的Exactly-once。 1.3 分布式快照的方法（distribution snapshot） 关于Flink 如何在不中断运算的状况下持续产生Global consistent snapshot，其方式是基于用 Simple lamport 演算法机制下延伸的。已知的一个点Checkpoint barrier，Flink 在某个Datastream 中会一直安插Checkpoint barrier，Checkpoint barrier 也会N – 1等等，Checkpoint barrier N 代表着所有在这个范围里面的数据都是Checkpoint barrier N。 举例：假设现在需要产生Checkpoint barrier N，但实际上在Flink 中是由Job manager 触发Checkpoint，Checkpoint 被触发后开始从数据源产生Checkpoint barrier。当Job 开始做Checkpoint barrier N 的时候，可以理解为Checkpoint barrier N 需要逐步填充左下角的表格。 如图，当部分事件标为红色，Checkpoint barrier N 也是红色时，代表着这些数据或事件都由Checkpoint barrier N 负责。Checkpoint barrier N 后面白色部分的数据或事件则不属于Checkpoint barrier N。 在以上的基础上，当数据源收到Checkpoint barrier N 之后会先将自己的状态保存，以读取Kafka资料为例，数据源的状态就是目前它在Kafka 分区的位置，这个状态也会写入到上面提到的表格中。下游的Operator 1 会开始运算属于Checkpoint barrier N 的数据，当Checkpoint barrier N 跟着这些数据流动到Operator 1 之后,Operator 1 也将属于Checkpoint barrier N 的所有数据都反映在状态中，当收到Checkpoint barrier N 时也会直接对Checkpoint去做快照。 当快照完成后继续往下游走，Operator 2 也会接收到所有数据，然后搜索Checkpoint barrier N 的数据并直接反映到状态，当状态收到Checkpoint barrier N 之后也会直接写入到Checkpoint N 中。以上过程到此可以看到Checkpoint barrier N 已经完成了一个完整的表格，这个表格叫做Distributed Snapshots，即分布式快照。分布式快照可以用来做状态容错，任何一个节点挂掉的时候可以在之前的Checkpoint 中将其恢复。继续以上Process，当多个Checkpoint 同时进行，Checkpoint barrier N 已经流到Job manager 2，Flink job manager 可以触发其他的Checkpoint，比如Checkpoint N + 1，Checkpoint N + 2 等等也同步进行，利用这种机制，可以在不阻挡运算的状况下持续地产生Checkpoint。 状态维护 状态维护即用一段代码在本地维护状态值，当状态值非常大时需要本地的状态后端来支持。 图中分别为： 状态识别id,状态数据形态资讯，注册状态如图，在Flink 程序中，可以采用getRuntimeContext().getState(desc); 这组API 去注册状态。Flink 有多种状态后端，采用API 注册状态后，读取状态时都是通过状态后端来读取的。Flink 有两种不同的状态值，也有两种不同的状态后端： -JVM Heap状态后端，适合数量较小的状态，当状态量不大时就可以采用JVM Heap 的状态后端。JVM Heap 状态后端会在每一次运算值需要读取状态时，用Java object read / writes 进行读或写，不会产生较大代价，但当Checkpoint 需要将每一个运算值的本地状态放入Distributed Snapshots 的时候，就需要进行序列化了。 -RocksDB状态后端，它是一种out of core 的状态后端。在Runtime 的本地状态后端让使用者去读取状态的时候会经过磁盘，相当于将状态维护在磁盘里，与之对应的代价可能就是每次读取状态时，都需要经过序列化和反序列化的过程。当需要进行快照时只将应用序列化即可，序列化后的数据直接传输到中央的共享DFS 中。 Flink目前支持以上两种状态后端，一种是纯 Memory 的状态后端，另一种是有资源磁盘的状态后端，在维护状态时可以根据状态的数量选择相应的状态后端。 event-time process3.1 不同的时间种类 在Flink 及其他进阶的流式处理引擎出现之前，大数据处理引擎一直只支持Processing-time 的处理。假设定义一个运算 Windows 的窗口，Windows 运算设定每小时进行结算。Processing-time 进行运算时，可以发现数据引擎将3 点至4 点间收到的数据进行结算。实际上在做报表或者分析结果时是想了解真实世界中3 点至4 点之间实际产生数据的输出结果，了解实际数据的输出结果就必须采用Event – Time 了。 如图，Event – Time 相当于事件，它在数据最源头产生时带有时间戳，后面都需要用时间戳来进行运算。用图来表示，最开始的队列收到数据，每小时对数据划分一个批次，这就是Event – Time Process 在做的事情。 3.2 Event-Time处理 Event – Time 是用事件真实产生的时间戳去做Re-bucketing，把对应时间3 点到4 点的数据放在3 点到4 点的Bucket，然后Bucket 产生结果。所以Event – Time 跟Processing – time 的概念是这样对比的存在。 Event – Time 的重要性在于记录引擎输出运算结果的时间。简单来说，流式引擎连续24 小时在运行、搜集资料，假设Pipeline 里有一个 Windows Operator 正在做运算，每小时能产生结果，何时输出 Windows的运算值，这个时间点就是Event – Time 处理的精髓，用来表示该收的数据已经收到。 3.3 Watermarks(水位线) Flink实际上是用 Watermarks 来实现Event – Time 的功能。Watermarks 在Flink 中也属于特殊事件，其精髓在于当某个运算值收到带有时间戳“ T ”的 Watermarks 时就意味着它不会接收到新的数据了。使用Watermarks 的好处在于可以准确预估收到数据的截止时间。举例，假设预期收到数据时间与输出结果时间的时间差延迟5 分钟，那么Flink 中所有的 Windows Operator 搜索3 点至4 点的数据，但因为存在延迟需要再多等5分钟直至收集完4：05 分的数据，此时方能判定4 点钟的资料收集完成了，然后才会产出3 点至4 点的数据结果。这个时间段的结果对应的就是 Watermarks 的部分。 状态保存与迁移 流式处理应用无时无刻不在运行，运维上有几个重要考量： 如何将前一步执行的的状态迁移到新的执行上 如何重新定义运行的平行化程度 Checkpoint完美符合以上需求，不过Flink 中还有另外一个名词保存点（Savepoint），当手动产生一个Checkpoint 的时候，就叫做一个Savepoint。Savepoint 跟Checkpoint 的差别在于Checkpoint是Flink 对于一个有状态应用在运行中利用分布式快照持续周期性的产生Checkpoint，而Savepoint 则是手动产生的Checkpoint，Savepoint 记录着流式应用中所有运算元的状态。 如图，Savepoint A 和Savepoint B，无论是变更底层代码逻辑、修bug 或是升级Flink 版本，重新定义应用、计算的平行化程度等，最先需要做的事情就是产生Savepoint。 Savepoint产生的原理是在Checkpoint barrier 流动到所有的Pipeline 中手动插入从而产生分布式快照，这些分布式快照点即Savepoint。Savepoint 可以放在任何位置保存，当完成变更时，可以直接从Savepoint 恢复、执行。 从Savepoint 的恢复执行需要注意，在变更应用的过程中时间在持续，如Kafka 在持续收集资料，当从Savepoint 恢复时，Savepoint 保存着Checkpoint 产生的时间以及Kafka 的相应位置，因此它需要恢复到最新的数据。无论是任何运算，Event – Time 都可以确保产生的结果完全一致。 假设恢复后的重新运算用Process Event – Time，将 Windows 窗口设为1 小时，重新运算能够在10 分钟内将所有的运算结果都包含到单一的 Windows 中。而如果使用Event – Time，则类似于做Bucketing。在Bucketing 的状况下，无论重新运算的数量多大，最终重新运算的时间以及Windows 产生的结果都一定能保证完全一致。 四、总结 本文首先从Apache Flink 的定义、架构、基本原理入手，对大数据流计算相关的基本概念进行辨析，在此基础上简单回顾了大数据处理方式的历史演进以及有状态的流式数据处理的原理，最后从目前有状态的流式处理面临的挑战分析Apache Flink 作为业界公认为最好的流计算引擎之一所具备的天然优势。希望有助于大家厘清大数据流式处理引擎涉及的基本概念，能够更加得心应手的使用Flink。 参考文章：Apache Flink 零基础入门（一&amp;二）：基础概念解析","link":"/2019/12/27/Flink-基本概念（有状态流式处理引擎）/"},{"title":"flink 基本概念","text":"一、Apache flink 基本概念，原理和架构 Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Apache Flink 是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态或无状态的计算，能够部署在各种集群环境，对各种规模大小的数据进行快速计算。 1. Flink Application 首先了解Flink的Streams、State、Time等基本处理语义以及Flink兼顾灵活性和方便性的多层次API。 Streams：流，分为有限数据流与无限数据流，unbounded stream 是有始无终的数据流，即无限数据流；而bounded stream 是限定大小的有始有终的数据集合，即有限数据流，二者的区别在于无限数据流的数据会随时间的推演而持续增加，计算持续进行且不存在结束的状态，相对的有限数据流数据大小固定，计算最终会完成并处于结束的状态。 State，状态是计算过程中的数据信息，在容错恢复和Checkpoint 中有重要的作用，流计算在本质上是Incremental Processing，因此需要不断查询保持状态；另外，为了确保Exactly- once 语义，需要数据能够写入到状态中；而持久化存储，能够保证在整个分布式系统运行失败或者挂掉的情况下做到Exactly- once，这是状态的另外一个价值。flink的状态管理还需加强研究。(Application state is a first-class citizen in flink) Time，分为Event time、Ingestion time、Processing time，Flink 的无限数据流是一个持续的过程，时间是我们判断业务状态是否滞后，数据处理是否及时的重要依据。（平常主要用Event Time 和 processing time） Event Time is the time when an event was created. It is usually described by a timestamp in the event.Ingestion time is the time when an event entrty the flink dataflow at the source operator.Processing Time is the local time at each operator that performs a time-based operation. API，API 通常分为三层，由上而下可分为SQL / Table API、DataStream API、ProcessFunction 三层，API 的表达能力及业务抽象能力都非常强大，但越接近SQL 层，表达能力会逐步减弱，抽象能力会增强，反之，ProcessFunction 层API 的表达能力非常强，可以进行多种灵活方便的操作，但抽象能力也相对越小。(目前主要使用 DataStream API 和 ProcessFunction) ProcessFunctions ：the most expressive function interfaces that Flink oﬀers. Flink provides ProcessFunctions to process individual events from one or two input streams or events that were grouped in a window.DataStreamAPI ：provides primitives for many common stream processing operations, such as windowing, record-at-a-time transformations, and enriching events by querying an external data store.SQL/TableAPI : relational APIs. 2.Flink Architecture 主要为以下四个部分： 第一，Flink 具备统一的框架处理有界和无界两种数据流的能力 第二， 部署灵活，Flink 底层支持多种资源调度器，包括Yarn、Kubernetes 等。Flink 自身带的Standalone 的调度器，在部署上也十分灵活。 第三， 极高的可伸缩性，可伸缩性对于分布式系统十分重要，阿里巴巴双11大屏采用Flink 处理海量数据，使用过程中测得Flink 峰值可达17 亿/秒。 第四， 极致的流式处理性能。Flink 相对于Storm 最大的特点是将状态语义完全抽象到框架中，支持本地状态读取，避免了大量网络IO，可以极大提升状态存取的性能。 3. Flink Operation后面会有专门课程讲解，此处简单分享Flink 关于运维及业务监控的内容： Flink具备7 X 24 小时高可用的SOA（面向服务架构），原因是在实现上Flink 提供了一致性的Checkpoint。Checkpoint是Flink 实现容错机制的核心，它周期性的记录计算过程中Operator 的状态，并生成快照持久化存储。当Flink 作业发生故障崩溃时，可以有选择的从Checkpoint 中恢复，保证了计算的一致性。 Flink本身提供监控、运维等功能或接口，并有内置的WebUI，对运行的作业提供DAG 图以及各种Metric 等，协助用户管理作业状态。 4.Flink 场景应用4.1 Flink 场景应用：Data PipelineData Pipeline 的核心场景类似于数据搬运并在搬运的过程中进行部分数据清洗或者处理，而整个业务架构图的左边是Periodic ETL，它提供了流式ETL 或者实时ETL，能够订阅消息队列的消息并进行处理，清洗完成后实时写入到下游的Database或File system 中。场景举例： 实时数仓 当下游要构建实时数仓时，上游则可能需要实时的Stream ETL。这个过程会进行实时清洗或扩展数据，清洗完成后写入到下游的实时数仓的整个链路中，可保证数据查询的时效性，形成实时数据采集、实时数据处理以及下游的实时Query。 搜索引擎推荐 搜索引擎这块以淘宝为例，当卖家上线新商品时，后台会实时产生消息流，该消息流经过Flink 系统时会进行数据的处理、扩展。然后将处理及扩展后的数据生成实时索引，写入到搜索引擎中。这样当淘宝卖家上线新商品时，能在秒级或者分钟级实现搜索引擎的搜索。 4.2 Flink 场景应用：Data AnalyticsData Analytics，如图，左边是Batch Analytics，右边是Streaming Analytics。Batch Analytics 就是传统意义上使用类似于Map Reduce、Hive、Spark Batch 等，对作业进行分析、处理、生成离线报表；Streaming Analytics 使用流式分析引擎如Storm、Flink 实时处理分析数据，应用较多的场景如实时大屏、实时报表。 4.3 Flink 场景应用：Data Driven从某种程度上来说，所有的实时的数据处理或者是流式数据处理都是属于Data Driven，流计算本质上是Data Driven 计算。应用较多的如风控系统，当风控系统需要处理各种各样复杂的规则时，Data Driven 就会把处理的规则和逻辑写入到Datastream 的API 或者是ProcessFunction 的API 中，然后将逻辑抽象到整个Flink 引擎，当外面的数据流或者是事件进入就会触发相应的规则，这就是Data Driven 的原理。在触发某些规则后，Data Driven 会进行处理或者是进行预警，这些预警会发到下游产生业务通知，这是Data Driven 的应用场景，Data Driven 在应用上更多应用于复杂事件的处理。 参考资料：Apache Flink 零基础入门（一&amp;二）：基础概念解析","link":"/2019/12/25/flink-基本概念/"},{"title":"scala 引包中遇到问题","text":"1.Error:(12, 30) could not find implicit value for evidence parameter of type org.apache.flink.api.common.typeinfo.TypeInformation[(String, String)] val value = env.addSource(new HBaseReader) 解决方法：import org.apache.flink.api.scala._ 2.Scala编程中常见错误：Error:(28, 21) value foreach is not a member of java.util.List[String] 解决方法：因为 Java 集合类型在 Scala 操作时没有 foreach 方法, 所以需要将其转换为Scala的集合类型,因此需要在代码中加入如下内容(Scala支持与Java的隐式转换),import scala.collection.JavaConversions._","link":"/2019/12/29/scala-引包中遇到问题/"},{"title":"Flink DataStream Api 编程","text":"流处理基本概念 对于什么是流处理，从不同的角度有不同的定义。其实流处理与批处理这两个概念是对立统一的，它们的关系有点类似于对于 Java 中的 ArrayList 中的元素，是直接看作一个有限数据集并用下标去访问，还是用迭代器去访问。 流处理系统本身有很多自己的特点。一般来说，由于需要支持无限数据集的处理，流处理系统一般采用一种数据驱动的处理方式。它会提前设置一些算子，然后等到数据到达后对数据进行处理。为了表达复杂的计算逻辑，包括 Flink 在内的分布式流处理引擎一般采用 DAG 图来表示整个计算逻辑，其中 DAG 图中的每一个点就代表一个基本的逻辑单元，也就是前面说的算子。由于计算逻辑被组织成有向图，数据会按照边的方向，从一些特殊的 Source 节点流入系统，然后通过网络传输、本地传输等不同的数据传输方式在算子之间进行发送和处理，最后会通过另外一些特殊的 Sink 节点将计算结果发送到某个外部系统或数据库中。 总结一下：流数据本身就是源源不断的产生，流数据处理就是提前设置一些算子，然后等待数据流到这些提前设置的算子进行处理的过程。而实际在分布式的情况下，需要考虑不同的实例之间数据传输。 对于实际的分布式流处理引擎，它们的实际运行时物理模型要更复杂一些，这是由于每个算子都可能有多个实例。如图 2 所示，作为 Source 的 A 算子有两个实例，中间算子 C 也有两个实例。在逻辑模型中，A 和 B 是 C 的上游节点，而在对应的物理逻辑中，C 的所有实例和 A、B 的所有实例之间可能都存在数据交换。在物理模型中，我们会根据计算逻辑，采用系统自动优化或人为指定的方式将计算工作分布到不同的实例中。只有当算子实例分布到不同进程上时，才需要通过网络进行数据传输，而同一进程中的多个实例之间的数据传输通常是不需要通过网络的。 Flink DataStram API 概览 首先用一个简单例子说明： //1、设置运行环境StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();//2、配置数据源读取数据DataStream text = env.readTextFile (“input”);//3、进行一系列转换DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = text.flatMap(new Tokenizer()).keyBy(0).sum(1);//4、配置数据汇写出数据counts.writeAsText(“output”);//5、提交执行env.execute(“Streaming WordCount”); 为了实现流式 Word Count，我们首先要先获得一个 StreamExecutionEnvironment 对象。它是我们构建图过程中的上下文对象。基于这个对象，我们可以添加一些算子。对于流处理程度，我们一般需要首先创建一个数据源去接入数据。在这个例子中，我们使用了 Environment 对象中内置的读取文件的数据源。这一步之后，我们拿到的是一个 DataStream 对象，它可以看作一个无限的数据集，可以在该集合上进行一序列的操作。例如，在 Word Count 例子中，我们首先将每一条记录（即文件中的一行）分隔为单词，这是通过 FlatMap 操作来实现的。调用 FlatMap 将会在底层的 DAG 图中添加一个 FlatMap 算子。然后，我们得到了一个记录是单词的流。我们将流中的单词进行分组（keyBy），然后累积计算每一个单词的数据（sum(1)）。计算出的单词的数据组成了一个新的流，我们将它写入到输出文件中。最后，我们需要调用 env#execute 方法来开始程序的执行。需要强调的是，前面我们调用的所有方法，都不是在实际处理数据，而是在构通表达计算逻辑的 DAG 图。只有当我们将整个图构建完成并显式的调用 Execute 方法后，框架才会把计算图提供到集群中，接入数据并执行实际的逻辑。基于流式 Word Count 的例子可以看出，基于 Flink 的 DataStream API 来编写流处理程序一般需要三步：通过 Source 接入数据、进行一系统列的处理以及将数据写出。最后，不要忘记显式调用 Execute 方式，否则前面编写的逻辑并不会真正执行。 从上面的例子中还可以看出，Flink DataStream API 的核心，就是代表流数据的 DataStream 对象。整个计算逻辑图的构建就是围绕调用 DataStream 对象上的不同操作产生新的 DataStream 对象展开的。整体来说，DataStream 上的操作可以分为四类。第一类是对于单条记录的操作，比如筛除掉不符合要求的记录（Filter 操作），或者将每条记录都做一个转换（Map 操作）。第二类是对多条记录的操作。比如说统计一个小时内的订单总成交量，就需要将一个小时内的所有订单记录的成交量加到一起。为了支持这种类型的操作，就得通过 Window 将需要的记录关联到一起进行处理。第三类是对多个流进行操作并转换为单个流。例如，多个流可以通过 Union、Join 或 Connect 等操作合到一起。这些操作合并的逻辑不同，但是它们最终都会产生了一个新的统一的流，从而可以进行一些跨流的操作。最后， DataStream 还支持与合并对称的操作，即把一个流按一定规则拆分为多个流（Split 操作），每个流是之前流的一个子集，这样我们就可以对不同的流作不同的处理。 参考文献：Apache Flink 零基础入门（四）：DataStream API 编程","link":"/2019/12/30/Flink-DataStream-Api-编程/"},{"title":"nifi process 介绍与使用","text":"GetJMSQueue 作用：读取activeMQ消息队列的数据 主要 URL ：tcp://localhost:61616 需要将ip地址写入到host文件中Destination Name : 消息队列的名称 2.Mysql –&gt; kafka 主要读取mysql数据 ExecuteSQL主要配置 Database Connection Pooling Service 配置 kafaka 生产者 3.Mysql增量 –》kafka 主要将 ExecuteSQL 替换为 QueryDatabaseTable 主要配置Maximum-value Columns 增量字段，nifi每次请求数据时都取前一次字段数据后的数据，保证增量读取数据，避免重复读。","link":"/2020/01/09/nifi-process-介绍与使用/"},{"title":"flink sources","text":"activemq 1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.gree.bdc&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.8.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-activemq_2.11&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.gree.bdc.io;import org.apache.activemq.ActiveMQConnectionFactory;import org.apache.flink.streaming.api.functions.source.SourceFunction;import javax.jms.*;/** * 获取activeMQ数据 */public class FlinkActiveMQStreamSources implements SourceFunction { private Connection connection = null; private Session session = null; private MessageConsumer messageConsumer = null; private boolean running = Boolean.TRUE; public void run(SourceContext ctx) throws Exception { ActiveMQConnectionFactory activeMQConnectionFactory = new ActiveMQConnectionFactory(ActiveMQConnectionFactory.DEFAULT_USER, ActiveMQConnectionFactory.DEFAULT_PASSWORD, ActiveMQConnectionFactory.DEFAULT_BROKER_URL); try { connection = activeMQConnectionFactory.createConnection(); connection.start(); session = connection.createSession(Boolean.FALSE,Session.AUTO_ACKNOWLEDGE); String queueName = &quot;test?consumer.exclusive=true&quot;; Destination destination = session.createQueue(queueName); messageConsumer = session.createConsumer(destination); while (running){ TextMessage message = (TextMessage)messageConsumer.receive(60 * 100); if (message instanceof TextMessage){ TextMessage textMessage =(TextMessage) message; String text = textMessage.getText(); ctx.collect(text); }else { System.out.println(&quot;未收到消息！！！&quot;); } } }catch (Exception e){ e.printStackTrace(); }finally { try { messageConsumer.close(); session.close(); connection.close(); }catch (JMSException e){ e.printStackTrace(); }finally { Thread.sleep(10 * 1000); } } } public void cancel() { running = Boolean.FALSE; }}","link":"/2020/01/10/flink-sources/"},{"title":"2019年度总结","text":"1.回顾123456主要作业1.机器学习（Python，spark ml）2.成品码大数据分析（Scala ，spark sql ,hive）3.微服务架构（spring boot ，spring cloud等）4.蓝牙定位（netty -》spark streaming ， structured streaming -&gt; redis,mysql --&gt; websocket）5.电工 （redis ，flink，java） 2.详细分析 1234567891011121314 2019学了很多，但真正精通，学的深的不多。1.年初学习了机器学习，主要是python的机器学习的包，但之后未用到实际的项目中，现在基本忘的差不多了，也就不多说了。2.成品码大数据分析项目，主要分为两部分，前一个月主要熟悉项目，这个项目是半路接手，准备第二期，我主要负责的数据处理部分。将需要的数据通过spark sql 处理形成中间结果表，给到后台进一步处理。后半部分则是彻底整合家用，商用，生活电器形成一张大而完整的表。主要学习到了spark 中 dataframe中的api,常用的有 spark.table获取hive数据仓库中的表，.where用来筛选符合条件的记录或者用.filter也是同样的作用，.startWith用来获取字段开始符合的条件，还有substr,na.fill,withColumn,lit,join，seq(),union等等，在项目中使用正则匹配进行数据处理时还是不够灵活，在与洪工对接需求时，需求较明确，反馈问题也能够及时得到回应。3.微服务架构，最近微服务也很流行，主要了解了spring boot 和 spring cloud 相关的知识，仅限于简单的使用，没有在复杂的场景中运用。4.蓝牙定位是本年最主要（即时间最久）的项目，从数据的接入到数据处理生成实时的定位信息传入到前端，在这个项目中学到了很多，也接触到了之前未接触到的的东西，比如kafak集群搭建（三个节点没有权限zk自己自带的），spark 集群（三个节点standalone模式）搭建，只是单独的spark集群没有hdfs。同时也接触到netty，网络之间数据传输（tcp/udp）数据传输。在做项目是由于前期对数据对接时考虑的问题不是很充足，以及对netty和kafak等一些参数不够了解耽误了不少时间，其中关于流处理的接触让我对流处理的一些框架有一定的了解，同时也了解了flink这个流处理的利器。总的来说还是完成了这个项目，尽管这个项目总是在关键时刻出现问题，但都合理的解决了。5.电工这个项目是临时接收的，当初只是简单的处理了一个上报的数据，替换相关数据源，将mysql数据源换成redis数据源，这里简单说明下，由于上个项目了解到redis，加之这个需求改动不大所有=以很快解决了，之后再很长一段时间是在看之前交接人写的代码，插一句由于项目交接多手，而且项目没有正规需求文档和相关的数据字典，难受。之后接了个上报收线重量的需求，由于之前的种种，以及flink窗口处理的能力出众，所以选择flink，数据源从activemq中读取，处理后上报给webservice服务。大致这样。下面配上几张数据流程图。 1.电工数据流图 2.蓝牙定位数据流图 3.微服务组件关系图 3.展望1多读书多看报，少吃零食多睡觉。","link":"/2020/01/18/2019年度总结/"},{"title":"shiro","text":"shiro Apache Shiro 是一个强大易用的 Java 安全框架，提供了认证、授权、加密和会话管理等功能，对于任何一个应用程序，Shiro 都可以提供全面的安全管理服务。 Authentication：身份认证 / 登录，验证用户是不是拥有相应的身份； Authorization：授权，即权限验证，验证某个已认证的用户是否拥有某个权限；即判断用户是否能做事情，常见的如：验证某个用户是否拥有某个角色。或者细粒度的验证某个用户对某个资源是否具有某个权限； Session Manager：会话管理，即用户登录后就是一次会话，在没有退出之前，它的所有信息都在会话中；会话可以是普通 JavaSE 环境的，也可以是如 Web 环境的； Cryptography：加密，保护数据的安全性，如密码加密存储到数据库，而不是明文存储； Web Support：Web 支持，可以非常容易的集成到 Web 环境； Caching：缓存，比如用户登录后，其用户信息、拥有的角色 / 权限不必每次去查，这样可以提高效率； Concurrency：shiro 支持多线程应用的并发验证，即如在一个线程中开启另一个线程，能把权限自动传播过去； Testing：提供测试支持； Run As：允许一个用户假装为另一个用户（如果他们允许）的身份进行访问； Remember Me：记住我，这个是非常常见的功能，即一次登录后，下次再来的话不用登录了。 Subject：主体，代表了当前 “用户”，这个用户不一定是一个具体的人，与当前应用交互的任何东西都是 Subject，如网络爬虫，机器人等；即一个抽象概念；所有 Subject 都绑定到 SecurityManager，与 Subject 的所有交互都会委托给 SecurityManager；可以把 Subject 认为是一个门面；SecurityManager 才是实际的执行者； SecurityManager：安全管理器；即所有与安全有关的操作都会与 SecurityManager 交互；且它管理着所有 Subject；可以看出它是 Shiro 的核心，它负责与后边介绍的其他组件进行交互，如果学习过 SpringMVC，你可以把它看成 DispatcherServlet 前端控制器； Realm：域，Shiro 从从 Realm 获取安全数据（如用户、角色、权限），就是说 SecurityManager 要验证用户身份，那么它需要从 Realm 获取相应的用户进行比较以确定用户身份是否合法；也需要从 Realm 得到用户相应的角色 / 权限进行验证用户是否能进行操作；可以把 Realm 看成 DataSource，即安全数据源。 参考：跟我学shiro","link":"/2020/02/26/shiro/"},{"title":"spark 常用操作","text":"一、spark 分组计算（转rdd 分组计算完再转回 dateset）1234567891011121314151617181920212223242526272829JavaRDD&lt;Row&gt; rowJavaRdd = orderDistanceCustomer.javaRDD();JavaRDD&lt;OrderLeaveTime&gt; leaveTimeJavaRDD = rowJavaRdd.groupBy(row -&gt; row.getString(1)) .map(new Function&lt;Tuple2&lt;String, Iterable&lt;Row&gt;&gt;, OrderLeaveTime&gt;() { @Override public OrderLeaveTime call(Tuple2&lt;String, Iterable&lt;Row&gt;&gt; v1) throws Exception { String order_number = v1._1; Iterator&lt;Row&gt; iterator = v1._2.iterator(); String report_time = &quot;&quot;; String flag = &quot;&quot;; while (iterator.hasNext()) { Row next = iterator.next(); double order_distance_customer = next.getDouble(14); report_time = next.getString(11); if (order_distance_customer - 2000 &lt; 0) { flag = &quot;已进入&quot;; } if (&quot;已进入&quot;.equals(flag) &amp;&amp; order_distance_customer - 2000 &gt; 0) { break; } } OrderLeaveTime orderArriveTime1 = new OrderLeaveTime(); orderArriveTime1.setOrderNumber(order_number); orderArriveTime1.setLeaveTime(report_time); return orderArriveTime1; } }); Encoder&lt;OrderLeaveTime&gt; orderLeaveTimeEncode = Encoders.bean(OrderLeaveTime.class); Dataset&lt;OrderLeaveTime&gt; leaveTimeDataset = spark.createDataset(leaveTimeJavaRDD.rdd(),orderLeaveTimeEncode); 二、spark dataset 分组计算（使用groupbyKey）1234567891011121314151617181920212223242526272829303132333435363738Dataset&lt;DeviceLocation&gt; deviceLocationDataset = devicePosition.groupByKey(new MapFunction&lt;Row, String&gt;() { @Override public String call(Row value) throws Exception { return value.getString(2); } }, Encoders.STRING()).flatMapGroups(new FlatMapGroupsFunction&lt;String, Row, DeviceLocation&gt;() { @Override public Iterator&lt;DeviceLocation&gt; call(String key, Iterator&lt;Row&gt; values) throws Exception { List&lt;DeviceLocation.Location&gt; locations = new ArrayList&lt;&gt;(); while (values.hasNext()) { DeviceLocation.Location location = new DeviceLocation.Location(); Row next = values.next(); location.setId((int)next.getLong(0)); location.setLocation_code(next.getString(1)); location.setLongitude(next.getString(3)); location.setLongmark(next.getString(4)); location.setLatitude(next.getString(5)); location.setLati_mark(next.getString(6)); location.setPosition_time(next.getTimestamp(7)); location.setSignal_strength(next.getString(8)); location.setPower(next.getString(9)); location.setAlarm_status(next.getString(10)); location.setExtended_data(next.getString(11)); locations.add(location); } DeviceLocation deviceLocation = new DeviceLocation(); deviceLocation.setImei(key); deviceLocation.setDateTime(&quot;2020-04-05&quot;); deviceLocation.setLocation(JSON.toJSONString(locations)); ArrayList&lt;DeviceLocation&gt; deviceLocations = new ArrayList&lt;&gt;(); deviceLocations.add(deviceLocation); return deviceLocations.iterator(); } }, Encoders.bean(DeviceLocation.class)); deviceLocationDataset.select(col(&quot;imei&quot;),col(&quot;dateTime&quot;),col(&quot;location&quot;).cast(&quot;String&quot;)).write().mode(SaveMode.Append).jdbc(url,&quot;test&quot;,properties); } spark 分组计算123456789101112131415161718192021222324252627282930313233//Encoder&lt;OrderArriveTime&gt; orderArriveTimeEncoder = Encoders.bean(OrderArriveTime.class); //通过转成list来遍历得到数据// List&lt;OrderArriveTime&gt; list = new ArrayList&lt;&gt;();// //效率太低，考虑自定义函数// List&lt;OrderArriveTime&gt; orderArriveTimes = orderArriveTimeDataset.collectAsList();// orderArriveTimes.stream().collect(Collectors.groupingBy(OrderArriveTime::getCustomerAddress))// .forEach(new BiConsumer&lt;String, List&lt;OrderArriveTime&gt;&gt;() {// @Override// public void accept(String s, List&lt;OrderArriveTime&gt; orderArriveTimes) {//// String arriveTime = null;// String finishTime = null;//// for (OrderArriveTime next : orderArriveTimes) {// OrderArriveTime orderArriveTime = new OrderArriveTime();// if (arriveTime == null) {// arriveTime = next.getArriveTime();// } else {// arriveTime = finishTime;// }// finishTime = next.getFinishTime();// orderArriveTime.setOrderNumber(next.getOrderNumber());// orderArriveTime.setCustomerAddress(next.getCustomerAddress());// orderArriveTime.setArriveTime(arriveTime);// orderArriveTime.setFinishTime(finishTime);// list.add(orderArriveTime);// }//// }// });//// Dataset&lt;OrderArriveTime&gt; realArriveDataset = spark.createDataset(list, orderArriveTimeEncoder); spark 自定义函数123456789101112131415161718192021222324252627282930313233时间格式转换自定义函数// spark.udf().register(&quot;TimeFormat&quot;, new UDF1&lt;String, String&gt;() {// @Override// public String call(String oldDateStr) throws Exception {// if (oldDateStr != null){// DateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd&apos;T&apos;HH:mm:ss.SSSXXX&quot;);// Date date = df.parse(oldDateStr);// SimpleDateFormat df1 = new SimpleDateFormat (&quot;EEE MMM dd HH:mm:ss Z yyyy&quot;, Locale.UK);// Date date1 = df1.parse(date.toString());// DateFormat df2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);// return df2.format(date1);// }// return null;// }// }, DataTypes.StringType); //自定义函数适应于多列数据通过逻辑处理得到一列数据的情况 //定义自定义函数 spark.udf().register(&quot;orderDistanceCustomer&quot;, (UDF4&lt;Double, Double, Double, Double, Double&gt;) (lat1, lon1, lat2, lon2) -&gt; { if (lat1 != null &amp;&amp; lon1 != null &amp;&amp; lat2 != null &amp;&amp; lon2 != null){ return MapUtil.distanceCalculate(lat1, lon1, lat2, lon2); }else { return -1.0; } }, DataTypes.DoubleType); //使用自定义函数 Dataset&lt;Row&gt; orderDistanceCustomer = orderLocalInfo.withColumn(&quot;order_distance_customer&quot;, callUDF(&quot;orderDistanceCustomer&quot;, orderLocalInfo.col(&quot;customer_lat&quot;), orderLocalInfo.col(&quot;customer_long&quot;), orderLocalInfo.col(&quot;latitude&quot;), orderLocalInfo.col(&quot;longitude&quot;))); spark提交命令1nohup spark-submit --class com.gree.bdc.analysis.DispatchDetailsInfo --master local[2] --executor-memory 8G --total-executor-cores 4 /home/bdcopt/spark/order-details-handle-1.0-SNAPSHOT-jar-with-dependencies.jar &gt; /tmp/spark/order/order-datails.log &amp;","link":"/2020/04/07/spark-常用操作/"},{"title":"权限设计管理","text":"基于RBAC（Resource-Based Access Control）资源权限控制抽象为uri的访问控制（uri权限控制表）自增长iduri(根据业务需要控制颗粒度，例如细可以精确到一条数据的访问，或者，粗可以只是控制一个页面的访问)access_allow 用户角色（user role）自增长id用户角色描述 用户组（user group）自增长id用户组描述 用户（user）自增长id用户其他相关信息（用户名，password等等） 用户角色（user role）-uri访问权限集合（角色-uri权限关系表）用户角色iduri权限id 用户组（user group）-uri访问权限集合（用户组-uri权限关系表）用户组iduri权限id 用户（user ）-uri访问权限集合（用户-uri权限关系表）用户iduri权限id 用户组（user group）- 用户角色（user role）集合（用户组-用户角色关系表）用户角色id用户组id 用户（user）- 用户组（user group）集合（用户-用户组关系表）用户id用户组id 用户（user）- 用户角色（user role）集合（用户-用户角色表）用户角色id用户id 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136/* Navicat Premium Data Transfer Source Server : 本地 Source Server Type : MySQL Source Server Version : 80012 Source Host : localhost:3306 Source Schema : gps_test Target Server Type : MySQL Target Server Version : 80012 File Encoding : 65001 Date: 21/02/2020 15:14:54*/SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;-- ------------------------------ Table structure for permission-- ----------------------------DROP TABLE IF EXISTS `permission`;CREATE TABLE `permission` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;权限ID&apos;, `parent_id` bigint(20) DEFAULT NULL COMMENT &apos;所属父级权限ID&apos;, `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限唯一CODE代码&apos;, `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限名称&apos;, `dsca` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限描述&apos;, `category` tinyint(1) DEFAULT NULL COMMENT &apos;权限类别&apos;, `uri` bigint(20) DEFAULT NULL COMMENT &apos;URL规则&apos;, `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;, `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;, `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;, `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;, `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;, PRIMARY KEY (`id`) USING BTREE, INDEX `parent_id`(`parent_id`) USING BTREE COMMENT &apos;父级权限ID&apos;, INDEX `code`(`code`) USING BTREE COMMENT &apos;权限CODE代码&apos;) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;权限&apos; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role-- ----------------------------DROP TABLE IF EXISTS `role`;CREATE TABLE `role` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;角色ID&apos;, `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色唯一CODE代码&apos;, `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色名称&apos;, `desc` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色描述&apos;, `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;, `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;, `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;, `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更改者&apos;, `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;, `type` tinyint(1) NOT NULL DEFAULT 1 COMMENT &apos;0：超管、1：管理员、2：普通人员&apos;, PRIMARY KEY (`id`) USING BTREE, INDEX `code`(`code`) USING BTREE COMMENT &apos;权限CODE代码&apos;) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;角色&apos; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role_permission-- ----------------------------DROP TABLE IF EXISTS `role_permission`;CREATE TABLE `role_permission` ( `role_id` int(11) NOT NULL COMMENT &apos;角色id&apos;, `permission_id` int(11) DEFAULT NULL COMMENT &apos;权限id&apos;) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user-- ----------------------------DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;用户ID&apos;, `state` tinyint(1) DEFAULT NULL COMMENT &apos;用户状态:0=正常,1=禁用&apos;, `user_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;姓名&apos;, `user_tel_number` varchar(11) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;手机号码&apos;, `salt` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;密码加盐&apos;, `password` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;登录密码&apos;, `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;, `creator` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;, `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;, `regenerator` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;, `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;用户&apos; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_group-- ----------------------------DROP TABLE IF EXISTS `user_group`;CREATE TABLE `user_group` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;ID&apos;, `parent_id` bigint(20) DEFAULT NULL COMMENT &apos;所属父级用户组ID&apos;, `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组名称&apos;, `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组CODE唯一代码&apos;, `desc` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组描述&apos;, `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;, `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;, `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;, `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;, `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;, PRIMARY KEY (`id`) USING BTREE, INDEX `parent_id`(`parent_id`) USING BTREE COMMENT &apos;父级用户组ID&apos;, INDEX `code`(`code`) USING BTREE COMMENT &apos;用户组CODE代码&apos;) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;用户组&apos; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_group_role-- ----------------------------DROP TABLE IF EXISTS `user_group_role`;CREATE TABLE `user_group_role` ( `user_group_id` int(11) NOT NULL COMMENT &apos;用户组id&apos;, `role_id` int(11) DEFAULT NULL COMMENT &apos;角色id&apos;) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_role-- ----------------------------DROP TABLE IF EXISTS `user_role`;CREATE TABLE `user_role` ( `user_id` int(11) NOT NULL COMMENT &apos;用户id&apos;, `role_id` int(11) DEFAULT NULL COMMENT &apos;角色id&apos;) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_user_group-- ----------------------------DROP TABLE IF EXISTS `user_user_group`;CREATE TABLE `user_user_group` ( `user_id` int(11) NOT NULL COMMENT &apos;用户id&apos;, `user_group_id` int(11) DEFAULT NULL COMMENT &apos;用户组id&apos;) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;SET FOREIGN_KEY_CHECKS = 1; 第二版：权限设计表结构（主要新增了user_permission表 和 user_group_permission表 用于灵活的控制权限修改和授权操作，user_group 用户组表 parent_id属性的删除，主要区别于组织架构的系统，能够更加通用化设计） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152/* Navicat Premium Data Transfer Source Server : 本地 Source Server Type : MySQL Source Server Version : 80012 Source Host : localhost:3306 Source Schema : gps_test Target Server Type : MySQL Target Server Version : 80012 File Encoding : 65001 Date: 24/02/2020 08:51:04*/SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;-- ------------------------------ Table structure for permission-- ----------------------------DROP TABLE IF EXISTS `permission`;CREATE TABLE `permission` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;权限ID&apos;, `parent_id` bigint(20) DEFAULT NULL COMMENT &apos;所属父级权限ID&apos;, `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限唯一CODE代码&apos;, `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限名称&apos;, `dsca` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;权限描述&apos;, `category` tinyint(1) DEFAULT NULL COMMENT &apos;权限类别&apos;, `uri` bigint(20) DEFAULT NULL COMMENT &apos;URL规则&apos;, `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;, `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;, `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;, `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;, `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;, PRIMARY KEY (`id`) USING BTREE, INDEX `parent_id`(`parent_id`) USING BTREE COMMENT &apos;父级权限ID&apos;, INDEX `code`(`code`) USING BTREE COMMENT &apos;权限CODE代码&apos;) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;权限&apos; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role-- ----------------------------DROP TABLE IF EXISTS `role`;CREATE TABLE `role` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;角色ID&apos;, `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色唯一CODE代码&apos;, `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色名称&apos;, `desc` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;角色描述&apos;, `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;, `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;, `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;, `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更改者&apos;, `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;, `type` tinyint(1) NOT NULL DEFAULT 1 COMMENT &apos;0：超管、1：管理员、2：普通人员&apos;, PRIMARY KEY (`id`) USING BTREE, INDEX `code`(`code`) USING BTREE COMMENT &apos;权限CODE代码&apos;) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;角色&apos; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for role_permission-- ----------------------------DROP TABLE IF EXISTS `role_permission`;CREATE TABLE `role_permission` ( `role_id` int(11) NOT NULL COMMENT &apos;角色id&apos;, `permission_id` int(11) DEFAULT NULL COMMENT &apos;权限id&apos;) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user-- ----------------------------DROP TABLE IF EXISTS `user`;CREATE TABLE `user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;用户ID&apos;, `state` tinyint(1) DEFAULT NULL COMMENT &apos;用户状态:0=正常,1=禁用&apos;, `user_name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;姓名&apos;, `user_tel_number` varchar(11) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;手机号码&apos;, `salt` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;密码加盐&apos;, `password` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;登录密码&apos;, `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;, `creator` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;, `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;, `regenerator` varchar(64) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;, `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;用户&apos; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_group-- ----------------------------DROP TABLE IF EXISTS `user_group`;CREATE TABLE `user_group` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &apos;ID&apos;, `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组名称&apos;, `code` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组CODE唯一代码&apos;, `desc` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;用户组描述&apos;, `create_time` datetime(0) DEFAULT NULL COMMENT &apos;创建时间&apos;, `creator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;创建人&apos;, `update_time` datetime(0) DEFAULT NULL COMMENT &apos;更新时间&apos;, `regenerator` varchar(36) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT &apos;更新者&apos;, `del_flag` tinyint(1) UNSIGNED ZEROFILL DEFAULT 0 COMMENT &apos;逻辑删除:0=未删除,1=已删除&apos;, PRIMARY KEY (`id`) USING BTREE, INDEX `code`(`code`) USING BTREE COMMENT &apos;用户组CODE代码&apos;) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = &apos;用户组&apos; ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_group_permission-- ----------------------------DROP TABLE IF EXISTS `user_group_permission`;CREATE TABLE `user_group_permission` ( `user_group_id` int(11) NOT NULL COMMENT &apos;用户组id&apos;, `permission_id` int(11) DEFAULT NULL COMMENT &apos;权限id&apos;) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_group_role-- ----------------------------DROP TABLE IF EXISTS `user_group_role`;CREATE TABLE `user_group_role` ( `user_group_id` int(11) NOT NULL COMMENT &apos;用户组id&apos;, `role_id` int(11) DEFAULT NULL COMMENT &apos;角色id&apos;) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_permission-- ----------------------------DROP TABLE IF EXISTS `user_permission`;CREATE TABLE `user_permission` ( `user_id` int(11) NOT NULL COMMENT &apos;用户id&apos;, `permission_id` int(11) DEFAULT NULL COMMENT &apos;权限id&apos;) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_role-- ----------------------------DROP TABLE IF EXISTS `user_role`;CREATE TABLE `user_role` ( `user_id` int(11) NOT NULL COMMENT &apos;用户id&apos;, `role_id` int(11) DEFAULT NULL COMMENT &apos;角色id&apos;) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Table structure for user_user_group-- ----------------------------DROP TABLE IF EXISTS `user_user_group`;CREATE TABLE `user_user_group` ( `user_id` int(11) NOT NULL COMMENT &apos;用户id&apos;, `user_group_id` int(11) DEFAULT NULL COMMENT &apos;用户组id&apos;) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;SET FOREIGN_KEY_CHECKS = 1; 设计表逻辑图","link":"/2020/02/24/权限设计管理/"},{"title":"flink数据处理","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package com.gree.bdc.device;import com.google.gson.Gson;import com.gree.bdc.entity.DevicePosition;import com.gree.bdc.position.LongSpot;import com.gree.bdc.util.InitFlinkUtils;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.api.common.typeinfo.TypeHint;import org.apache.flink.api.common.typeinfo.TypeInformation;import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.api.java.tuple.Tuple4;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;import java.sql.Timestamp;import java.util.Properties;/** * 设备久置点计算 * @author hadoop * */public class DeviceLongSpotInfo { public static void main(String[] args) { System.setProperty(&quot;java.security.auth.login.config&quot;,&quot;/home/bdcopt/spark/jaas.conf&quot;); System.setProperty(&quot;java.security.krb5.conf&quot;,&quot;/home/bdcopt/spark/krb5.conf&quot;); StreamExecutionEnvironment env = InitFlinkUtils.getEnv(); env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); //读取kafka数据源 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;,&quot;kafka01:30001,kafka02:30002,kafka03:30003,kafka04:30004,kafka05:30005&quot;); properties.setProperty(&quot;group.id&quot;,&quot;test&quot;); properties.setProperty(&quot;auto.offset.reset&quot;,&quot;latest&quot;); properties.setProperty(&quot;security.protocol&quot;,&quot;SASL_PLAINTEXT&quot;); properties.setProperty(&quot;sasl.kerberos.service.name&quot;,&quot;kafka&quot;); FlinkKafkaConsumer&lt;String&gt; kafkatopic0619 = new FlinkKafkaConsumer&lt;String&gt;(&quot;test&quot;, new SimpleStringSchema(), properties); kafkatopic0619.setStartFromLatest(); env.setParallelism(4); DataStreamSource&lt;String&gt; sourceStream = env.addSource(kafkatopic0619); //数据处理： //1.按照设备id进行分组计算，按照每个久置点周期为一个窗口大小 // 2.移动距离为每个上报频率为滑动窗口的距离，计算在这个窗口的上报的设备点的距离与第一个设备的点的距离在 //可以接受的范围，那么这个点就是一个久置点。 sourceStream.map(value -&gt; { Gson gson = new Gson(); DevicePosition devicePosition = gson.fromJson(value, DevicePosition.class); String imei = devicePosition.getImei(); Timestamp positionTime = devicePosition.getPosition_time(); String longitude = devicePosition.getLongitude(); String latitude = devicePosition.getLatitude(); return new Tuple4&lt;&gt;(positionTime,imei,latitude,longitude); }).returns(TypeInformation.of(new TypeHint&lt;Tuple4&lt;Timestamp, String, String, String&gt;&gt;() { })).assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;Tuple4&lt;Timestamp, String, String, String&gt;&gt;(Time.minutes(1)) { @Override public long extractTimestamp(Tuple4&lt;Timestamp, String, String, String&gt; element) { return element.f0.getTime(); } }).keyBy((KeySelector&lt;Tuple4&lt;Timestamp,String,String,String&gt;,String&gt;) map -&gt; map.f1) .window(SlidingEventTimeWindows.of(Time.minutes(30L),Time.minutes(3L))) .process(new LongSpot()); }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143package com.gree.bdc;import com.google.gson.Gson;import com.gree.bdc.entity.DeviceLongSpot;import com.gree.bdc.entity.DevicePosition;import com.gree.bdc.entity.DeviceResidenceTime;import com.gree.bdc.position.LongSpotNew;import com.gree.bdc.sink.MysqlSink;import com.gree.bdc.util.DateUtils;import com.gree.bdc.util.InitFlinkUtils;import com.gree.bdc.util.MapUtils;import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.functions.RichFlatMapFunction;import org.apache.flink.api.common.functions.RichMapFunction;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.api.common.state.StateTtlConfig;import org.apache.flink.api.common.state.ValueState;import org.apache.flink.api.common.state.ValueStateDescriptor;import org.apache.flink.api.common.time.Time;import org.apache.flink.api.common.typeinfo.TypeHint;import org.apache.flink.api.common.typeinfo.TypeInformation;import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.api.java.tuple.Tuple4;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.api.java.tuple.Tuple6;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.TimeCharacteristic;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;import org.apache.flink.util.Collector;import java.sql.Timestamp;import java.util.Properties;public class ReadKafkaTest { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = InitFlinkUtils.getEnv(); env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); Properties properties = new Properties(); //properties.setProperty(&quot;bootstrap.servers&quot;,&quot;kafka01:30001,kafka02:30002,kafka03:30003,kafka04:30004,kafka05:30005&quot;); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); properties.setProperty(&quot;group.id&quot;,&quot;test&quot;); FlinkKafkaConsumer&lt;String&gt; kafkatopic0619 = new FlinkKafkaConsumer&lt;String&gt;(&quot;test08&quot;, new SimpleStringSchema(), properties); kafkatopic0619.setStartFromLatest(); DataStreamSource&lt;String&gt; sourceStream = env.addSource(kafkatopic0619); SingleOutputStreamOperator&lt;Tuple6&lt;String,Timestamp,String,Double,Double,Long&gt;&gt; map = sourceStream.map(value -&gt; { Gson gson = new Gson(); DevicePosition devicePosition = gson.fromJson(value, DevicePosition.class); String imei = devicePosition.getImei(); Timestamp positionTime = devicePosition.getPosition_time(); String longitude = devicePosition.getLongitude(); String latitude = devicePosition.getLatitude(); return new Tuple4&lt;&gt;(positionTime, imei, latitude, longitude); }).returns(TypeInformation.of(new TypeHint&lt;Tuple4&lt;Timestamp, String, String, String&gt;&gt;() { })).keyBy(1) .map(new RichMapFunction&lt;Tuple4&lt;Timestamp, String, String, String&gt;, Tuple6&lt;String,Timestamp,String,Double,Double,Long&gt;&gt;() { private ValueState&lt;Tuple4&lt;Timestamp, String, String, String&gt;&gt; valueState; @Override public Tuple6&lt;String,Timestamp,String,Double,Double,Long&gt; map(Tuple4&lt;Timestamp, String, String, String&gt; value) throws Exception { Tuple4&lt;Timestamp, String, String, String&gt; currentValue = valueState.value(); System.out.println(&quot;刚开始的状态 ： &quot; + currentValue); if (currentValue == null) { currentValue = Tuple4.of(value.f0, value.f1, value.f2, value.f3); valueState.update(currentValue); } double currentLat = Double.parseDouble(currentValue.f2); double currentLong = Double.parseDouble(currentValue.f3); Timestamp currentTime = currentValue.f0; double valueLat = Double.parseDouble(value.f2); double valueLong = Double.parseDouble(value.f3); Timestamp valueTime = value.f0; Double distanceCalculate = MapUtils.distanceCalculate(currentLat, currentLong, valueLat, valueLong); Long min = DateUtils.dateSubtraction(currentTime.toString(), valueTime.toString()); System.out.println(distanceCalculate); System.out.println(&quot;状态点： &quot; + currentValue); System.out.println(&quot;当前数据： &quot; + value); if (distanceCalculate &gt; 500) { currentValue.f0 = value.f0; currentValue.f1 = value.f1; currentValue.f2 = value.f2; currentValue.f3 = value.f3; valueState.update(currentValue); if (min &gt; 30) { return Tuple6.of(&quot;久置点&quot;, currentTime, value.f1, currentLat, currentLong, min); } else { return Tuple6.of(&quot;非久置点&quot;, currentTime, value.f1, currentLat, currentLong, min); } } return Tuple6.of(&quot;邻域点&quot;, currentTime, value.f1, currentLat, currentLong, min); } @Override public void open(Configuration parameters) throws Exception { super.open(parameters); // keyedState可以设置TTL过期时间// StateTtlConfig config= StateTtlConfig// .newBuilder(Time.seconds(30))// .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)// .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)// .build(); ValueStateDescriptor valueStateDescriptor = new ValueStateDescriptor(&quot;agvKeyedState&quot;, TypeInformation.of(new TypeHint&lt;Tuple4&lt;Timestamp, String, String, String&gt;&gt;() { })); //设置支持TTL配置// valueStateDescriptor.enableTimeToLive(config); valueState = getRuntimeContext().getState(valueStateDescriptor); } }); SingleOutputStreamOperator&lt;Tuple6&lt;String, Timestamp, String, Double, Double, Long&gt;&gt; filter = map.filter(value -&gt; { return value.f0.equals(&quot;久置点&quot;); }); filter.addSink(new MysqlSink()); env.execute(&quot;read-kafka-test&quot;); }}","link":"/2020/04/09/flink数据处理/"},{"title":"mysql常见异常处理","text":"mysql异常处理123解决java.sql.SQLException: Value &apos;0000-00-00&apos; can not be represented as java.sql.Datejdbc:mysql://yourserver:3306/yourdatabase?zeroDateTimeBehavior=convertToNull #mysql文本类型数据存储12345mysql中text 最大长来度为65,535(2的16次方–1)字符的TEXT列。如果你觉得text长度自不够百，可以选择MEDIUMTEXT最大长度为16,777,215。LONGTEXT最大长度为4,294,967,295Text主要度是用来存放非二进制的文本","link":"/2020/04/14/mysql常见异常处理/"},{"title":"spark 读取 kafka 并写入mysql","text":"spark maven 依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.gree.bdc&lt;/groupId&gt; &lt;artifactId&gt;dispatch-kafka-installer&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;spark.version&gt;2.4.0&lt;/spark.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- spark start--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spark end --&gt; &lt;!-- structStreaming kafka --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- sparkStreaming kafka --&gt; &lt;!-- 建议新集群使用sparkStreaming处理kafka数据--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 数据库连接--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 数据库连接 end --&gt; &lt;!-- 常用工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.2&lt;/version&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.58&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 常用工具 end --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 读取kafka ,写入mysql123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113package com.gree.bdc.installer;import com.alibaba.fastjson.JSON;import com.gree.bdc.entity.InstallerInfo;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.common.serialization.StringDeserializer;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.sql.*;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.apache.spark.streaming.kafka010.ConsumerStrategies;import org.apache.spark.streaming.kafka010.KafkaUtils;import org.apache.spark.streaming.kafka010.LocationStrategies;import java.util.*;/** * 读取kafka * @author hadoop * @date 2020-03-26 */public class ReadKafkaInstaller { public static void main(String[] args) { ///本地测试 //System.setProperty(&quot;java.security.auth.login.config&quot;, &quot;E:\\\\workspace\\\\dispatch-kafka-data\\\\src\\\\main\\\\resources\\\\jaas.conf&quot;); //System.setProperty(&quot;java.security.krb5.conf&quot;, &quot;E:\\\\workspace\\\\dispatch-kafka-data\\\\src\\\\main\\\\resources\\\\krb5.conf&quot;); //服务器上跑 System.setProperty(&quot;java.security.auth.login.config&quot;,&quot;/home/bdcopt/spark/jaas.conf&quot;); System.setProperty(&quot;java.security.krb5.conf&quot;,&quot;/home/bdcopt/spark/krb5.conf&quot;); SparkSession spark = SparkSession.builder() .appName(&quot;dispatch-installer&quot;) .master(&quot;local[2]&quot;) .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;) .config( &quot;spark.driver.allowMultipleContexts&quot;,true ) .getOrCreate(); // 读取kafka配置 Map&lt;String,Object&gt; kafkaParams = new HashMap&lt;&gt;( 16); kafkaParams.put( &quot;bootstrap.servers&quot;,&quot;kafka01:30001,kafka02:30002,kafka03:30003,kafka04:30004,kafka05:30005&quot; ); kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class); kafkaParams.put(&quot;value.deserializer&quot;, StringDeserializer.class); kafkaParams.put(&quot;group.id&quot;, &quot;dispatchInstaller297&quot;); kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;latest&quot;); kafkaParams.put(&quot;security.protocol&quot;,&quot;SASL_PLAINTEXT&quot; ); kafkaParams.put(&quot;sasl.kerberos.service.name&quot;,&quot;kafka&quot; ); kafkaParams.put(&quot;enable.auto.commit&quot;, true); Collection&lt;String&gt; topics = Collections.singletonList(&quot;dispatchInstaller&quot;); JavaStreamingContext javaStreamingContext = new JavaStreamingContext( spark.sparkContext().conf(), Durations.seconds( 1 )); JavaInputDStream&lt;ConsumerRecord&lt;String, String&gt;&gt; stream = KafkaUtils.createDirectStream( javaStreamingContext, LocationStrategies.PreferConsistent(), ConsumerStrategies.Subscribe(topics, kafkaParams) ); //数据遍历写入mysql stream.foreachRDD((rdd,time) -&gt;{ JavaRDD&lt;InstallerInfo&gt; installerInfoJavaRdd = rdd.map(stringValue -&gt; JSON.parseObject(stringValue.value(), InstallerInfo.class)); Dataset&lt;Row&gt; installerInfoDataFrame = spark.createDataFrame(installerInfoJavaRdd, InstallerInfo.class);/// String url = &quot;jdbc:mysql://localhost:3306/gpsdev?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot;;// //rm-wz9k56kkaxj0e9mct33150.mysql.rds.aliyuncs.com// String tableName = &quot;assign_ls_lc&quot;;// Properties prop = new Properties();// prop.put(&quot;user&quot;,&quot;root&quot;);// prop.put(&quot;password&quot;,&quot;123456&quot;);// prop.put(&quot;driver&quot;,&quot;com.mysql.cj.jdbc.Driver&quot;); //新增自增id// StructType schema = installerInfoDataFrame.schema().add(DataTypes.createStructField(&quot;id&quot;, DataTypes.LongType, false));// JavaRDD&lt;Row&gt; installerInfoDataFrameRdd = installerInfoDataFrame// .javaRDD() // 转为JavaRDD// .zipWithIndex() // 添加索引，结果为JavaPairRDD&lt;Row, Long&gt;，即行数据和对应的索引// .map(new Function&lt;Tuple2&lt;Row, Long&gt;, Row&gt;() {// @Override// public Row call(Tuple2&lt;Row, Long&gt; v1) throws Exception {// Object[] objects = new Object[v1._1.size() + 1];// for (int i = 0; i &lt; v1._1.size(); i++) {// objects[i] = v1._1.get(i);// }// objects[objects.length - 1] = v1._2;// return RowFactory.create(objects);// }// }); // 把索引值作为ID字段值，构造新的行数据// installerInfoDataFrame = spark.createDataFrame(installerInfoDataFrameRdd, schema); String url = &quot;jdbc:mysql://localhost:3306/paigong_map?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&quot;; String tableName = &quot;t_installer_original_info&quot;; Properties prop = new Properties(); prop.put(&quot;user&quot;,&quot;paigong_map&quot;); prop.put(&quot;password&quot;,&quot;paigong_map@dept1&quot;); prop.put(&quot;driver&quot;,&quot;com.mysql.cj.jdbc.Driver&quot;); installerInfoDataFrame.write().mode( SaveMode.Append).jdbc(url,tableName,prop); }); //启动程序 javaStreamingContext.start(); try { javaStreamingContext.awaitTermination(); } catch (InterruptedException e) {// TODO Auto-generated catch block e.printStackTrace(); } }} 采用上述方式写入mysql 速度较快，但是也有局限性，它仅适用于新增的模式，大批量的导入，对于数据量较小，而且需要更灵活导入（如 更新操作）时建议采用原生的kafka消费者 kafka consumer1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.gree.bdc&lt;/groupId&gt; &lt;artifactId&gt;dispatch-kafka-order-consumer&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!--引入kafka-client依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.62&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 数据库连接--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.44&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- 数据库连接 end --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; consumer.properties123456789101112131415#配置消费者集群bootstrap.servers=kafka01:30001#消费者的组ID，group.id=enable.auto.commit=truesession.timeout.ms=300000#消费策略auto.offset.reset=earliest#实现了Serializer接口的序列化类。用于告诉kafka如何序列化keykey.deserializer=org.apache.kafka.common.serialization.StringDeserializer#实现了Serializer接口的序列化类。用于告诉kafka如何序列化valuevalue.deserializer=org.apache.kafka.common.serialization.StringDeserializersecurity.protocol=SASL_PLAINTEXTsasl.kerberos.service.name=kafka druid.properties12345678driverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql://localhost:3306/test?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falseusername=rootpassword=123456initialSize=5maxActive=20maxWait=3000minIdle=3 jaas.conf123456789101112131415161718KafkaClient { com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true useKeyTab=true storeKey=true renewTicket=true keyTab=&quot;E:\\\\test.keytab&quot; principal=&quot;test&quot;;};Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true storeKey=true keyTab=&quot;test.keytab&quot; principal=&quot;test&quot;;}; krb5.conf等配置文件创建消费者1234567891011121314151617181920212223242526272829303132package com.gree.bdc.consumer;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.io.IOException;import java.util.Properties;/** * @author :yelinlin() * @date :2019/3/10 * @description :消费者 */public class Consumer { /** * 获得一个Kafka消费者 * @return kafkaConsumer */ public KafkaConsumer&lt;String,String&gt; getConsumer (){ //配置读取配置文件 System.setProperty(&quot;java.security.auth.login.config&quot;, &quot;/home/bdcopt/spark/jaas.conf&quot;); System.setProperty(&quot;java.security.krb5.conf&quot;, &quot;/home/bdcopt/spark/krb5.conf&quot;); Properties props = new Properties(); try { //获取kafka消费者配置信息 props.load(this.getClass().getResourceAsStream(&quot;/consumer.properties&quot;)); } catch (IOException e) { e.printStackTrace(); } return new KafkaConsumer&lt;String,String&gt;(props); }} 读取数据写入mysql 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.gree.bdc.order;import com.alibaba.fastjson.JSON;import com.gree.bdc.consumer.Consumer;import com.gree.bdc.entity.DispatchInfo;import com.gree.bdc.util.DateUtils;import com.gree.bdc.util.DruidPoolUtils;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.sql.Connection;import java.sql.PreparedStatement;import java.sql.SQLException;import java.text.ParseException;import java.util.Collections;/** * @author yelinlin (260269) * @createTime 2019/11/21 * @description kafka消费者启动类 */public class OrderConsumerStarter { private static final String TOPIC=&quot;test&quot;; private static final Logger logger= LoggerFactory.getLogger(OrderConsumerStarter.class); public static void main(String[] args) throws SQLException, ParseException { Connection connection; PreparedStatement statement; //获取一个消费者 KafkaConsumer&lt;String,String&gt; consumer = new Consumer().getConsumer(); consumer.subscribe(Collections.singletonList(TOPIC)); //消费并打印消费结果 while (true) { //进行消费 ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String,String&gt; record: records) { connection = DruidPoolUtils.getConnection(); DispatchInfo dispatchInfo = JSON.parseObject(record.value(), DispatchInfo.class); //写入mysql statement = connection.prepareStatement(&quot;&quot; + &quot;insert into paigong_map.t_dispatch_info (customer_address,customer_name,&quot; + &quot; customer_tel,dispatching_time,finish_time,installation_number,order_number,&quot; + &quot; unfinish_number,work_number) values(?,?,?,?,?,?,?,?,?)&quot;); statement.setString(1,dispatchInfo.getCustomer_address()); statement.setString(2,dispatchInfo.getCustomer_name()); statement.setString(3,dispatchInfo.getCustomer_tel()); statement.setString(4,DateUtils.dealDateFormat(dispatchInfo.getDispatching_time())); statement.setString(5,DateUtils.dealDateFormat(dispatchInfo.getFinish_time())); statement.setDouble(6,dispatchInfo.getInstallation_number()); statement.setString(7,dispatchInfo.getOrder_number()); statement.setDouble(8,dispatchInfo.getUnfinish_number()); statement.setString(9,dispatchInfo.getWork_number().split(&quot;\\\\.&quot;)[0]); statement.execute(); DruidPoolUtils.close(connection,statement); } } }} 补充下数据库连接池工具类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package com.gree.bdc.util;import com.alibaba.druid.pool.DruidDataSourceFactory;import javax.sql.DataSource;import java.sql.Connection;import java.sql.ResultSet;import java.sql.SQLException;import java.sql.Statement;import java.util.Properties;/** * Druid 数据库连接池 * @author hadoop * @date 2020-04-06 */public class DruidPoolUtils { /** * 创建成员变量,获取数据源 */ private static DataSource dataSource; //加载配置文件 static { try { Properties properties = new Properties();//加载类路径 properties.load(DruidPoolUtils.class.getResourceAsStream(&quot;/druid.properties&quot;));//读取属性文件，创建连接池 dataSource = DruidDataSourceFactory.createDataSource(properties); }catch (Exception e){ e.printStackTrace(); } } /** * 获取数据源 * @return 数据源 */ public static DataSource getDataSource(){ return dataSource; } /** * 获取连接对象 * @return 连接对象 */ public static Connection getConnection(){ try { return dataSource.getConnection(); }catch (SQLException e){ throw new RuntimeException(e); } } //释放资源 private static void close(Connection connection, Statement statement, ResultSet resultSet){ if (resultSet != null){ try { resultSet.close(); } catch (SQLException e) { e.printStackTrace(); } } if (statement != null){ try { statement.close(); }catch (SQLException e){ e.printStackTrace(); } } if (connection != null){ try { connection.close(); }catch (SQLException e){ e.printStackTrace(); } } } public static void close(Connection connection,Statement statement){ close(connection,statement,null); }} spark自定义批量写入mysql12345678910111213141516171819202122232425262728293031323334353637383940 //自定义批量写入mysql dispatchChange.foreachPartition(new ForeachPartitionFunction&lt;Row&gt;() { @Override public void call(Iterator&lt;Row&gt; t) throws Exception {// while(t.hasNext()){// System.out.println(t.next());// } PreparedStatement statement; Connection connection = DruidPoolUtils.getConnection(); statement = connection.prepareStatement(&quot;replace into test&quot; + &quot; (dispatching_time,order_number,imei,work_number,driver_name,license_plate,&quot; + &quot; installation_number,unfinish_number,customer_address,arrive_time,finish_time,&quot; + &quot; leave_time,total_time,average_time,order_status,affiliation,prod_id) values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)&quot;); while(t.hasNext()){ Row next = t.next(); statement.setTimestamp(1,next.getTimestamp(0)); statement.setString(2,next.getString(1)); statement.setString(3,next.getString(2)); statement.setString(4,next.getString(3)); statement.setString(5,next.getString(4)); statement.setString(6,next.getString(5)); statement.setDouble(7,next.getDouble(6)); statement.setDouble(8,next.getDouble(7)); statement.setString(9,next.getString(8)); statement.setTimestamp(10,next.getTimestamp(9)); statement.setTimestamp(11,next.getTimestamp(10)); statement.setTimestamp(12,next.getTimestamp(11)); statement.setDouble(13,next.getDouble(12)); statement.setDouble(14,next.getDouble(13)); statement.setInt(15,next.getInt(14)); statement.setString(16,next.getString(15)); statement.setString(17,next.getString(16)); statement.addBatch(); } statement.executeBatch(); DruidPoolUtils.close(connection,statement); } });","link":"/2020/04/08/spark-读取-kafka-并写入mysql/"},{"title":"接口调试问题","text":"一、postman接口测试工具如何接收java.util.date参数 时间格式采用：2020/04/11 14:12:14 二、java 参数类型与mysql类型比较12345678910111213141516171819202122232425262728293031323334353637383940414243===========java注入数据库==========java类型 mysql类型 成功与否date date yesdate time nodate timestamp nodate datetime notime date notime time yestime timestamp notime datetime notimestamp date yestimestamp time yestimestamp timestamp yestimestamp datetime yes==========end java注入数据库========总规律，如果A完全包含B，则A可以向B注入数据，否则报错 ==========从数据库提取到java ==========mysql类型 java类型 成与否date date yesdate time yes --------------缺少的部分使用历元date timestamp yes --------------缺少的部分使用历元 time date yes --------------缺少的部分使用历元time time yestime timestamp yes --------------缺少的部分使用历元timestamp date yestimestamp time yestimestamp timestamp yesdatetime date yesdatetime time yesdatetime timestamp yes==========end 从数据库提取到java=======不会出错，缺少的部分使用历元，而不是当前日期时间 123456789实战经验：在java中处理时间数据，建议采用java8中的时间工具类localtimelocaldatelocaldatetime在spark中处理时间的字段类型采用timestamp来计算，不要使用date类型计算mysql数据库中的日期类型字段采用date类型时间字段采用datetime 或者timestamp都可以，避免使用字符串，看起来更加正规。","link":"/2020/04/11/接口调试问题/"},{"title":"大数据处理地理数据","text":"1.调用http请求工具类1234567&lt;!-- http start --&gt; &lt;dependency&gt; &lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt; &lt;artifactId&gt;okhttp&lt;/artifactId&gt; &lt;version&gt;4.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- http end --&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142import okhttp3.*;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;public class OkHttpUtil { private static final Logger log = LoggerFactory.getLogger(OkHttpUtil.class); public final static int CONNECT_TIMEOUT = 10; public final static int READ_TIMEOUT = 30; public final static int WRITE_TIMEOUT = 60; private OkHttpClient mOkHttpClient; private OkHttpUtil() { OkHttpClient.Builder clientBuilder = new OkHttpClient.Builder(); clientBuilder.readTimeout(READ_TIMEOUT, TimeUnit.SECONDS);//读取超时 clientBuilder.connectTimeout(CONNECT_TIMEOUT, TimeUnit.SECONDS);//连接超时 clientBuilder.writeTimeout(WRITE_TIMEOUT, TimeUnit.SECONDS);//写入超时 mOkHttpClient = clientBuilder.build(); } public static OkHttpUtil getInstance() { return Holder.OK_HTTP_UTIL; } public String get(String url) throws Exception { return get(url, null); } public String get(String url, HashMap&lt;String, String&gt; headers, HashMap&lt;String, String&gt; params) { String result = &quot;&quot;; url = handleRequestParams(url, params); Request request = new Request.Builder() .url(url) .headers(Headers.of(headers)) .get() .build(); Call call = mOkHttpClient.newCall(request); try { Response execute = call.execute(); result = execute.body().string(); } catch (IOException e) { log.error(&quot;http 访问错误：&quot; + e); } return result; } public String get(String url, HashMap&lt;String, String&gt; params) throws Exception { String result = &quot;&quot;; url = handleRequestParams(url, params); Request request = new Request.Builder() .url(url) .get() .build(); Call call = mOkHttpClient.newCall(request); try { Response execute = call.execute(); int code = execute.code(); result = execute.body().string(); } catch (IOException e) { log.error(&quot;http 访问错误：&quot; + e.getMessage()); throw new IOException(e.getMessage()); } return result; } private String handleRequestParams(String url, HashMap&lt;String, String&gt; params) { if (params != null &amp;&amp; params.values().size() &gt; 0) { StringBuilder stringBuilder = new StringBuilder(); stringBuilder.append(&quot;?&quot;); for (Map.Entry&lt;String, String&gt; entry : params.entrySet()) { stringBuilder.append(entry.getKey() + &quot;=&quot; + entry.getValue() + &quot;&amp;&quot;); } String substring = stringBuilder.substring(0, stringBuilder.length() - 1); url += substring; } return url; } public String post(String url, HashMap&lt;String, String&gt; headerParams, String body) { String result = &quot;&quot;; Request request = new Request.Builder() .url(url) .headers(Headers.of(headerParams)) .post(RequestBody.create(MediaType.parse(&quot;application/json&quot;), body)) .build(); Call call = mOkHttpClient.newCall(request); try { Response execute = call.execute(); int code = execute.code(); } catch (Exception e) { log.info(&quot;post请求异常&quot; + e.getMessage()); } return result; } public String post(String url, String body) { String result = &quot;&quot;; Request request = new Request.Builder() .url(url) .post(RequestBody.create(MediaType.parse(&quot;application/json&quot;), body)) .build(); Call call = mOkHttpClient.newCall(request); try { Response execute = call.execute(); result = execute.body().string(); } catch (IOException e) { e.printStackTrace(); } return result; } public String xmlPost(String url, String body) { String result = &quot;&quot;; Request request = new Request.Builder() .url(url) .post(RequestBody.create(MediaType.parse(&quot;application/xml&quot;), body)) .build(); Call call = mOkHttpClient.newCall(request); try { Response execute = call.execute(); result = execute.body().string(); } catch (IOException e) { e.printStackTrace(); } return result; } private static class Holder { private static final OkHttpUtil OK_HTTP_UTIL = new OkHttpUtil(); }} 2.腾讯地图调用工具类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package com.gree.bdc.util;import com.alibaba.fastjson.JSON;import com.gree.bdc.entity.AddressToCoordinateResult;import com.gree.bdc.entity.CoordinateToAddressResult;import com.gree.bdc.entity.Point;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.HashMap;/** * @author hadoop */public class MapUtil { private static final Logger log = LoggerFactory.getLogger(MapUtil.class); //key需要自己申请 public static final String KEY = &quot;xxxx&quot;; /** * 通过地址转坐标 * @param address 地址 * @throws Exception exception */ public static Point addressToCoordinate(String address) throws Exception { String url = &quot;https://apis.map.qq.com/ws/geocoder/v1/&quot;; HashMap&lt;String, String&gt; params = new HashMap&lt;&gt;(8); params.put(&quot;address&quot;,address); params.put(&quot;key&quot;,KEY); String result = OkHttpUtil.getInstance().get(url, params); AddressToCoordinateResult addressToCoordinateResult = JSON.parseObject(result, AddressToCoordinateResult.class); Integer status = addressToCoordinateResult.getStatus(); if (status == 0 ){ AddressToCoordinateResult.Result.Location location = addressToCoordinateResult.getResult().getLocation(); Double lng = location.getLng(); Double lat = location.getLat(); Point point = new Point(); point.setLat(lat); point.setLng(lng); return point; } return null; }/***两个坐标的距离***/ public static Double distanceCalculate(double lat1, double lon1, double lat2, double lon2){ Double latitude1, longitude1, latitude2, longitude2; Double dlat, dlon; latitude1 = lat1; longitude1 = lon1; latitude2 = lat2; longitude2 = lon2; //computing procedure Double a, c, distance; dlon = Math.abs((longitude2 - longitude1)) * Math.PI / 180; dlat = Math.abs((latitude2 - latitude1)) * Math.PI / 180; a = (Math.sin(dlat / 2) * Math.sin(dlat / 2)) + Math.cos(latitude1 * Math.PI / 180) * Math.cos(latitude2 * Math.PI / 180) * (Math.sin(dlon / 2) * Math.sin(dlon / 2)); if (a == 1.0){ c = Math.PI; } else{ c = 2 * Math.atan(Math.sqrt(a) / Math.sqrt(1 - a)); } distance = 6378137.0 * c; return distance; } /** * 通过地址转坐标 * @param location 地址 * @throws Exception exception */ public static String coordinateToAddress(String location) throws Exception { String url = &quot;https://apis.map.qq.com/ws/geocoder/v1/&quot;; HashMap&lt;String, String&gt; params = new HashMap&lt;&gt;(8); params.put(&quot;location&quot;,location); params.put(&quot;key&quot;,KEY); String result = OkHttpUtil.getInstance().get(url, params); CoordinateToAddressResult coordinateToAddressResult = JSON.parseObject(result, CoordinateToAddressResult.class); ///System.out.println(coordinateToAddressResult); Integer status = coordinateToAddressResult.getStatus(); if (status == 0 ){ String address = coordinateToAddressResult.getResult().getAddress(); return address; } return null; }} 一般而言，腾讯地图的接口调用都有限制，对于普通的开发者上线为10000，并发5次/s，不建议正规项目使用，一旦并发过高会导致很多的地址解析失败，单个解析没问题，很难发现这个问题，故最好还是申请企业认证，申请企业认证每天可以300万的次数和1000的并发。腾讯地图位置服务网站","link":"/2020/05/07/大数据处理地理数据/"},{"title":"常用工具类","text":"时间工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114import java.sql.Timestamp;import java.text.DateFormat;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Calendar;import java.util.Date;import java.util.Locale;/** * 时间工具类 * @author hadoop * @date 2020-03-24 */public class DateUtils { private static SimpleDateFormat standardTimeFormat = new SimpleDateFormat(&quot;yyyy-MM-dd hh:mm:ss&quot;); /** * 比较两个时间大小 格式yyyy-MM-dd hh:mm:ss * @return 布尔值 */ public static Boolean compare2Time(Timestamp startTime, Timestamp endTime) throws ParseException { if (startTime == null || endTime == null){ return false; }else { return startTime.before(endTime); } } /** * 日期格式转换 * @param oldDateStr 原先时间格式 * @return 新时间格式 * @throws ParseException 转换 */ public static String dealDateFormat(String oldDateStr) throws ParseException{ //此格式只有 jdk 1.7才支持 yyyy-MM-dd‘T‘HH:mm:ss.SSSXXX DateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd&apos;T&apos;HH:mm:ss.SSSXXX&quot;); if (oldDateStr != null){ Date date = df.parse(oldDateStr); SimpleDateFormat df1 = new SimpleDateFormat (&quot;EEE MMM dd HH:mm:ss Z yyyy&quot;, Locale.UK); Date date1 = df1.parse(date.toString()); DateFormat df2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); return df2.format(date1); }else { return null; } } /** * 得到两个时间差 格式yyyy-MM-dd HH:mm:ss * @param start 2019-06-27 14:12:40 * @param end 2019-08-27 14:12:40 * @return 5270400000 */ public static Long dateSubtraction(String start, String end) { SimpleDateFormat df = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); try { Date date1 = df.parse(start); Date date2 = df.parse(end); Long diff = date2.getTime() - date1.getTime(); Long days = diff / (1000 * 60 * 60 * 24); Long hours = (diff-days*(1000 * 60 * 60 * 24))/(1000* 60 * 60); Long minutes = (diff-days*(1000 * 60 * 60 * 24)-hours*(1000* 60 * 60))/(1000* 60); Long totalMin = days * 12 + hours*60 + minutes; return totalMin; } catch (ParseException e) { e.printStackTrace(); return 0L; } } /** * 获取当前日期前一天 * @return 2019-08-26 */ public static String getBeforeDay() { SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); Date date = new Date(); Calendar calendar = Calendar.getInstance(); calendar.setTime(date); calendar.add(Calendar.DAY_OF_MONTH, -1); date = calendar.getTime(); return sdf.format(date); } /** * 获取当前日期前一天新格式（yyyyMMdd） * @return 2019-08-26 */ public static String getBeforeDayNewFormat() { SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyyMMdd&quot;); Date date = new Date(); Calendar calendar = Calendar.getInstance(); calendar.setTime(date); calendar.add(Calendar.DAY_OF_MONTH, -1); date = calendar.getTime(); return sdf.format(date); } /** * 添加几分钟时间 * @param timestamp 时间戳 * @param time 毫秒数 * @return 时间戳 */ public static Timestamp addTime(Timestamp timestamp ,Long time){ return new Timestamp(timestamp.getTime() + time); }}","link":"/2020/05/11/常用工具类/"},{"title":"flink 常见问题","text":"1.Exception in thread “main” org.apache.flink.api.common.functions.InvalidTypesException: The generic type parameters of ‘Collector’ are missing. In many cases lambda methods don’t provide enough information for automatic type extraction when Java generics are involved. An easy workaround is to use an (anonymous) class instead that implements the ‘org.apache.flink.streaming.api.functions.windowing.AllWindowFunction’ interface. Otherwise the type has to be specified explicitly using type information 解决办法：加上返回数据类型，如：12.returns(TypeInformation.of(new TypeHint&lt;List&lt;Tuple8&lt;Timestamp, String, Double, Double, String, Integer, Long, String&gt;&gt;&gt;(){ } 2.flink 一直重复计算第一条数据 原因：1与flink的状态值有关 解决办法123每次用到value是都调用 .value方法,如： wayBillState.value().equals(0)","link":"/2020/05/27/flink-常见问题/"},{"title":"mysql 常见问题","text":"1.druidpool连接池空指针异常1234Connection conn; Statement statement; conn = DruidPoolUtils.getConnection(); statement = conn.createStatement(); 2.Exception in thread “main” java.sql.SQLException: Can not call getNString() when field’s charset isn’t UTF-812wayBill.setWayBill(resultSet.getNString(&quot;waybill_no&quot;)); System.out.println(resultSet.getString(&quot;waybill_no&quot;));","link":"/2020/05/29/mysql-常见问题/"},{"title":"flink 算子记录","text":"1、join静态数据库1234567891011121314151617181920212223242526/** * 关联运单表,以及订单完成情况 * 输入 Tuple5&lt;&gt;(positionTime, imei, latitude, longitude, geoHashValue) * 输出 WayBill * @author hadoop */public class JoinWaybill extends RichFlatMapFunction&lt;Tuple5&lt;Timestamp, String, Double, Double, String&gt;, WayBill&gt; { @Override public void flatMap(Tuple5&lt;Timestamp, String, Double, Double, String&gt; value, Collector&lt;WayBill&gt; out) throws Exception { Connection conn; Statement statement; conn = DruidPoolUtils.getConnection(); statement = conn.createStatement(); ResultSet resultSet = statement.executeQuery(&quot;SELECT waybill_no,device_id,longitude,latitude,&quot; + &quot; geo_hash,factory_time,bind_status,leave_time,distance_departure,waybill_status,finish_time FROM t_gps_waybill where device_id = &quot; + value.f1); while (resultSet.next()){ WayBill wayBill = new WayBill(); wayBill.setWayBill(resultSet.getString(&quot;waybill_no&quot;)); out.collect(wayBill); } DruidPoolUtils.close(conn,statement); } 2.sink mysql1234567891011121314151617.flatMap(new JoinWaybill()) .addSink(new RichSinkFunction&lt;WayBill&gt;() { @Override public void invoke(WayBill value, Context context) throws Exception { Connection conn; PreparedStatement statement; conn = DruidPoolUtils.getConnection(); statement = conn.prepareStatement( &quot; INSERT INTO t_gps_test (waybill_no)&quot; + &quot; values(?)&quot; ); statement.setString( 1, value.getWayBill() ); statement.execute(); DruidPoolUtils.close(conn,statement); } }) 3.window12.window(SlidingEventTimeWindows.of(Time.minutes(1),Time.minutes(1))) .process(new JoinWaybill()).print()","link":"/2020/05/29/flink-算子记录/"}],"tags":[{"name":"数据分析","slug":"数据分析","link":"/tags/数据分析/"},{"name":"决策树","slug":"决策树","link":"/tags/决策树/"},{"name":"Dataframe","slug":"Dataframe","link":"/tags/Dataframe/"},{"name":"cdh","slug":"cdh","link":"/tags/cdh/"},{"name":"NIFI","slug":"NIFI","link":"/tags/NIFI/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"hbase","slug":"hbase","link":"/tags/hbase/"},{"name":"IDEA","slug":"IDEA","link":"/tags/IDEA/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"neo4j","slug":"neo4j","link":"/tags/neo4j/"},{"name":"spark","slug":"spark","link":"/tags/spark/"},{"name":"flink","slug":"flink","link":"/tags/flink/"},{"name":"Scala","slug":"Scala","link":"/tags/Scala/"},{"name":"hive","slug":"hive","link":"/tags/hive/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"kafka","slug":"kafka","link":"/tags/kafka/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"kylin","slug":"kylin","link":"/tags/kylin/"},{"name":"mybatis","slug":"mybatis","link":"/tags/mybatis/"},{"name":"正则匹配","slug":"正则匹配","link":"/tags/正则匹配/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"springboot","slug":"springboot","link":"/tags/springboot/"},{"name":"spring cloud","slug":"spring-cloud","link":"/tags/spring-cloud/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"学习","slug":"学习","link":"/tags/学习/"},{"name":"朴素贝叶斯","slug":"朴素贝叶斯","link":"/tags/朴素贝叶斯/"},{"name":"蓝牙定位","slug":"蓝牙定位","link":"/tags/蓝牙定位/"},{"name":"校验","slug":"校验","link":"/tags/校验/"},{"name":"netty","slug":"netty","link":"/tags/netty/"},{"name":"scala","slug":"scala","link":"/tags/scala/"},{"name":"总结","slug":"总结","link":"/tags/总结/"},{"name":"权限设计","slug":"权限设计","link":"/tags/权限设计/"},{"name":"postman","slug":"postman","link":"/tags/postman/"},{"name":"工具类","slug":"工具类","link":"/tags/工具类/"},{"name":"数据库","slug":"数据库","link":"/tags/数据库/"}],"categories":[{"name":"大数据","slug":"大数据","link":"/categories/大数据/"},{"name":"后台","slug":"后台","link":"/categories/后台/"},{"name":"IDEA","slug":"IDEA","link":"/categories/IDEA/"},{"name":"程序语言","slug":"程序语言","link":"/categories/程序语言/"},{"name":"数据库 ","slug":"数据库","link":"/categories/数据库/"},{"name":"脚本","slug":"脚本","link":"/categories/脚本/"},{"name":"学习","slug":"学习","link":"/categories/学习/"},{"name":"蓝牙定位","slug":"蓝牙定位","link":"/categories/蓝牙定位/"}]}