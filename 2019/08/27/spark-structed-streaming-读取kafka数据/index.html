<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
    <meta name="description" content="天道酬勤">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    spark structed streaming 读取kafka数据 |
    
    上善若水博客</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
    <link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">
  
  <script src="/js/pace.min.js"></script>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-spark-structed-streaming-读取kafka数据" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      spark structed streaming 读取kafka数据
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/08/27/spark-structed-streaming-读取kafka数据/" class="article-date">
  <time datetime="2019-08-27T02:34:36.000Z" itemprop="datePublished">2019-08-27</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

      </div>
    

    
      




    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <p>structed streaming :</p>
<blockquote>
<p>Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine.</p>
</blockquote>
<p>Structed streaming 官方文档 <a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">2.4.3官方文档</a></p>
<h3 id="一、部分关于如何连接kafka数据，并做计算。"><a href="#一、部分关于如何连接kafka数据，并做计算。" class="headerlink" title="一、部分关于如何连接kafka数据，并做计算。"></a>一、部分关于如何连接kafka数据，并做计算。</h3><p>首先引入pom.xml</p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.gree.cn&lt;/groupId&gt;
    &lt;artifactId&gt;structstreaminglocation&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;StructStreamingLocation&lt;/name&gt;
    &lt;description&gt;Demo project for StructStreamingLocation&lt;/description&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!-- spark 相关依赖 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- sparkStreaming kafka 整合--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- sparkStreaming kafka end --&gt;

        &lt;!-- struct streaming kafka --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- struct streaming kafka 结束--&gt;

        &lt;!-- spark 相关依赖 --&gt;

        &lt;!-- 数据库连接--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;version&gt;8.0.12&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
            &lt;artifactId&gt;druid&lt;/artifactId&gt;
            &lt;version&gt;1.1.12&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- 数据库连接 end --&gt;

        &lt;!-- Redis客户端 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;redis.clients&lt;/groupId&gt;
            &lt;artifactId&gt;jedis&lt;/artifactId&gt;
            &lt;version&gt;2.7.1&lt;/version&gt;
        &lt;/dependency&gt;


        &lt;!-- 常用工具 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;version&gt;1.18.2&lt;/version&gt;
            &lt;optional&gt;true&lt;/optional&gt;
        &lt;/dependency&gt;
        &lt;!-- 常用工具  end --&gt;

    &lt;/dependencies&gt;
    &lt;build&gt;
       &lt;plugins&gt;
           &lt;plugin&gt;
               &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
               &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
               &lt;version&gt;2.5.5&lt;/version&gt;
               &lt;configuration&gt;
                   &lt;archive&gt;
                       &lt;manifest&gt;
                           &lt;mainClass&gt;com.gree.cn.location.LocationMap&lt;/mainClass&gt;
                       &lt;/manifest&gt;
                   &lt;/archive&gt;
                   &lt;descriptorRefs&gt;
                       &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                   &lt;/descriptorRefs&gt;
               &lt;/configuration&gt;
               &lt;executions&gt;
                   &lt;execution&gt;
                       &lt;id&gt;make-assembly&lt;/id&gt;
                       &lt;phase&gt;package&lt;/phase&gt;
                       &lt;goals&gt;
                           &lt;goal&gt;single&lt;/goal&gt;
                       &lt;/goals&gt;
                   &lt;/execution&gt;
               &lt;/executions&gt;
           &lt;/plugin&gt;
           &lt;plugin&gt;
               &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
               &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
               &lt;version&gt;2.3.1&lt;/version&gt;
               &lt;configuration&gt;
                   &lt;source&gt;1.8&lt;/source&gt;
                   &lt;target&gt;1.8&lt;/target&gt;
               &lt;/configuration&gt;
           &lt;/plugin&gt;
       &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;


</code></pre><p>然后编写相应的代码逻辑：</p>
<pre><code>package com.gree.cn.location;

import com.gree.cn.entity.*;
import com.gree.cn.sink.BaseHeartSink;
import com.gree.cn.sink.LabelSink;
import com.gree.cn.sink.MaterialInfoSink;
import com.gree.cn.sink.MaterialRedisSink;
import com.gree.cn.utils.DruidPoolUtils;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.*;
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;

import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Properties;

import static org.apache.spark.sql.functions.*;

public class LocationMap {
    public static void main(String[] args) {
        //创建SparkSession
        SparkSession spark = SparkSession
                .builder()
                .appName(&quot;小车定位程序&quot;)
                //.master(&quot;local[2]&quot;)
                .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)
                .config(&quot;spark.local.dir&quot;, &quot;/lvm/data1/bluetooth_location/spark/location&quot;)
                .config(&quot;spark.ui.port&quot;, &quot;8083&quot;)
                .config(&quot;spark.sql.shuffle.partitions&quot;, &quot;50&quot;)
                .config(&quot;spark.default.parallelism&quot;, &quot;12&quot;)
                .getOrCreate();

        //读取kafka数据
        Dataset&lt;Row&gt; kafkaValue = spark.readStream()
                .format(&quot;kafka&quot;)
                .option(&quot;kafka.bootstrap.servers&quot;, &quot;10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092&quot;)
                .option(&quot;subscribe&quot;, &quot;topic0619&quot;)
                //.option(&quot;kafka.bootstrap.servers&quot;,&quot;172.16.247.129:9092&quot;)
                //.option(&quot;subscribe&quot;,&quot;test06&quot;)
                .option(&quot;startingOffsets&quot;, &quot;latest&quot;)
                .load()
                .selectExpr(&quot;CAST(value AS STRING)&quot;);




        //基站数据包括 接收时间，基站ID，信标ID，信号强度
        Dataset&lt;BaseTable&gt; baseTableDs = kafkaValue.as(Encoders.STRING()).map(
                (MapFunction&lt;String, BaseTable&gt;) values -&gt; {
                    BaseTable baseTable = new BaseTable();
            //具体逻辑
                      ...
            ...
            ...
                    return baseTable;
                } , ExpressionEncoder.javaBean(BaseTable.class));

        //获取信标电量表数据
        Dataset&lt;LabelData&gt; labelData = baseTableDs
                .filter(col(&quot;orderName&quot;).equalTo(&quot;66&quot;))
                .select(col(&quot;labelID&quot;), col(&quot;electricity&quot;),
                        col(&quot;baseID&quot;),col(&quot;recDatetime&quot;))
                .as(ExpressionEncoder.javaBean(LabelData.class));

        /*StreamingQuery labelDataStart =*/ labelData.repartition(2, col(&quot;labelID&quot;))
                .writeStream()
                .outputMode(&quot;append&quot;)
                .foreach(new LabelSink())
                .start();

        //labelData.writeStream().format(&quot;console&quot;).start();
}

</code></pre><h3 id="二、如何将数据sink到mysql"><a href="#二、如何将数据sink到mysql" class="headerlink" title="二、如何将数据sink到mysql"></a>二、如何将数据sink到mysql</h3><pre><code>package com.gree.cn.sink;


import com.gree.cn.entity.LabelData;
import com.gree.cn.utils.DruidPoolUtils;
import org.apache.spark.sql.ForeachWriter;

import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;


public class LabelSink extends ForeachWriter&lt;LabelData&gt; {
    /**
     * sink 信标数据到MySQL中
     *
     */
    private PreparedStatement statement;
    private Connection connection;


    public boolean open(long arg0, long arg1) {

        return true;
    }


    public void process(LabelData labelData) {
        try {
            connection = DruidPoolUtils.getConnection();

            String insertSql = &quot; REPLACE INTO bluetooth_location.beacon_info (beaconId,electricity,stationId,updateTime) VALUES(?,?,?,?) &quot;;

            statement = connection.prepareStatement(insertSql);
            statement.setString(1,labelData.getLabelID());
            statement.setInt(2,labelData.getElectricity());
            statement.setString(3,labelData.getBaseID());
            statement.setString(4,labelData.getRecDatetime());
            statement.executeUpdate();

        } catch (SQLException e) {
            e.printStackTrace();
        }finally {
            DruidPoolUtils.close(connection,statement);
        }
    }

    @Override
    public void close(Throwable arg0) {

    }
}


</code></pre><h3 id="三、数据库连接池配置"><a href="#三、数据库连接池配置" class="headerlink" title="三、数据库连接池配置"></a>三、数据库连接池配置</h3><pre><code>package com.gree.cn.utils;

import com.alibaba.druid.pool.DruidDataSourceFactory;

import javax.sql.DataSource;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.Properties;

/**
 * 数据库连接池工具类
 */

public class DruidPoolUtils {

    //创建成员变量
    private static DataSource dataSource;
    //加载配置文件
    static {
        try {
            Properties properties = new Properties();
            //加载类路径
            properties.load(DruidPoolUtils.class.getResourceAsStream(&quot;/druid.properties&quot;));

            //读取属性文件，创建连接池
            dataSource = DruidDataSourceFactory.createDataSource(properties);
        }catch (Exception e){
            e.printStackTrace();
        }
    }

    //获取数据源
    public static DataSource getDataSource(){
        return dataSource;
    }

    //获取连接对象
    public static Connection getConnection(){
        try {
            return dataSource.getConnection();
        }catch (SQLException e){
            throw new RuntimeException(e);
        }
    }

    //释放资源

    public static void close(Connection connection, Statement statement, ResultSet resultSet){
        if (resultSet != null ){
            try {
                resultSet.close();
            }catch (SQLException e){
                e.printStackTrace();
            }
        }
        if (statement != null){
            try {
                statement.close();
            }catch (SQLException e){
                e.printStackTrace();
            }
        }
        if (connection != null){
            try {
                connection.close();
            }catch (SQLException e){
                e.printStackTrace();
            }
        }
    }

    public static void close(Connection connection,Statement statement){
        close(connection,statement,null);
    }
}

</code></pre><p>druid.properties</p>
<blockquote>
<p>driverClassName=com.mysql.cj.jdbc.Driver<br>url=jdbc:mysql://localhost:3306?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false<br>username=root<br>password=lyxbdw<br>initialSize=5<br>maxActive=20<br>maxWait=3000<br>minIdle=3</p>
</blockquote>
<h2 id="Scala编写structed-stremaing"><a href="#Scala编写structed-stremaing" class="headerlink" title="Scala编写structed stremaing"></a>Scala编写structed stremaing</h2><h3 id="引入pom-xml"><a href="#引入pom-xml" class="headerlink" title="引入pom.xml"></a>引入pom.xml</h3><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

    &lt;groupId&gt;com.gree.cn&lt;/groupId&gt;
    &lt;artifactId&gt;write2mysql&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;name&gt;write2mysql&lt;/name&gt;
    &lt;description&gt;Demo project for Spring Boot&lt;/description&gt;

    &lt;properties&gt;
        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
        &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt;
        &lt;java.version&gt;1.8&lt;/java.version&gt;
    &lt;/properties&gt;

    &lt;dependencies&gt;
        &lt;!-- 常用插件 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;
            &lt;artifactId&gt;lombok&lt;/artifactId&gt;
            &lt;version&gt;1.18.6&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- 常用插件 结束 --&gt;

        &lt;!-- spark 相关依赖 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- spark streaming kafka --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- spark streaming kafka 结束--&gt;
        &lt;!-- struct streaming kafka --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt;
            &lt;version&gt;2.4.3&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- struct streaming kafka 结束--&gt;
        &lt;!-- spark 相关依赖 结束--&gt;

        &lt;!-- 数据库连接--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;mysql&lt;/groupId&gt;
            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
            &lt;version&gt;8.0.15&lt;/version&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;
            &lt;artifactId&gt;druid&lt;/artifactId&gt;
            &lt;version&gt;1.1.12&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- 数据库连接 end --&gt;

        &lt;!-- log 日志--&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j&lt;/artifactId&gt;
            &lt;version&gt;1.2.17&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;!-- log 日志结束--&gt;

    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;
                &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.15.2&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;scala-compile-first&lt;/id&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;compile&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;includes&gt;
                                &lt;include&gt;**/*.scala&lt;/include&gt;
                            &lt;/includes&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                    &lt;execution&gt;
                        &lt;id&gt;scala-test-compile&lt;/id&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;testCompile&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;

            &lt;plugin&gt;
                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;
                    &lt;descriptorRefs&gt;
                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                    &lt;/descriptorRefs&gt;
                    &lt;archive&gt;
                        &lt;manifest&gt;
                            &lt;mainClass&gt;com.gree.cn.write2mysql.Write2mysql&lt;/mainClass&gt;
                        &lt;/manifest&gt;
                    &lt;/archive&gt;
                &lt;/configuration&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;make-assembly&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;single&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;

        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
</code></pre><p>与java的差别主要是打包插件不同，编写相应逻辑</p>
<pre><code>package com.gree.cn.write2mysql


import com.gree.cn.sink.MysqlSink
import org.apache.spark.sql.SparkSession

/**
  * 原始数据保存
  * @author sfz
  * Time 2019-08-08
  */
object Write2mysql {
  def main(args: Array[String]): Unit = {

    //创建SparkSession
    val spark = SparkSession
      .builder()
      .appName(&quot;write2mysql&quot;)
      .master(&quot;local[*]&quot;)
      .config(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)
      .config(&quot;spark.local.dir&quot;, &quot;/lvm/data1/bluetooth_location/spark/write2mysql&quot;)
      .config(&quot;spark.ui.port&quot;, &quot;8084&quot;)
      .getOrCreate()

    //引入隐式转换，保证String可以序列化
    import spark.implicits._

    //读取kafka数据
    val kafkaValue = spark.readStream
      .format(&quot;kafka&quot;)
      //.option(&quot;kafka.bootstrap.servers&quot;, &quot;10.7.20.190:9092,10.7.20.191:9092,10.7.17.9:9092&quot;)
     // .option(&quot;subscribe&quot;, &quot;topic0619&quot;)
      .option(&quot;kafka.bootstrap.servers&quot;,&quot;localhost:9092&quot;)
      .option(&quot;subscribe&quot;,&quot;test06&quot;)
      .option(&quot;startingOffsets&quot;, &quot;latest&quot;)
      .load()
      .selectExpr(&quot;CAST(value AS STRING)&quot;)

    //将kafka数据转化为BaseTable结构
    val baseData = kafkaValue.as[String].map(values =&gt; {
//具体的解析数据逻辑
。。。
。。。
。。。
        BaseTable.apply(frameHeader, equipment, frameLength, orderName, baseID, labelID, signalPower, electricity, datetime, checkName, recDatetime)
      }
      else {
        BaseTable.apply(frameHeader, equipment, frameLength, orderName, baseID77, labelID, signalPower, electricity, datetime, checkName, recDatetime)
      }
    }).as[BaseTable]

    //将数据sink到mysql数据库中
    val query =  baseData.writeStream
      .foreach(new MysqlSink)
      .outputMode(&quot;append&quot;)
      .start()

    query.awaitTermination()
  }


  //创建BaseTable结构
  case class BaseTable(frameHeader: String, equipment: String, frameLength: String, orderName: String, baseId: String,
                       labelId: String, signalPower: Integer, electricity: Integer, datetime: String, checkName: String, recDatetime: String)

}


</code></pre><p>接着sink到mysql 数据库中</p>
<pre><code>package com.gree.cn.sink

import java.sql.{Connection, PreparedStatement, SQLException}

import com.gree.cn.utils.{DateUtils, MysqlPoolUtils}
import com.gree.cn.write2mysql.Write2mysql.BaseTable
import org.apache.spark.sql.ForeachWriter

class MysqlSink() extends ForeachWriter[BaseTable](){

  var conn:Connection = _
  var statement : PreparedStatement = _
  override def open(partitionId: Long, epochId: Long): Boolean = {
     conn = MysqlPoolUtils.getConnection.get
     conn.setAutoCommit(false)
     true
  }

  override def process(baseTable: BaseTable): Unit = {
    val insertSql = &quot; &quot;

    statement = conn.prepareStatement(insertSql)
    statement.setString(1, baseTable.frameHeader)
 。。。
。。。
    statement.setString(11, baseTable.recDatetime)
    statement.executeUpdate()
  }

  override def close(errorOrNull: Throwable): Unit = {
    try{
      if ( statement != null &amp;&amp; conn != null) {
        statement.close()
        conn.close()
      }
    } catch {
      case e: SQLException =&gt;
        e.printStackTrace()
    }
  }
}



</code></pre><p>最后配上数据库连接池</p>
<pre><code>package com.gree.cn.utils

import java.sql.Connection
import java.util.Properties

import com.alibaba.druid.pool.DruidDataSourceFactory
import javax.sql.DataSource
import org.apache.log4j.Logger

/**
  * @author sfz
  * MySql 数据库连接池
  */
object MysqlPoolUtils {

private val log = Logger.getLogger(MysqlPoolUtils.getClass.getName)

val dataSource:Option[DataSource] = {
  try {
    val druidProps = new Properties()
    //获取Druid连接池配置文件
    val druidConfig = getClass.getResourceAsStream(&quot;/application.properties&quot;)
    //加载配置文件
    druidProps.load(druidConfig)
    Some(DruidDataSourceFactory.createDataSource(druidProps))
  }catch {
    case error :Exception =&gt;
      log.error(&quot;Error Create Mysql Connection &quot;,error)
      None
  }
}
  //获取连接
  def getConnection:Option[Connection] = {
    dataSource match {
      case Some(ds) =&gt; Some(ds.getConnection)
      case None =&gt; None
    }
  }
}


</code></pre><p>application.properties</p>
<blockquote>
<p>driver-class-name = com.mysql.cj.jdbc.Driver<br>url=jdbc:mysql://localhost:3306/base_data?serverTimezone=GMT&amp;autoReconnect=true&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false<br>username=root<br>password=lyxbdw<br>filters=stat,config<br>initialSize=2<br>maxActive=20<br>minIdle=2<br>maxWait=60000<br>timeBetweenEvictionRunsMillis=60000<br>minEvictableIdleTimeMillis=300000<br>validationQuery=SELECT 1<br>testWhileIdle=true<br>testOnBorrow=false<br>testOnReturn=false<br>removeAbandoned=true<br>logAbandoned=true<br>poolPreparedStatements=false<br>removeAbandonedTimeout=1800<br>maxOpenPreparedStatements=100</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/27/spark-structed-streaming-读取kafka数据/" data-id="ck3o5xx7x002x8wujzxrr18dj"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2019/08/30/flink-基础/" class="article-nav-link">
        <strong class="article-nav-caption">Newer posts</strong>
        <div class="article-nav-title">
          
            flink 基础
          
        </div>
      </a>
    
    
      <a href="/2019/08/26/蓝牙信标定位项目总结/" class="article-nav-link">
        <strong class="article-nav-caption">Olde posts</strong>
        <div class="article-nav-title">蓝牙信标定位项目总结</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2019 上善若水博客</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>

<aside class="sidebar sidebar-specter">
  
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/hexo.svg" alt="上善若水博客"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">Archives</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">Gallery</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">About</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/lazyload.min.js"></script>
<script src="/js/busuanzi-2.3.pure.min.js"></script>

  <script src="/fancybox/jquery.fancybox.min.js"></script>



  <script src="/js/tocbot.min.js"></script>
  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>


<script src="/js/ocean.js"></script>

</body>
</html>